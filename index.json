[{"content":"This is a collection of articles I have written at work and which are published on other websites. The articles cover various topics, including technical insights, best practices, and personal experiences related to my professional journey.\nAll links, if possible, point to Wayback Machine archives, to protect against link rot. The downside is that not all pictures are archived, so some articles may not look as originally intended.\n2025 # Preventing duplicate and risky apps: How to monitor user-installed AppX packages Improving Microsoft Teams audio/video in Citrix: Verifying SlimCore optimization with uberAgent 2024 # MCS Write Cache Cost Optimization on Azure with uberAgent Maximize Citrix Performance and Security with the Power of uberAgent 7.3 2023 # uberAgent Helpdesk App Updated for uberAgent 7.1 uberAgent Helpdesk App Updated for uberAgent 7.1 2022 # uberAgent Helpdesk App Updated for uberAgent 7 2021 # uberAgent 6.2 Preview: Splunk Enterprise Security Integration uberAgent Helpdesk App Updated for uberAgent 6.1 2020 # Announcing the Experience Score Dashboard Citrix SCOM Management Packs Are EOL. Time to Move On! Visualizing uberAgent Data in Azure Monitor—Part 4: Conclusion Visualizing uberAgent Data in Azure Monitor—Part 3: Grafana Visualizing uberAgent Data in Azure Monitor—Part 2: AM vs. Splunk Visualizing uberAgent Data in Azure Monitor—Part 1: Basics Monitoring #WFH \u0026amp; uberAgent Free For Unlimited Users For 2 Months 2019 # Monitoring User Session Activity With uberAgent 2018 # Top 10 Issues uberAgent Helps Identify in Minutes Monitoring User Profile Sizes With uberAgent Monitoring RDP Session Hijacking 2016 # Wie kann ich ältere Webseiten in meinem Unternehmen unterstützen? Teil 1, Teil 2 Export And Import Citrix XenDesktop Published Apps Year Unknown # List of articles where I can\u0026rsquo;t remember the year.\nPractice guide: Citrix ADC Monitoring \u0026amp; Alerting Practice guide: Generating Driver Version Inventory Reports Practice guide: Documenting Applied Computer GPOs Practice guide: Collecting the Processor Temperature With uberAgent Practice guide: Querying Windows Event Log Practice guide: Collecting More WiFi Details From WFH Employees Practice guide: Creating a TPM Status Inventory Report ","date":"19 August 2025","externalUrl":null,"permalink":"/posts/work-articles/","section":"Posts","summary":"This is a collection of articles I have written at work and which are published on other websites. The articles cover various topics, including technical insights, best practices, and personal experiences related to my professional journey.\nAll links, if possible, point to Wayback Machine archives, to protect against link rot. The downside is that not all pictures are archived, so some articles may not look as originally intended.\n","title":"Collection: Work Articles Published on Other Sites","type":"posts"},{"content":"","date":"19 August 2025","externalUrl":null,"permalink":"/","section":"DominikBritz.com","summary":"","title":"DominikBritz.com","type":"page"},{"content":"","date":"19 August 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"19 August 2025","externalUrl":null,"permalink":"/tags/splunk/","section":"Tags","summary":"","title":"Splunk","type":"tags"},{"content":"","date":"19 August 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"19 August 2025","externalUrl":null,"permalink":"/tags/uberagent/","section":"Tags","summary":"","title":"UberAgent","type":"tags"},{"content":"","date":"19 August 2025","externalUrl":null,"permalink":"/tags/work/","section":"Tags","summary":"","title":"Work","type":"tags"},{"content":"","date":"9 February 2025","externalUrl":null,"permalink":"/tags/blowfish/","section":"Tags","summary":"","title":"Blowfish","type":"tags"},{"content":"","date":"9 February 2025","externalUrl":null,"permalink":"/tags/cloudflare/","section":"Tags","summary":"","title":"Cloudflare","type":"tags"},{"content":"","date":"9 February 2025","externalUrl":null,"permalink":"/tags/giscus/","section":"Tags","summary":"","title":"Giscus","type":"tags"},{"content":"","date":"9 February 2025","externalUrl":null,"permalink":"/tags/github/","section":"Tags","summary":"","title":"GitHub","type":"tags"},{"content":"This article explains how to create this beautiful blog with the static site generator Hugo, the theme Blowfish, GitHub for storage, Giscus for comments, and Cloudflare for hosting. And the best thing: it doesn\u0026rsquo;t cost you a penny!\nMy Blog Journey # I\u0026rsquo;m blogging for more than 15 years now. During that time, I tested and used different blogging technologies and products. I started with a hosted service. While writing was okay, it lacked flexibility and personalization possibilities. I then moved to a self-hosted WordPress installation, which solved these problems. But, WordPress was slow, maintenance intensive, and I had to pay and maintain a virtual private server which was idling the majority of the time.\nBefore the current setup I used GitHub pages, which is also free. I invested quite some time in finding the perfect theme, and right after I had everything set up, the theme\u0026rsquo;s developer stopped maintaining the free version and focused on an overpriced paid version. What a waste of time\u0026hellip;\nArchitecture # The final architecture is shown below:\nArchitecture Let\u0026rsquo;s find out how my requirements led to that architecture.\nPerformance: the blog should load amazingly fast for my readers. Low maintenance: I don\u0026rsquo;t want to upgrade servers and apps all the time. I\u0026rsquo;m okay with a little maintenance here and there. Markdown support: articles should be written in Markdown for a good writing experience and easy migrations in case I move to another solution in the future. Active developers or community: the solution should be well maintained by the project developers or the community. I don\u0026rsquo;t want a second GitHub pages scenario. Easy and high security: should use the latest and greatest security techniques which I can switch on easily. Cost: keep the cost minimal or to zero. Beautiful, with dark mode: a beautiful and modern theme, that includes dark mode. I love dark mode, but there are others who don\u0026rsquo;t. I want dark mode as default, but give the reader the option to choose. Comments: Readers should have the ability to comment on my articles to share thoughts and provide feedback. I don\u0026rsquo;t want to outsource the comments, but have the data in my control. Hugo - Static Site Generator # A static site generator (SSG) is a tool that compiles content into static HTML files, which can be served quickly and efficiently. The benefits of SSGs compared to tools like WordPress include better performance due to faster load times since static sites don’t require server-side processing, enhanced security with fewer vulnerabilities as there’s no database or dynamic processing, and simplicity in hosting and management, often requiring only basic web hosting. Additionally, content can be managed in code repositories like GitHub, which I like.\nThe most popular SSGs are Jekyll, used by GitHub pages under the hood, and Hugo. I\u0026rsquo;m sure there are differences if you ask a web developer, but for me as someone who wants to run just a small blog, the nuances don\u0026rsquo;t matter.\nI went with Hugo mainly because of the theme Blowfish.\nBlowfish - A Beautiful and Powerful Hugo Theme # There are lots of Hugo themes available, but only Blowfish ticked all boxes for me.\nFeature overview:\nMultiple homepage layouts Dark mode (forced on/off or auto-switching with user toggle) Series of articles Scrollable table of contents Support for several shortcodes like Gallery, Timeline, GitHub cards, and Carousels Fully documented with regular updates Full feature list GitHub - Store Drafts and Public Content # As SSGs create static HTML files, you can store your whole blog in a git system like GitHub and can benefit from version control. Also, GitHub allows unlimited private and public repositories.\nI store my work in a private repository, so I can privately work on new articles.\nCloudflare - Ship Your Website Fast and Secure # Cloudflare is a web performance and security company that provides services like content delivery network (CDN), DDoS protection, and website optimization. By acting as a reverse proxy, it helps enhance site speed, improve security, and manage traffic efficiently.\nThey have a very generous free tier and allow you to host multiple websites for free including the latest security and optimization technologies.\nGiscus - Using GitHub Issues For Comments # I use Giscus for comments. Giscus is a great solution for small blogs like mine. It\u0026rsquo;s free, easy to set up, and the comments are stored in my GitHub repository. I don\u0026rsquo;t have to worry about a third-party service shutting down and losing all my comments. Well, GitHub could shut down, but that is unlikely to happen soon.\nGiscus comment UI Blog Requirements Solved ✅ # Let\u0026rsquo;s check how the above technologies solve my requirements.\nPerformance: achieved through Cloudflare Low maintenance: there are no servers to manage, only Hugo and Blowfish need to be updated. Markdown support: Hugo supports markdown. Active developers or community: Hugo and Blowfish both receive regular updates. Easy and high security: achieved through Cloudflare. Cost: all technologies described above are free to use. I only pay for the domain. Beautiful, with dark mode: supported by Blowfish. Installation Instructions # This section gives detailed installation instructions and shows how the technologies work together.\nHugo # Follow the installation instructions for Hugo for your operating system (details).\nI\u0026rsquo;ve used winget on Windows:\nwinget install Hugo.Hugo.Extended Don\u0026rsquo;t create a new Hugo project, Blowfish will do that for you.\nBlowfish # First, create a new private repository in GitHub and clone it locally.\ncd your/path git clone git@github.com:YourUsername/YourRepository.git Normally, you create Hugo projects and install themes manually. But the Blowfish team created a CLI which makes the installation and maintenance a lot easier.\nDownload the latest NPM LTS version and install it.\nInstall the Blowfish CLI:\nnpm i -g blowfish-tools cd into to your git repository directory and run the command blowfish-tools to start an interactive setup. It will take you through installation and configuration. When asked for a directory, use . to specify the current one.\nMore information on the installation is available in the Blowfish docs.\nFollow the getting started guide to customize your site. There is also this tutorial to support with the first steps.\nCloudflare # Create a free account on cloudflare.com.\nGo to Workers \u0026amp; Pages and click Create to create a new page.\nChoose Create by importing an existing Git repository and connect Cloudflare to GitHub.\nIn the build configuration, choose Hugo as the framework and select your repository. The build command is hugo and the output directory is public.\nEvery time you push changes to your repository, Cloudflare will automatically build and deploy your site. Pushes to the main branch will be deployed to the production URL, while pushes to other branches will be deployed to a preview URL.\nGiscus Comments # To enable comments, follow the Giscus installation guide. It\u0026rsquo;s a simple process and only requires a few clicks.\nBlowfish supports any comment system, including Giscus, with a Hugo partial. Create a layouts/partials/comments.html and paste the code provided by Giscus.\n\u0026lt;script src=\u0026#34;https://giscus.app/client.js\u0026#34; data-repo=\u0026#34;[ENTER REPO HERE]\u0026#34; data-repo-id=\u0026#34;[ENTER REPO ID HERE]\u0026#34; data-category=\u0026#34;[ENTER CATEGORY NAME HERE]\u0026#34; data-category-id=\u0026#34;[ENTER CATEGORY ID HERE]\u0026#34; data-mapping=\u0026#34;url\u0026#34; data-strict=\u0026#34;0\u0026#34; data-reactions-enabled=\u0026#34;1\u0026#34; data-emit-metadata=\u0026#34;0\u0026#34; data-input-position=\u0026#34;top\u0026#34; data-theme=\u0026#34;transparent_dark\u0026#34; data-lang=\u0026#34;en\u0026#34; data-loading=\u0026#34;lazy\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; In every article where you want to enable comments, add the following line to the site\u0026rsquo;s metadata:\ncomments: true Publish Your First Article # Create a new folder in /content/posts and add a index.md with some metadata about your article.\n# index.md --- author: Dominik Britz # Your name date: \u0026#34;2024-09-18\u0026#34; # Publishing data title: \u0026#34;Article title\u0026#34; # Title featured: false # Highlight featured articles on the homepage draft: yes # Drafts are ignored by the `hugo` build commands tags: # Tag your articles to make it easier for readers to find similar content - Hugo - Blowfish - GitHub - Cloudflare comments: true # Whether comments should appear under the article series: [\u0026#34;My series\u0026#34;] # Optional. All series articles are highlighted and linked at the beginning of the article series_order: 1 # Optional. The current article\u0026#39;s order in the series --- Start writing your article in Markdown below the metadata.\nYou can live preview all changes and drafts you are working on with hugo server --buildDrafts. The live preview refreshes every time you save a file. You can save your progress with git in your private repository anytime and nobody sees your unfinished drafts.\nOnce you are ready to publish your article, in the metadata section, change draft to no and change the date to the current date. Then run hugo to build the final site to the public folder.\nPush the changes to your private repository. Cloudflare sees the new commit in the repository and starts building your website.\nAfter a few minutes, your first article is publically available.\nKeeping Your Blog Up To Date # Always keep Hugo and Blowfish updated to benefit from the latest features and security updates. I suggest subscribing to the GitHub releases of both projects.\nOn the GitHub project page, click on Watch -\u0026gt; Custom and choose Releases.\nSubscribe to new GitHub releases Hugo # Update Hugo with winget.\nwinget upgrade Hugo.Hugo.Extended Blowfish # Update Blowfish with its CLI.\nblowfish-tools Further Reading # Here are some other articles that follow the same idea, but differ slightly in the implementation:\nMarkus Kraus - Built with Hugo, Cloudflare and Love Pablo Jesús González Rubio - Setting Up a Static Page with Hugo, Cloudflare, and Umami Analytics Colin Fretwell - Deploy a Hugo Site with Cloudflare Pages and GitHub Changelog # 2025-06-10: Changed the article to use Cloudflare\u0026rsquo;s Hugo integration instead of a manual setup with copying the public folder to a public repository. I had all kind of issues with the manual setup, especially with CSS. The new setup is much easier and more reliable. 2025-02-09: Initial version ","date":"9 February 2025","externalUrl":null,"permalink":"/posts/how-to-create-this-blog/","section":"Posts","summary":"This article explains how to create this beautiful blog with the static site generator Hugo, the theme Blowfish, GitHub for storage, Giscus for comments, and Cloudflare for hosting. And the best thing: it doesn’t cost you a penny!\nMy Blog Journey # I’m blogging for more than 15 years now. During that time, I tested and used different blogging technologies and products. I started with a hosted service. While writing was okay, it lacked flexibility and personalization possibilities. I then moved to a self-hosted WordPress installation, which solved these problems. But, WordPress was slow, maintenance intensive, and I had to pay and maintain a virtual private server which was idling the majority of the time.\n","title":"How To Create This Beautiful Blog - (It Is Free!)","type":"posts"},{"content":"","date":"9 February 2025","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"18 January 2025","externalUrl":null,"permalink":"/tags/cadvisor/","section":"Tags","summary":"","title":"CAdvisor","type":"tags"},{"content":"","date":"18 January 2025","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"This article describes how to monitor Docker containers and host performance as well as Docker logs with VictoriaMetrics, VictoriaLogs and Grafana.\nIt is part of my series on creating a home server on an old laptop. I’m assuming that you’ve set up Docker, Caddy, and Dockge, as described in the first article of the series.\nGrafana - Single Pane of Glass # Grafana dashboard example The most important things to monitor on a home server are performance metrics and logs for both the Docker host and your containers. Performance metrics help you to understand the health of your server and to identify bottlenecks. Logs are important to troubleshoot issues and to understand what is happening on your server.\nThe ideal solution for a home server is lightweight and easy to set up and maintain. It should also be able to scale with your needs, because you might want to add more monitoring in the future.\nEnterprise-grade solution like Nagios or Zabbix are too complex to set up and too resource hungry for a single home server. I ruled these out from the beginning.\nThe Docker monitoring space is quite crowded. Beszel, despite being pretty new, is doing the best job in my opinion. There is also Dozzle for Docker logs. However, all these tools are single-purpose and don\u0026rsquo;t provide a single pane of glass for both metrics and logs. That\u0026rsquo;s where Grafana comes in.\nGrafana is a charting software that can visualize data from multiple backends like Elasticsearch, Splunk, or InfluxDB. It is quite popular in the home server community and comes as open-source and enterprise version. The open-source version is fine for a home server and fulfills all my needs: it is easy to set up, has a lot of plugins for future use-cases, and is well documented.\nBut, Grafana is only for charting. The collected data must be stored somewhere else. I chose VictoriaMetrics and VictoriaLogs for data storage.\nWhy Not Prometheus and Loki? # Prometheus seems to be the standard storage for performance metrics on Linux. Loki was developed for logs by Grafana and quickly gained a lot of popularity in the self-hosting community.\nI work with Splunk on a daily basis, and found the Prometheus/Loki universe somewhat opaque in comparison. Just the fact that two technologies are needed to collect metrics and logs didn\u0026rsquo;t sit well with me. It feels like a workaround for a problem that should be solved by one tool.\nMy colleague went the Loki route, and was not impressed at all by the search language. So I went in search of a suitable alternative and found VictoriaMetrics and VictoriaLogs.\nVictoriaMetrics # VictoriaMetrics (VME) is a time-series database like Prometheus, InfluxDB and many others. What I like about VME is its ease-of-use character. It is a single binary, as it is written in Go (like the aforementioned Beszel), and comes with reasonable defaults. VME is compatible to many popular data collectors like Prometheus or Data Dog. Also, VME is open-source, and the free version has nearly all the features of the enterprise one. Below are the most important VME features (source).\nIt can be used as long-term storage for Prometheus. It can be used as a drop-in replacement for Prometheus in Grafana, because it supports the Prometheus querying API. It is easy to set up and operate: VME consists of a single small executable without external dependencies. All the configuration is done via explicit command-line flags with reasonable defaults. All the data is stored in a single directory specified by the -storageDataPath command-line flag. It implements a PromQL-like query language - MetricsQL, which provides improved functionality on top of PromQL. The documentation also mentions superior performance in comparison to other time-series databases. That\u0026rsquo;s only interesting when you ingest tons of data, though, which is not the case on home servers. But, it is nice to have.\nVictoriaLogs # VictoriaLogs (VL) is the component for log storage. Yes, I know, with Victoria you also have two solutions for metrics and logs like with Prometheus and Loki, but at least they come under one umbrella and are perfectly integrated out-of-the-box.\nBelow are the most important VL features (source).\nIt is much easier to set up and operate compared to Elasticsearch and Grafana Loki. I can confirm that for Elasticsearch. The setup is a nightmare, even with community efforts like docker-elk. It provides an easy yet powerful query language (LogsQL) with full-text search across all the log fields. Powerful is relative, when you come from Splunk. More on that later. It can handle up to 30x bigger data volumes than Elasticsearch and Grafana Loki when running on the same hardware. (Nice to have) It provides a Grafana plugin for querying logs. Installation # VME and VL are installed with Docker Compose on my home server. They provide example compose files on their GitHub. I took what I needed from there, changed the networking to work with my Caddy reverse proxy, and configured some data collectors.\nThe result can be found in my blog-victoria-metrics repository.\nDominikBritz/blog-victoria-metrics null 6 1 Components # The following components (containers) and configs are included in the compose file:\nvector: Vector is Data Dog\u0026rsquo;s log collector. It collects log files and sends them to VL. I use it to send Docker logs to VL. Read on for more information. victorialogs: The container for VL. Data is stored in ./vldata. victoriametrics: The container for VME. Data is stored in ./vmdata. I use VME\u0026rsquo;s Prometheus integration to collect metrics via ./prometheus.yml. grafana: used for charting. VME and VL are added as data sources automatically. vmauth: is a router and balancer for HTTP requests. It proxies query requests from vmalert to either VME or VL, depending on the requested path. vmalert: executes alerting and recording rules. Five examples are added automatically to play around with. alertmanager: receives alerting notifications from vmalert and distributes them according to --config.file. Nothing is set here by default. You must add your own destinations like SMTP or similar. node-exporter: adds performance monitoring for the Docker host. cadvisor: adds performance monitoring for Docker containers. Clone the Repo # # Make a new folder for VME mkdir -p /usr/local/data/docker/victoriametrics # Go to your docker folder cd /usr/local/data/docker/victoriametrics # Clone the repo. Note the dot at the end. This will clone in the current dir and not create a \u0026#34;blog-victoria-metrics\u0026#34; subdir. git clone https://github.com/DominikBritz/blog-victoria-metrics . # Change Grafana folder permissions chown -Rfv 472:472 grafana # Create a copy of the .env.template and fill out the variables cp .env.template .env Change the GRAFANA_URL variable according to your domain. Make sure it\u0026rsquo;s the same URL as you choose in the Caddy section in a bit.\nData will be retained until the configured time or space settings are hit - whatever comes first.\n# .env GRAFANA_URL=https://grafana.domain.com # Grafana outside container RETENTION_PERIOD=30d # Data retention - time RETENTION_MAXDISKSPACE=30Gib # Data retention - space VLDATASOURCE_VERSION=v0.13.2 # New release: https://github.com/VictoriaMetrics/victorialogs-datasource/releases CADVISOR_VERSION=v0.49.2 # New release: https://gcr.io/cadvisor/cadvisor VECTOR_HOSTNAME=homeserver # Name that should appear as hostname in Grafana dashboards Let’s Encrypt Certificate via Caddy # Caddyfile # Add an entry for Grafana to Caddyfile (details). VME and VL come with their own web interfaces, but they don\u0026rsquo;t have any benefit over using Grafana.\ngrafana.{$MY_DOMAIN} { reverse_proxy grafana:3000 tls { dns netcup { customer_number {env.NETCUP_CUSTOMER_NUMBER} api_key {env.NETCUP_API_KEY} api_password {env.NETCUP_API_PASSWORD} } propagation_timeout 900s propagation_delay 600s resolvers 1.1.1.1 } } DNS A Record # Add the following A record to your DNS domain:\ngrafana.home.yourdomain.com 192.168.178.254 # replace with your Docker host\u0026#39;s IP address Reload Caddy’s Configuration # Instruct Caddy to reload its configuration by running:\ndocker exec -w /etc/caddy caddy caddy reload Start the Stack # In Dockge, click Scan Stacks Folder in the user menu top right. The victoriametrics stack should appear in the list.\nDeploy the stack through the Dockge UI. Check the terminal window for any errors.\nLogin With OIDC # If you prefer logging in with OIDC, checkout my colleagues article on OIDC with Grafana.\nMonitoring With Grafana # Grafana has a central store for dashboards. You can add dashboards by ID by browsing to Dashboards \u0026gt; New \u0026gt; Import. Below are the dashboards I found most useful:\nPurpose Dashboard name Dashboard ID Docker host Node Exporter Full 1860 Docker container cAdvisor Exporter 14282 VictoriaLogs instance VictoriaLogs 22084 VictoriaMetrics instance VictoriaMetrics - single-node 10229 Docker Logs # Node Exporter and cAdvisor both create metrics and hence use VME. For Docker logs, we finally need VL as we\u0026rsquo;re looking to ingest text.\nI could not find any dashboard for Docker logs for Grafana, so I created one. You can find it here or import it with the ID 22593.\nDocker Logs Dashboard At the top the dashboards shows the total numbers of logs, errors, and warnings, as well as error and warnings ratios. Below, you see the top 10 error and warning producers by container and host.\nAt the bottom is a table that lists all logs. You can filter the table by each column, which allow you take a look at certain containers, hosts, log types, and even parts of the log messages. This is a standard functionality in Grafana, which I now sorely miss in Splunk.\nMy Thoughts on LogsQL # The VictoriaLogs team describes their query language LogsQL as easy yet powerful. When you must work with the awful Loki LogQL language, that might be true. But if you use Splunk as a comparison, the language is still in its infancy.\nBelow is the LogsQL query that powers the table. I wanted to extract the log type with a case-insensitive regular expression. That is not available in LogsQL. As a workaround, I had to use a capture group with multiple casing options per log type. Not only becomes the search unnecessary long, it also might be inaccurate if some clever developer uses eRror in their messages.\nsource_type:=\u0026#34;docker_logs\u0026#34; host:=$host | extract_regexp \u0026#34;(?P\u0026lt;type\u0026gt;error|ERROR|Error|warn|WARN|Warn|info|INFO|Info|debug|DEBUG|Debug)\u0026#34; | sort by (Time desc) I\u0026rsquo;ve only created a very simple dashboard, and already ran in the above issue. I doubt this is production ready in its current form for large enterprises. Nevertheless, it is a solid base to expand on, and I\u0026rsquo;m curious what LogsQL has to offer in the future.\nUpdate: I was wrong. The above query is not necessary. The LogsQL query below does the same job in a much cleaner way.\nsource_type:=\u0026#34;docker_logs\u0026#34; host:=$host | extract_regexp \u0026#34;(?P\u0026lt;type\u0026gt;(?i)error|warn|info|debug)\u0026#34; | sort by (Time desc) I\u0026rsquo;ve updated my Grafana dashboard accordingly.\nSummary # I was looking for a lightweight and easy-to-maintain yet flexible solution for monitoring my home server. That is apparently not available. You either have to go with a single-purpose tool like Beszel or Dozzle, or with a more complex solution like I\u0026rsquo;ve built. I chose the more complex solution, because I prefer the flexibility vs. the limited feature set of single-purpose tools.\nI found VictoriaMetrics and VictoriaLogs more reasonable than Prometheus and Loki, but it\u0026rsquo;s not a huge difference to be honest. The Grafana integration is a big plus, and the performance is good. The query languages are not as powerful as I\u0026rsquo;m used to with Splunk, but they are good enough for a home server.\nChangelog # 2025-01-21 Updated the LogsQL query in the Grafana dashboard section ","date":"18 January 2025","externalUrl":null,"permalink":"/posts/victoria-metrics-grafana/","section":"Posts","summary":"This article describes how to monitor Docker containers and host performance as well as Docker logs with VictoriaMetrics, VictoriaLogs and Grafana.\nIt is part of my series on creating a home server on an old laptop. I’m assuming that you’ve set up Docker, Caddy, and Dockge, as described in the first article of the series.\n","title":"Docker Monitoring With Grafana and VictoriaMetrics","type":"posts"},{"content":"","date":"18 January 2025","externalUrl":null,"permalink":"/tags/grafana/","section":"Tags","summary":"","title":"Grafana","type":"tags"},{"content":"","date":"18 January 2025","externalUrl":null,"permalink":"/series/home-server/","section":"Series","summary":"","title":"Home Server","type":"series"},{"content":"","date":"18 January 2025","externalUrl":null,"permalink":"/tags/home-server/","section":"Tags","summary":"","title":"Home Server","type":"tags"},{"content":"","date":"18 January 2025","externalUrl":null,"permalink":"/tags/node-exporter/","section":"Tags","summary":"","title":"Node Exporter","type":"tags"},{"content":"","date":"18 January 2025","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"18 January 2025","externalUrl":null,"permalink":"/tags/victorialogs/","section":"Tags","summary":"","title":"VictoriaLogs","type":"tags"},{"content":"","date":"18 January 2025","externalUrl":null,"permalink":"/tags/victoriametrics/","section":"Tags","summary":"","title":"VictoriaMetrics","type":"tags"},{"content":"","date":"18 September 2024","externalUrl":null,"permalink":"/tags/mtls/","section":"Tags","summary":"","title":"MTLS","type":"tags"},{"content":"","date":"18 September 2024","externalUrl":null,"permalink":"/tags/remote-access/","section":"Tags","summary":"","title":"Remote Access","type":"tags"},{"content":"This article explains how to securely access your home server and its services with mTLS and WireGuard.\nIt is part of my series on creating a home server on an old laptop. I’m assuming that you’ve set up Docker, Caddy, and Authelia, as described in the first article of the series.\nWhile the described techniques work for all services, I focus on Nextcloud and Immich, which are part of the series.\nIs Exposing Your Services to the Internet Secure? # When you expose your apps and services from your home server to the public Internet, you are immediately a target for exploits, malware, and so on and so forth. Hence, it is generally not a good idea. If you can, avoid it.\nThat is why the common strategy of self-hosters is to choose a VPN like WireGuard or OpenVPN to access their services. With a VPN, your accessing device becomes part of your local network and your services don\u0026rsquo;t need to be exposed.\nI Did It Anyway # I had the requirement to make Immich and Nextcloud available for non-tech save people like my parents and parents in law. Teaching them that my services can only be accessed with a VPN seemed impossible. Also, I find that WireGuard on Android drains the battery unnecessarily heavy.\nHence, I started searching for alternatives and found Mutual TLS.\nMutual TLS # With Mutual TLS (mTLS), also known as client certificate authentication, both client and server have a certificate, and both sides authenticate using their public/private key pair. That is different to normal TLS, where only the server has a certificate.\nBenefits # This yields to the following benefits when used with a reverse proxy like Caddy:\nAttackers need a valid private key to authenticate to the reverse proxy. The private key is secured with a passphrase, too. The mTLS handshake happens before access to the web service is granted. Hence, attackers can\u0026rsquo;t use exploits or issues in web services as they can\u0026rsquo;t see the web service in the first place. mTLS is transparent to the user. You install a certificate on the user\u0026rsquo;s devices once and, they can transparently use your services around the globe. No additional battery consumption. Downsides And Constraints # If your ISP is putting you behind CGNAT, mTLS does not work. You need a VPN then.\nWith mTLS you expose your public IP. If that is a problem for you, a VPN is the only option. It\u0026rsquo;s not a problem for me, though.\nA more serious constraint is, that mTLS is not well-known and that not all services or devices support it. Luckily, Immich and Nextcloud both support mTLS.\nAlso, you have to install a certificate on every device. For me, that means I have to install certificates on multiple laptops, Android, and iOS devices of my parents, wife, and so on. Unfortunately, Android Family Link does not support installing certificates. So it\u0026rsquo;s really a manual step. Depending on the certificate validity, this task becomes tedious. Still the pros outweighed the cons for me.\nWireGuard for Everything Else # I\u0026rsquo;m the sole user of the other services that are running on my home server and, I\u0026rsquo;m fine accessing them via WireGuard. My router, a Fritz Box, can act as a WireGuard server out-of-the-box and that\u0026rsquo;s what I\u0026rsquo;m using.\nBut, as soon as I get to know that mTLS is supported for one of these services, I\u0026rsquo;ll make the switch as it is much more convenient.\nNetwork Architecture # This section shows the final network architecture for my home lab.\nNetwork Architecture Internal Traffic # Services that are not exposed to the Internet can be accessed through the normal Caddy container that runs on port 443. That is also true for devices that are connected via WireGuard.\nExternal Traffic Via Second Caddy Reverse Proxy # I added a second Caddy reverse proxy (caddy-pub) just for external services. This allows me to separate settings for external and internal services more easily. It also saves me from accidentally publishing services through my normal Caddy reverse proxy.\nFor the external services, I\u0026rsquo;ve set caddy-pub to listen on port 444 and configured forwarding from the public port 443 on my router to caddy-pub on port 444.\nFirewall Changes # Allow port 444 through your firewall on your home server. If you\u0026rsquo;ve followed my series, add the following to the home.yml variable file.\nufw_rules: - rule: allow to_port: \u0026#34;444\u0026#34; protocol: tcp DNS # To access your services from the Internet you need public DNS entries pointing to your public IP. But, the public IP assigned by your ISP changes. You need a dynamic DNS service to solve that problem. My Fritz Box router has this option built-in. If you don\u0026rsquo;t have a Fritz Box, IPv64.net is a service I can recommend.\nFor every service you want to expose, add a CNAME entry pointing to the dynamic DNS address.\nCNAME nextcloud.home.dominikbritz.de -\u0026gt; Your-DynDNS-String.myfritz.net caddy-pub Docker Container # Docker Preparations With Ansible # As described in my Immich and Nextcloud articles, you would rather not run the caddy-pub container as root as it is exposed to the Internet.\nCreate an Ansible role for caddy-pub to run with limited permissions like this.\nCompose File # Create the Caddy container like this. Below is my Docker compose file for reference.\nservices: caddy-pub: build: ./dockerfile-dns container_name: caddy-pub hostname: caddy-pub restart: unless-stopped ports: - 444:443 - 444:443/udp volumes: - ./Caddyfile:/etc/caddy/Caddyfile - ./data:/data - ./config:/config - /usr/local/data/certificates:/certificates:ro # Path to the CA and client certificates root folder networks: - caddy_caddynet networks: caddy_caddynet: external: true Create a Certificate Authority # You need to create a certificate authority (CA) on your home server. With the CA you can then issue certificates for your devices. On your devices, you install the device certificate as well as the CA certificate.\nThe following commands are optimized to provide all required input at the command line. There is no interactive input that annoys you. There are no config files you have to mess around with. All necessary steps are executed by a single OpenSSL invocation: from private key generation up to the self-signed certificate.\nThe first step is to install certtool which we require later for Android certificates. Use the command below or, better yet, add it to your base Ansible playbook.\napt install gnutls-bin Second, create some folders that hold your CA and client certificates. Make sure that it is the same path as in the compose file.\nmkdir -p /usr/local/data/certificates/ca mkdir -p /usr/local/data/certificates/clients Third, create the CA. Change the domain name accordingly.\ncd /usr/local/data/certificates/ca openssl req -x509 -newkey rsa:4096 -sha256 -days 3650 \\ -nodes -keyout home.dominikbritz.de.key -out home.dominikbritz.de.crt -subj \u0026#34;/CN=home.dominikbritz.de\u0026#34; \\ -addext \u0026#34;subjectAltName=DNS:home.dominikbritz.de,DNS:*.home.dominikbritz.de\u0026#34; Each of the above commands creates a certificate that is\nvalid for the domain home.dominikbritz.de, also valid for the wildcard domain *.home.dominikbritz.de, relatively strong but also highly compatible by using RSA with 4096 bits valid for 3650 days (~10 years). The following files are generated:\nPrivate key: home.dominikbritz.de.key Certificate: home.dominikbritz.de.crt Create the Client Certificates # For each of your users or each of your devices, depending on how complex you want to have it, run the following commands.\n# Nice oneliner that only prompts for a passphrase # Replace client1 and the CA accordingly # -days lets you specify the certificate validity cd /usr/local/data/certificates/clients openssl req -new -newkey rsa:4096 -keyout client1.key -subj \u0026#34;/C=DE/ST=State/L=Location/O=Organisation/CN=client1\u0026#34; | openssl x509 -req -CA ../ca/home.dominikbritz.de.crt -CAkey ../ca/home.dominikbritz.de.key -CAcreateserial -out client1.crt -days 3650 Android requires different hashes and ciphers. Run the below.\ncd /usr/local/data/certificates/clients certtool --load-privkey client1.key --load-certificate client1.crt \\ --load-ca-certificate ../ca/home.dominikbritz.de.crt \\ --to-p12 --outder --outfile client1-android.p12 \\ --p12-name \u0026#34;client1-android\u0026#34; \\ --hash SHA1 --pkcs-cipher 3des-pkcs12 --password YourPassword Caddy Configuration # Implementing mTLS with Caddy is a breeze. Add the following reusable block at the top of your Caddyfile (details).\n(mTLS) { client_auth { mode require_and_verify trusted_ca_cert_file /certificates/ca/home.dominikbritz.de.crt } } Then import it for your services in the tls statement. Here is an example for Nextcloud (details).\nnextcloud.{$MY_DOMAIN} { reverse_proxy nextcloud:80 redir /.well-known/carddav /remote.php/dav 301 redir /.well-known/caldav /remote.php/dav 301 redir /.well-known/webfinger /index.php/.well-known/webfinger redir /.well-known/nodeinfo /index.php/.well-known/nodeinfo tls { import mTLS dns netcup { customer_number {env.NETCUP_CUSTOMER_NUMBER} api_key {env.NETCUP_API_KEY} api_password {env.NETCUP_API_PASSWORD} } propagation_timeout 900s propagation_delay 600s resolvers 1.1.1.1 } import personal_headers } Instruct caddy-pub to reload its configuration by running:\ndocker exec -w /etc/caddy caddy-pub caddy reload Backup the Certificates # Add the certificates to your backup. If you\u0026rsquo;ve followed my series, add the following to data/resticprofile/profiles.yml (details).\nsource: - \u0026#34;/usr/local/data/certificates\u0026#34; Client Certificate Installation # Android # Install the CA and client certificate in the Android system store.\nCA certificate: Settings app -\u0026gt; Security \u0026amp; privacy -\u0026gt; More security settings -\u0026gt; Encryption \u0026amp; credentials -\u0026gt; Install certificate -\u0026gt; CA certificate Client certificate: Settings app -\u0026gt; Security \u0026amp; privacy -\u0026gt; More security settings -\u0026gt; Encryption \u0026amp; credentials -\u0026gt; Install certificate -\u0026gt; VPN \u0026amp; app user certificate Nextcloud # The Nextcloud Android app uses the certificate in the system store. There is nothing to configure.\nImmich # If you are already logged on, log off first. Go to Settings -\u0026gt; Advanced -\u0026gt; SSL Client Certificate -\u0026gt; Import and import the client1-android.p12 certificate. Log on again. Firefox/Chrome # While there are articles on the Internet claiming that they can use Firefox on Android with mTLS, I was unable to do so. Chrome works fine with certificates in the system store, though.\nSo if you have apps or services that you access through the browser only secured with mTLS, Chrome is your best bet on Android.\nWindows # Use PowerShell to import the certificates.\n# Import the server CA Import-Certificate -FilePath \u0026#34;home.dominikbritz.de.crt\u0026#34; -CertStoreLocation Cert:\\LocalMachine\\Root # Import the client TLS certificate and key Import-PfxCertificate -FilePath \u0026#34;client1.p12\u0026#34; -CertStoreLocation Cert:\\LocalMachine\\My ","date":"18 September 2024","externalUrl":null,"permalink":"/posts/secure-remote-access/","section":"Posts","summary":"This article explains how to securely access your home server and its services with mTLS and WireGuard.\nIt is part of my series on creating a home server on an old laptop. I’m assuming that you’ve set up Docker, Caddy, and Authelia, as described in the first article of the series.\n","title":"Secure Remote Home Server Access With mTLS and WireGuard","type":"posts"},{"content":"","date":"18 September 2024","externalUrl":null,"permalink":"/tags/wireguard/","section":"Tags","summary":"","title":"WireGuard","type":"tags"},{"content":"","date":"24 March 2024","externalUrl":null,"permalink":"/tags/immich/","section":"Tags","summary":"","title":"Immich","type":"tags"},{"content":"This article explains how to set up a self-hosted photo gallery with Immich including cloud backup and OpenID Connect to Authelia.\nIt is part of my series on creating a home server on an old laptop. I’m assuming that you’ve set up Docker, Caddy, and Authelia, as described in the first article of the series.\nTake Control Over Your Memories # Preserving photos for eternity is the ultimate challenge in a personal context. You want to make sure that your children, friends, and family photos will never be lost.\nIn the past, I have set up a cascade of external hard drives to store sensitive data, so that if one or more failed, a complete backup would always be available. However, there was always a nagging question in my mind that what if the house and all the hard drives caught fire?\nToday\u0026rsquo;s world has a lot of public cloud services that take care of this concern and give you the peace of mind that your pics are safe, no matter what. Google Photos is the market leader, and rightly so, as the app works exceptionally well — especially the facial recognition and memories features that have become so popular in our household.\nBut, that comes with a cost: you are no longer in control over your most precious data. I wanted my control back and started searching for alternatives.\nPhoto Gallery Requirements # Picking out a photo gallery is important, as you intend to keep it for a long time. Below is my list of criteria:\nSelf-hosted: must be running on my home server and not in the cloud Actively maintained: when Google started charging money for Google Photos, the open-source community quickly started creating alternatives. Some projects started out promising, but were abandoned quickly. I want a solution with a big community and active developers that will be here in a few years. Open-source: Synology, the vendor of my former home server, has a viable solution that ticks all boxes, but you have no influence on the development, nor can you see the current state or a roadmap. I want a solution where I can share ideas, discuss features, and see where the project is going to. Native app with upload functionality: a native app for mobile phones that is easy to use and uploads photos automatically to my home server. Using an additional app like Syncthing does not adhere to the WAF (wife acceptance factor). Facial recognition: group photos by people, place, and time Multi-user and sharing: I\u0026rsquo;m not the sole user of the service. At least my wife is using it, too. The service must have multi-user support and excellent sharing functionality. Memories: present memories from the past years in the mobile app. That\u0026rsquo;s a hard non-negotiable factor in my family. OIDC support: support single sign-on through OIDC with Authelia Photo Gallery Selection # Below is a list of galleries that I\u0026rsquo;ve checked against my requirements. For a more detailed comparison, refer to this table.\nImmich: ticks all boxes, but is under active development. There may be breaking changes along the way, according to their website.\nLibrephotos: no native automatic mobile upload\nNextcloud and Nextcloud Memories: I use Nextcloud for files (details), so using it for photos as well is close. But I think that software should concentrate on a few things and then do them really well, rather than wanting to be a jack of all trades.\nPhotoprism: it\u0026rsquo;s a progressive web app. Uploads are handled through a third-party app. Also, certain features are only available on paid plans.\nFrom the above, Immich and Nextcloud have all the features I require. I prefer Immich\u0026rsquo;s design over Nextcloud\u0026rsquo;s though, as Immich looks nearly identical to Google Photos. After some research, it turned out that \u0026ldquo;breaking changes\u0026rdquo; in Immich means that you have to change something here and there in the Docker compose file and update it carefully. I have the confidence to do that. If that doesn\u0026rsquo;t suit you, you\u0026rsquo;re probably better off with Nextcloud.\nBeautiful Immich UI Immich Installation in Docker Container # Preparation # We have to prepare a few things before we can create the stack.\nLimited User And Group # As with Nextcloud, Immich should be running with limited permissions. Hence, we run this stack with the dockerlimited user (more info).\nFiles And Folders # In your WSL client, create a new Ansible role for Immich.\nmkdir -p roles/immich/tasks vim roles/immich/tasks/main.yml The playbook below is similar to my Nextcloud one, and one could argue that creating the dockerlimited user again makes no sense. But, that\u0026rsquo;s the beauty of Ansible\u0026rsquo;s idempotent nature: if the user already exists, it skips the step.\n# main.yml # Ansible role for Immich - name: Create user dockerlimited with UID 2000 user: name: dockerlimited # change to your preferred username uid: 2000 # change to your preferred UID - name: Create directories for Immich file: path: \u0026#34;/usr/local/data/docker/{{ item }}\u0026#34; # change to your preferred directory state: directory owner: dockerlimited group: dockerlimited mode: 0755 # owner has full read, write, and execute permissions, while the group and others have read and execute permissions, but no write permissions loop: - immich/data # upload storage - name: Create compose.yaml file file: path: \u0026#34;/usr/local/data/docker/immich/compose.yaml\u0026#34; state: touch owner: dockerlimited group: dockerlimited mode: 0644 # Owner has read and write permissions, while the group and others have read permissions Add the new role to the home.yml playbook in the root:\n--- - name: home server hosts: localhost become: true vars_files: - vars_files/home.yml roles: - base - home - oefenweb.ufw - geerlingguy.docker - geerlingguy.ntp - geerlingguy.security - geerlingguy.node_exporter - nextcloud - immich Push the changes to GitHub (details).\ngitall Deploy With Ansible # On your home server, deploy the just created Immich role.\nexport PAT=YOUR_PAT_TOKEN sudo ansible-pull -U https://$PAT:x-oauth-basic@github.com/Username/repo-name home.yml Compose File # In Dockge, click Scan Stacks Folder in the user menu at the top right. The immich stack should appear in the list.\nAt the time I was writing this article, my Immich server was on version 1.99. The compose file below is valid for this version. As Immich is under active development, the contents of the compose file change. Please check https://github.com/immich-app/immich/releases/latest/download/docker-compose.yml for the latest compose file.\nNote the following lines:\nlabels: - com.centurylinklabs.watchtower.enable=false I update all my Docker images with Watchtower every night. Immich must be excluded with com.centurylinklabs.watchtower.enable=false, as sometimes changes to the compose file are required when you go to the next version. I update Immich manually.\nuser: 2000:2000 This line is not in the original compose file. Change it to the IDs of your dockerlimited user.\nversion: \u0026#34;3.8\u0026#34; # # WARNING: Make sure to use the docker-compose.yml of the current release: # # https://github.com/immich-app/immich/releases/latest/download/docker-compose.yml # # The compose file on main may not be compatible with the latest release. # name: immich services: immich-server: container_name: immich_server hostname: photos user: 2000:2000 image: ghcr.io/immich-app/immich-server:${IMMICH_VERSION:-release} command: - start.sh - immich volumes: - ${UPLOAD_LOCATION}:/usr/src/app/upload - /etc/localtime:/etc/localtime:ro env_file: - .env expose: - 3001 depends_on: - redis - database restart: unless-stopped networks: - caddy_caddynet labels: - com.centurylinklabs.watchtower.enable=false immich-microservices: container_name: immich_microservices user: 2000:2000 image: ghcr.io/immich-app/immich-server:${IMMICH_VERSION:-release} # extends: # file: hwaccel.yml # service: hwaccel command: - start.sh - microservices volumes: - ${UPLOAD_LOCATION}:/usr/src/app/upload - /etc/localtime:/etc/localtime:ro env_file: - .env depends_on: - redis - database restart: unless-stopped networks: - caddy_caddynet labels: - com.centurylinklabs.watchtower.enable=false immich-machine-learning: container_name: immich_machine_learning image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_VERSION:-release} volumes: - model-cache:/cache env_file: - .env restart: unless-stopped networks: - caddy_caddynet labels: - com.centurylinklabs.watchtower.enable=false redis: container_name: immich_redis image: redis:6.2-alpine@sha256:b6124ab2e45cc332e16398022a411d7e37181f21ff7874835e0180f56a09e82a restart: unless-stopped environment: [] networks: - caddy_caddynet database: container_name: immich_postgres image: tensorchord/pgvecto-rs:pg14-v0.2.0@sha256:90724186f0a3517cf6914295b5ab410db9ce23190a2d9d0b9dd6463e3fa298f0 env_file: - .env environment: POSTGRES_PASSWORD: ${DB_PASSWORD} POSTGRES_USER: ${DB_USERNAME} POSTGRES_DB: ${DB_DATABASE_NAME} volumes: - pgdata:/var/lib/postgresql/data restart: unless-stopped networks: - caddy_caddynet labels: - com.centurylinklabs.watchtower.enable=false volumes: pgdata: null model-cache: null networks: caddy_caddynet: external: true env File # The .env file holds configurations for the Immich stack and can be edited in the Dockge UI directly.\nGenerate a random alphanumeric string to be used as database password:\ntr -cd \u0026#39;[:alnum:]\u0026#39; \u0026lt; /dev/urandom | fold -w \u0026#34;32\u0026#34; | head -n 1 # You can find documentation for all the supported env variables at https://immich.app/docs/install/environment-variables # The location where your uploaded files are stored UPLOAD_LOCATION=/media/18TB/photos/library # The Immich version to use. You can pin this to a specific version like \u0026#34;v1.71.0\u0026#34; IMMICH_VERSION=release # Connection secret for postgres. You should change it to a random password DB_PASSWORD=your_password # The values below this line do not need to be changed ################################################################################### DB_HOSTNAME=immich_postgres DB_USERNAME=postgres DB_DATABASE_NAME=immich REDIS_HOSTNAME=immich_redis Let’s Encrypt Certificate via Caddy # Caddyfile # Add the following to Caddyfile (details):\nphotos.{$MY_DOMAIN} { reverse_proxy immich:3001 tls { dns netcup { customer_number {env.NETCUP_CUSTOMER_NUMBER} api_key {env.NETCUP_API_KEY} api_password {env.NETCUP_API_PASSWORD} } propagation_timeout 900s propagation_delay 600s resolvers 1.1.1.1 } } DNS A Record # Add the following A record to your DNS domain:\nphotos.home.yourdomain.com 192.168.178.254 # replace with your Docker host\u0026#39;s IP address Reload Caddy’s Configuration # Instruct Caddy to reload its configuration by running:\ndocker exec -w /etc/caddy caddy caddy reload Immich is now accessible via https://photos.home.yourdomain.com.\nPost Install Steps # Follow the official post install steps to:\nCreate your user as the admin user. Create additional users (optional) Update the storage template Download and set up the mobile app SSO to Immich via OpenID Connect (OIDC) Authentication to Authelia # This section describes how to set up single sign-on to Immich via OpenID Connect authentication to Authelia.\nClient Secret # Generate a random alphanumeric string to be used as client secret:\ntr -cd \u0026#39;[:alnum:]\u0026#39; \u0026lt; /dev/urandom | fold -w \u0026#34;64\u0026#34; | head -n 1 Authelia Configuration File # Add the following to the oidc: section of Authelia’s configuration file config/configuration.yml:\nclients: - id: immich description: immich secret: \u0026#39;your secret\u0026#39; scopes: - openid - profile - email redirect_uris: - https://photos.home.yourdomain.com/auth/login - https://photos.home.yourdomain.com/user-settings - app.immich:/ Restart Authelia # We changed the container’s environment, which makes it necessary to recreate the container (stopping and starting is not enough). Navigate to the Authelia stack in Dockge and click Stop \u0026amp; Inactive. Then start the stack.\nInspect Dockge\u0026rsquo;s terminal window for any errors.\nImmich Configuration # In Immich, go to Administration → Settings → OAuth Authentication and configure the following:\nEnable: yes Issuer URL: https://authelia.home.yourdomain.com Client ID: Immich Client secret: your secret Scope: openid email profile Signing algorithm: RS256 Storage label claim: preferred_username Storage quota claim: immich_quota Default storage quota: you may define a quota here Auto register: yes Auto launch: yes Mobile redirect uri override: no Restart the Immich stack through Dockge. Log in with your user via Authelia.\nMigrate from Google Photos # The community created a command-line tool to import your Google photos into Immich.\nExport your Google photos with Google Takeout. Read the best practices first. Import the exported zip files with immich-go Migrate from Synology Photos # I had many photos stored in a directory structure containing years and event names (like 2024\\2024-01-01 happy new year) on my old Synology NAS, which I moved to Immich as well. While I moved off a Synology, the instructions below can be used for any folder-based structure.\nClean the Import # I did not do a great job organizing my photos on the Synology and had numerous duplicates. I took the migration to Immich as an opportunity to fix that.\nCzkawka was very helpful in cleaning my files before importing.\nFile Copy Operations # Mount the Synology photo share and copy the files to your home server.\n# Mount the Synology share on my home server mkdir /mnt/synology_photos mount -t cifs -o user=username //synology/photo /mnt/synology_photos # Copy the files to my external media drive mkdir /media/18TB/photos/from_synology rsync -avh /mnt/synology_photos/ /media/18TB/photos/from_synology Update The Compose File # Add the from_synology folder as a read-only volume to the immich-server and immich-microservices services in the compose.yaml.\nvolumes: - /media/18TB/photos/from_synology:/mnt/media/from_synology:ro Add External Library in Immich # External libraries in Immich are perfect for adding existing photos. From their documentation:\nExternal libraries tracks assets stored outside of immich, i.e. in the file system. Immich will only read data from the files, and will not modify them in any way. Therefore, the delete button is disabled for external assets. When the external library is scanned, immich will read the metadata from the file and create an asset in the library for each image or video file. These items will then be shown in the main timeline, and they will look and behave like any other asset, including viewing on the map, adding to albums, etc.\nGo to Administration → External Libraries and add /mnt/media/from_synology as a library.\nAfter Immich has processed all files, they appear in your timeline.\nCloud Backup # A good backup is crucial for photos. I back up my data encrypted to the cloud with resticprofile.\nAccording to the Immich documentation, one must back up the database and a few folders in the file system.\nDatabase # Immich added automatic database backups with version 1.120, which are enabled by default. I prefer my own backups and disabled the setting in Administration \u0026gt; Settings \u0026gt; Backup Settings. Add the pg_dumpall command to the run-before section of your resticprofile configuration in /usr/local/data/resticprofile/profiles.yaml.\nrun-before: - \u0026#34;docker exec -t immich_postgres pg_dumpall --clean --if-exists --username=postgres | gzip \u0026gt; /usr/local/data/docker/immich/db_dump.sql.gz\u0026#34; - \u0026#34;systemctl stop docker.socket\u0026#34; - \u0026#34;systemctl stop docker\u0026#34; Verify that the Docker folder is part of the source section.\nsource: - \u0026#34;/usr/local/data/docker\u0026#34; File System # If you place the uploaded photos in the stack folder, i.e., /usr/local/data/docker/immich, add /usr/local/data/docker/immich/thumbs and /usr/local/data/docker/immich/encoded-video to the excludes file (more info).\nIf you don\u0026rsquo;t place the uploaded photos in the stack folder, like me, add the following paths to the source section. Replace UPLOAD_LOCATION with the path set in your .env file.\nUPLOAD_LOCATION/library UPLOAD_LOCATION/upload UPLOAD_LOCATION/profile Also, add any external library.\nHere is my source section in the profiles.yaml:\nsource: - \u0026#34;/usr/local/data/docker\u0026#34; - \u0026#34;/usr/local/data/resticprofile\u0026#34; - \u0026#34;/media/18TB/photos/library/library\u0026#34; - \u0026#34;/media/18TB/photos/library/upload\u0026#34; - \u0026#34;/media/18TB/photos/library/profile\u0026#34; - \u0026#34;/media/18TB/photos/from_synology\u0026#34; - \u0026#34;/etc/unbound/unbound.conf\u0026#34; Changelog # 2024-11-07 # Added a note about automatic database backups to the database section. ","date":"24 March 2024","externalUrl":null,"permalink":"/posts/immich-self-hosted-photos-with-cloud-backup/","section":"Posts","summary":"This article explains how to set up a self-hosted photo gallery with Immich including cloud backup and OpenID Connect to Authelia.\nIt is part of my series on creating a home server on an old laptop. I’m assuming that you’ve set up Docker, Caddy, and Authelia, as described in the first article of the series.\n","title":"Immich: Self-Hosted Photo Gallery With Cloud Backup \u0026 OpenID Connect","type":"posts"},{"content":"","date":"24 March 2024","externalUrl":null,"permalink":"/tags/openid-connect/","section":"Tags","summary":"","title":"OpenID Connect","type":"tags"},{"content":"","date":"5 February 2024","externalUrl":null,"permalink":"/tags/nextcloud/","section":"Tags","summary":"","title":"Nextcloud","type":"tags"},{"content":"This article explains how to set up a fast and efficient Nextcloud as file management tool with OpenID Connect to Authelia.\nIt is part of my series on creating a home server on an old laptop. I\u0026rsquo;m assuming that you\u0026rsquo;ve set up Docker, Caddy, and Authelia, as described in the first article of the series.\nWhy Nextcloud? # My previous home server was a Synology NAS, that came with a file management/syncing solution, which worked pretty well. It has all the nice features like selective sync, sharing, requesting files, and it was fast and fun to work with. Before I moved to a custom home server, I did some research on possible solutions with similar features and came up with ownCloud and Nextcloud.\nownCloud vs. Nextcloud # ownCloud and Nextcloud used to be one solution but then went in different directions. In the first article of the series, I\u0026rsquo;ve written that my home server is based on Helge Klein\u0026rsquo;s series on home automation and if you are following his series, you know that he decided to go with ownCloud Infinite Scale (oCIS) instead of Nextcloud. I tested ownCloud, but was not convinced.\nBlob Storage # ownCloud kidnaps your files and stores them in blob storage format. You can\u0026rsquo;t access your files in the classic way by browsing the file system. I understand the technical background, but I don\u0026rsquo;t like the fact that I can\u0026rsquo;t access my data directly, but always have to go through an ownCloud client.\nExternal Storage # oCIS can\u0026rsquo;t integrate other storage types like WebDAV, not even a folder on your local disks.\nCommunity # oCIS is relatively new compared to Nextcloud and there doesn\u0026rsquo;t seem to be a huge community. I had some issues while testing ownCloud and if you search for something on Google but get exactly 0 results, that makes me wonder.\nNextcloud on the other hand has a large community and you can be sure that every question you have is already answered somewhere on the Internet.\nStreamlining Nextcloud # I only need Nextcloud to synchronise files. But the makers seem to think it\u0026rsquo;s a good idea to pack everything that can be programmed into Nextcloud. It\u0026rsquo;s not for nothing that Nextcloud has a reputation for eating up plenty of resources.\nTo top it all off, there is a dedicated Nextcloud All-In-One project including antivirus, backup, and image updates, to name just a few. Ridiculously, it\u0026rsquo;s the recommended method to deploy Nextcloud.\nMy Nextcloud build, instead, is focused on the core functionality, file syncing, as well as performance. When you need additional features, you can simply turn them on.\nNextcloud Installation in Docker Container # Preparation # We have to prepare a few things before we can create the stack.\nLimited User And Group # Docker is running as root and so are the containers. If the image supports it, you can specify another user and the container will access the file system with that user.\nNextcloud is one of the services I want to be accessible externally (future article) hence running it with root permissions is a no-go.\nStatic User ID # Users and groups get IDs on Ubuntu. System users get an ID below 1000. Your user most likely has the ID 1000 as you were the first user on the machine. When you create a user, Ubuntu assignes the next available ID. For our limited user we want something static and something that\u0026rsquo;s not already taken on the current machine and on any other machine you might deploy in the future. We want that because the restic backup includes permissions based on IDs. So when you restore your backup on a new machine, the ID of the user in the backup and the one on the new machine must match to be able to access files.\n2000 is easy to remember and on a home server with few users most likely free. If you want to be sure, you can check the allocated IDs first.\nsudo cut -d: -f1,3 /etc/passwd Files And Folders # In your WSL client, create a new Ansible role for Nextcloud.\nmkdir -p roles/nextcloud/tasks vim roles/nextcloud/tasks/main.yml # main.yml # Ansible role for Nextcloud - name: Create user dockerlimited with UID 2000 user: name: dockerlimited # change to your preferred username uid: 2000 # change to your preferred UID - name: Create directories for Nextcloud file: path: \u0026#34;/usr/local/data/docker/{{ item }}\u0026#34; # change to your preferred directory state: directory owner: dockerlimited group: dockerlimited mode: 0755 # owner has full read, write, and execute permissions, while the group and others have read and execute permissions, but no write permissions loop: - nextcloud/secrets # files containing secrets for Nextcloud - nextcloud/config # config for Nextcloud - nextcloud/data # your files for Nextcloud - name: Create compose.yaml file file: path: \u0026#34;/usr/local/data/docker/nextcloud/compose.yaml\u0026#34; state: touch owner: dockerlimited group: dockerlimited mode: 0644 # Owner has read and write permissions, while the group and others have read permissions # generate a random 32 character alphanumeric string for the Nextcloud database password and the Nextcloud database root password. # 32 is the max for MyQSL. Reduce the length if you run in to issues. - name: Generate MySQL password for Nextcloud shell: \u0026#34;tr -cd \u0026#39;[:alnum:]\u0026#39; \u0026lt; /dev/urandom | fold -w \u0026#39;32\u0026#39; | head -n 1 \u0026gt; /usr/local/data/docker/nextcloud/secrets/MYSQL_PASSWORD\u0026#34; args: creates: \u0026#34;/usr/local/data/docker/nextcloud/secrets/MYSQL_PASSWORD\u0026#34; # run only if not exists - name: Generate MySQL root password for Nextcloud shell: \u0026#34;tr -cd \u0026#39;[:alnum:]\u0026#39; \u0026lt; /dev/urandom | fold -w \u0026#39;32\u0026#39; | head -n 1 \u0026gt; /usr/local/data/docker/nextcloud/secrets/MYSQL_ROOT_PASSWORD\u0026#34; args: creates: \u0026#34;/usr/local/data/docker/nextcloud/secrets/MYSQL_ROOT_PASSWORD\u0026#34; # run only if not exists Add the new role to the home.yml playbook in the root:\n--- - name: home server hosts: localhost become: true vars_files: - vars_files/home.yml roles: - base - home - oefenweb.ufw - geerlingguy.docker - geerlingguy.ntp - geerlingguy.security - geerlingguy.node_exporter - nextcloud Push the changes to GitHub (details).\ngitall Deploy With Ansible # On your home server, deploy the just created Nextcloud role.\nexport PAT=YOUR_PAT_TOKEN sudo ansible-pull -U https://$PAT:x-oauth-basic@github.com/Username/repo-name home.yml Compose File # If available, I prefer Docker images from linuxserver.io, as they are updated regularly with the latest security patches, the documentation is excellent, and they support using different user IDs than root. The latter is crucial to run the container with the dockerlimited user.\nIn Dockge, click Scan Stacks Folder in the user menu top right. The nextcloud stack should appear in the list. Paste the following contents in the compose.yaml section:\nservices: nextcloud: image: lscr.io/linuxserver/nextcloud:latest container_name: nextcloud hostname: nextcloud volumes: - ./config:/config - ./data:/data # the folder where files are placed - /media/18TB:/media # add as many local folders as you like and access/share them via nextcloud. Remember that UID=2000 needs read/write permissions. expose: - 80 # default is 443 with a self-signed certificate. We use 80 and create the certificate through Caddy. restart: unless-stopped networks: - caddy_caddynet mariadb: image: lscr.io/linuxserver/mariadb:latest container_name: nextcloud-mariadb hostname: nextcloud-mariadb volumes: - ./db-config:/config expose: - 3306 restart: unless-stopped networks: - caddy_caddynet redis: container_name: nextcloud-redis hostname: nextcloud-redis image: redis:latest restart: unless-stopped networks: - caddy_caddynet expose: - 6379 volumes: - ./redis-data:/data networks: caddy_caddynet: external: true Add the following variables to the env section:\nPUID=2000 # UID of the dockerlimited user PGID=2000 # GID of the dockerlimited group TZ=Europe/Berlin # Your timezone MYSQL_DATABASE=nextcloud MYSQL_USER=nextcloud FILE__MYSQL_PASSWORD=/secrets/MYSQL_PASSWORD FILE__MYSQL_ROOT_PASSWORD=/secrets/MYSQL_ROOT_PASSWORD Deploy the stack through the Dockge UI. Check the terminal window for any errors and verify that the container is running with your UID.\nnextcloud | ─────────────────────────────────────── nextcloud | nextcloud | ██╗ ███████╗██╗ ██████╗ nextcloud | ██║ ██╔════╝██║██╔═══██╗ nextcloud | ██║ ███████╗██║██║ ██║ nextcloud | ██║ ╚════██║██║██║ ██║ nextcloud | ███████╗███████║██║╚██████╔╝ nextcloud | ╚══════╝╚══════╝╚═╝ ╚═════╝ nextcloud | nextcloud | Brought to you by linuxserver.io nextcloud | ─────────────────────────────────────── nextcloud | nextcloud | To support LSIO projects visit: nextcloud | https://www.linuxserver.io/donate/ nextcloud | nextcloud | ─────────────────────────────────────── nextcloud | GID/UID nextcloud | ─────────────────────────────────────── nextcloud | nextcloud | User UID: 2000 nextcloud | User GID: 2000 nextcloud | ─────────────────────────────────────── Let’s Encrypt Certificate via Caddy # Caddyfile # Add the following to Caddyfile (details):\nnextcloud.{$MY_DOMAIN} { reverse_proxy nextcloud:80 redir /.well-known/carddav /remote.php/dav 301 redir /.well-known/caldav /remote.php/dav 301 redir /.well-known/webfinger /index.php/.well-known/webfinger redir /.well-known/nodeinfo /index.php/.well-known/nodeinfo tls { import mTLS dns netcup { customer_number {env.NETCUP_CUSTOMER_NUMBER} api_key {env.NETCUP_API_KEY} api_password {env.NETCUP_API_PASSWORD} } propagation_timeout 900s propagation_delay 600s resolvers 1.1.1.1 } import personal_headers } DNS A Record # Add the following A record to your DNS domain:\nnextcloud.home.yourdomain.com 192.168.178.254 # replace with your Docker host\u0026#39;s IP address Reload Caddy’s Configuration # Instruct Caddy to reload its configuration by running:\ndocker exec -w /etc/caddy caddy caddy reload Nextcloud Setup # Browse to nextcloud.home.yourdomain.com and fill out the necessary fields in the setup screen. Take the database password from the /secrets/MYSQL_PASSWORD file.\nNextcloud setup If you can\u0026rsquo;t log in with your admin credentials after the setup, try logging in with a private browser window.\nChoose Skip when Nextclouds asks you to install recommended apps.\nOn the top right, click the user icon and choose Apps in the menu. Disable all apps except the ones on the list below.\nComments Deleted files File sharing Log Reader Notifications Password policy PDF viewer Privacy Text Versions In the list of apps find and install the apps External storage support, Group folders, and OpenID Connect Login.\nShut down the stack.\nBest Practice Settings # Open Nextcloud\u0026rsquo;s configuration file.\nvim /usr/local/data/docker/nextcloud/config/www/nextcloud/config/config.php Add the following settings to the $CONFIG = array ( block.\n// disable upgrade notifications in nextcloud web \u0026#39;upgrade.disable-web\u0026#39; =\u0026gt; true, // disable \u0026#34;welcome\u0026#34; files for new users \u0026#39;skeletondirectory\u0026#39; =\u0026gt; \u0026#39;\u0026#39;, \u0026#39;templatedirectory\u0026#39; =\u0026gt; \u0026#39;\u0026#39;, // route users directly to the files view \u0026#39;defaultapp\u0026#39; =\u0026gt; \u0026#39;files\u0026#39;, // language \u0026#39;default_language\u0026#39; =\u0026gt; \u0026#39;de\u0026#39;, // delete all files in the trash bin that are older than 3 days automatically, delete other files anytime if space needed \u0026#39;trashbin_retention_obligation\u0026#39; =\u0026gt; \u0026#39;auto, 3\u0026#39;, E-Mail Settings # I\u0026rsquo;ve configured Gmail as my SMTP server. Follow these instructions to activate Gmail SMTP.\n// config.php // mail \u0026#39;mail_smtpmode\u0026#39; =\u0026gt; \u0026#39;smtp\u0026#39;, \u0026#39;mail_sendmailmode\u0026#39; =\u0026gt; \u0026#39;smtp\u0026#39;, \u0026#39;mail_smtpauth\u0026#39; =\u0026gt; 1, \u0026#39;mail_smtphost\u0026#39; =\u0026gt; \u0026#39;smtp.gmail.com\u0026#39;, \u0026#39;mail_smtpport\u0026#39; =\u0026gt; \u0026#39;587\u0026#39;, \u0026#39;mail_from_address\u0026#39; =\u0026gt; \u0026#39;your email without the @gmail.com\u0026#39;, \u0026#39;mail_domain\u0026#39; =\u0026gt; \u0026#39;gmail.com\u0026#39;, \u0026#39;mail_smtpname\u0026#39; =\u0026gt; \u0026#39;your full gmail address\u0026#39;, \u0026#39;mail_smtppassword\u0026#39; =\u0026gt; \u0026#39;your smtp password\u0026#39;, SSO to Nextcloud via OpenID Connect (OIDC) Authentication to Authelia # This section describes how to set up single sign-on to Nextcloud via OpenID Connect authentication to Authelia. It is based on the Authelia Nextcloud integration guide.\nClient Secret # Generate a random alphanumeric string to be used as client secret:\ntr -cd \u0026#39;[:alnum:]\u0026#39; \u0026lt; /dev/urandom | fold -w \u0026#34;64\u0026#34; | head -n 1 Authelia Configuration File # Add the following to the oidc: section of Authelia’s configuration file config/configuration.yml:\nclients: - id: nextcloud description: Nextcloud secret: \u0026#39;your secret\u0026#39; public: false redirect_uris: - https://nextcloud.home.yourdomain.com/apps/oidc_login/oidc scopes: - openid - profile - email userinfo_signing_algorithm: none require_pkce: true pkce_challenge_method: \u0026#39;S256\u0026#39; Differing from the original guide I removed the groups scope, as it erased all group memberships in Nextcloud after every log off.\nRestart Authelia # We changed the container’s environment, which makes it necessary to recreate the container (stopping and starting is not enough). Navigate to the Authelia stack in Dockge and click Stop \u0026amp; Inactive. Then start the stack.\nInspect Dockge\u0026rsquo;s terminal window for any errors.\nNextcloud Configuration File # Open the config file again.\nvim /usr/local/data/docker/nextcloud/config/www/nextcloud/config/config.php Add the following:\n// OIDC \u0026#39;allow_user_to_change_display_name\u0026#39; =\u0026gt; false, \u0026#39;lost_password_link\u0026#39; =\u0026gt; \u0026#39;disabled\u0026#39;, \u0026#39;oidc_login_provider_url\u0026#39; =\u0026gt; \u0026#39;https://authelia.home.yourdomain.com\u0026#39;, \u0026#39;oidc_login_client_id\u0026#39; =\u0026gt; \u0026#39;nextcloud\u0026#39;, \u0026#39;oidc_login_client_secret\u0026#39; =\u0026gt; \u0026#39;your secret\u0026#39;, \u0026#39;oidc_login_auto_redirect\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_end_session_redirect\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_button_text\u0026#39; =\u0026gt; \u0026#39;SSO\u0026#39;, \u0026#39;oidc_login_hide_password_form\u0026#39; =\u0026gt; true, \u0026#39;oidc_login_use_id_token\u0026#39; =\u0026gt; true, \u0026#39;oidc_login_attributes\u0026#39; =\u0026gt; array ( \u0026#39;id\u0026#39; =\u0026gt; \u0026#39;preferred_username\u0026#39;, \u0026#39;name\u0026#39; =\u0026gt; \u0026#39;name\u0026#39;, \u0026#39;mail\u0026#39; =\u0026gt; \u0026#39;email\u0026#39;, ), \u0026#39;oidc_login_default_group\u0026#39; =\u0026gt; \u0026#39;users\u0026#39;, \u0026#39;oidc_login_use_external_storage\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_scope\u0026#39; =\u0026gt; \u0026#39;openid profile email\u0026#39;, \u0026#39;oidc_login_proxy_ldap\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_disable_registration\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_redir_fallback\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_alt_login_page\u0026#39; =\u0026gt; \u0026#39;assets/login.php\u0026#39;, \u0026#39;oidc_login_tls_verify\u0026#39; =\u0026gt; true, \u0026#39;oidc_create_groups\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_webdav_enabled\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_password_authentication\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_public_key_caching_time\u0026#39; =\u0026gt; 86400, \u0026#39;oidc_login_min_time_between_jwks_requests\u0026#39; =\u0026gt; 10, \u0026#39;oidc_login_well_known_caching_time\u0026#39; =\u0026gt; 86400, \u0026#39;oidc_login_update_avatar\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_code_challenge_method\u0026#39; =\u0026gt; \u0026#39;S256\u0026#39; Start the stack through Dockge.\nClick on the new SSO button below the login field and log in with your user via Authelia. The account gets auto-provisioned in Nextcloud. Logout directly after that.\nNew SSO button Log in with the admin one last time. In the menu top right, click Users and add your user to the admins group.\nWe can now disable normal user login. Open the config file yet again.\nvim /usr/local/data/docker/nextcloud/config/www/nextcloud/config/config.php Hide the normal login form and only present the SSO button.\n\u0026#39;auth.webauthn.enabled\u0026#39; =\u0026gt; false, \u0026#39;hide_login_form\u0026#39; =\u0026gt; true, \u0026#39;simpleSignUpLink.shown\u0026#39; =\u0026gt; false, Restart the stack through Dockge.\nAt the time of writing this article, Nextcloud presents an ugly warning message. It is a known issue and the team is working on it.\nUgly warning message Troubleshooting SSO # After upgrading to Nextcloud 31, I had issues with the SSO login. The login page would not show the SSO button. The update had marked the SSO app as incompatible. Enabling the app manually in the Nextcloud UI was not possible. I had to enable it via the command line.\nsudo docker exec -it nextcloud occ app:enable oidc_login After enabling the app, the SSO button appeared again.\nMemory Cache # To reduce the load on the database, we introduce a Redis memory cache. If you have set up Authelia like me, do not use the existing Redis container for Nextcloud. I had issues signing in with Authelia when it shared its Redis container with Nextcloud.\nOpen the config file a last time and replace the existing memory cache with the new Redis container.\nvim /usr/local/data/docker/nextcloud/config/www/nextcloud/config/config.php // config.php \u0026#39;memcache.local\u0026#39; =\u0026gt; \u0026#39;\\\\OC\\\\Memcache\\\\Redis\u0026#39;, \u0026#39;filelocking.enabled\u0026#39; =\u0026gt; true, \u0026#39;memcache.locking\u0026#39; =\u0026gt; \u0026#39;\\\\OC\\\\Memcache\\\\Redis\u0026#39;, \u0026#39;redis\u0026#39; =\u0026gt; array ( \u0026#39;host\u0026#39; =\u0026gt; \u0026#39;nextcloud-redis\u0026#39;, \u0026#39;port\u0026#39; =\u0026gt; 6379, ), Restart the Nextcloud stack and check Dockge\u0026rsquo;s terminal window for errors.\nExternal Storage \u0026amp; Group Folders # Go to the user menu top right and choose Administration settings.\nExternal Storage # The External storage app allows you to map FTP, WebDAV, and more options into the Nextcloud interface for easy access.\nI\u0026rsquo;ve used the type local to get access to my external drive in my Nextcloud client apps. I can easily share anything I have through the Nextcloud UI with others.\nExternal storage in Nextcloud Group Folders # Group folders are much like Spaces in ownCloud oCIS. They appear at the root for each user and are perfect for families to share common documents and pictures. One can assign granular permissions per folder.\nGroup folders in Nextcloud Changelog # 2025-03-21: modified the Authelia setup. After updating to Authelia 4.39 I saw the message Can not get identifier from provider and I was unable to login. I had to add require_pkce: true and pkce_challenge_method: 'S256' to the Authelia configuration file. Also, I needed to add 'oidc_login_code_challenge_method' =\u0026gt; 'S256' to the Nextcloud configuration file. 2025-03-01: added the section Troubleshooting SSO. 2024-02-08: added the creates parameter to the Nextcloud playbook in the files and folders section. ","date":"5 February 2024","externalUrl":null,"permalink":"/posts/streamlined-nextcloud-with-openid-connect-authentication/","section":"Posts","summary":"This article explains how to set up a fast and efficient Nextcloud as file management tool with OpenID Connect to Authelia.\nIt is part of my series on creating a home server on an old laptop. I’m assuming that you’ve set up Docker, Caddy, and Authelia, as described in the first article of the series.\n","title":"Streamlined Nextcloud With OpenID Connect Authentication","type":"posts"},{"content":"","date":"23 January 2024","externalUrl":null,"permalink":"/tags/ansible/","section":"Tags","summary":"","title":"Ansible","type":"tags"},{"content":"","date":"23 January 2024","externalUrl":null,"permalink":"/tags/caddy/","section":"Tags","summary":"","title":"Caddy","type":"tags"},{"content":"A guide to setting up an old laptop as your new home server, powered by Ubuntu, Ansible, and Docker. It securely hosts videos, files, and photos. Add HTTPS everywhere, cloud backup and performance monitoring.\nBye Bye Synology, Hello Ubuntu # TL;DR # Synology has a great ecosystem, but one is locked in. I\u0026rsquo;m using Ubuntu LTS server now, as it gives full control and is focused on stability. I chose it over Proxmox for simplicity, as I don\u0026rsquo;t need virtual machines.\nFull Story # I had a home server for years. I started with a Synology DS210+ thirteen years ago and upgraded to a DS720+ three years ago. The latter allowed running Docker containers and extend the feature set of Synology packages easily. With time, I noticed that I was using only a few Synology packages, namely photos and drive, but the rest was already running in Docker.\nIn addition, I have these problems with Synology:\nIt is built to hold 3,5” spinning disks. These are fine for storage, but not for performance. You can add SSDs as a read cache, but the community is divided about the benefits. The CPUs are crap. The DS720+ comes with an Intel Celeron J4125\u0026hellip; Synology OS can\u0026rsquo;t map external drives in a storage pool. When you add a drive with files to a storage pool, everything is lost as the drive needs to be formatted. Frustrating\u0026hellip; In addition to the point above: I had two volumes and a degraded RAID configuration that I could only solve with manual hacking through ssh, which is frustrating (again!) as Synology OS is a very lightweight Linux built. There is no package manager, for example. It\u0026rsquo;s a viable solution, but not for tech enthusiasts like me who like to have full control over their devices.\nI examined alternatives and ended up with Ubuntu. I\u0026rsquo;m used to it, and the LTS server variant has a strong focus on stability. It\u0026rsquo;s also free. Second place was Proxmox. I don\u0026rsquo;t need virtual machines, though, where Proxmox excels in. I also wanted to use Ansible to automatically install and configure everything, and having Proxmox in between just adds complexity.\nHome Server Goals # These were my goals for the laptop home server:\nAutomated deployment: do as little as possible manually. In case of a hardware failure, I can immediately switch to new hardware and have the exact same configuration. Docker: put as much as possible into Docker containers for easy management and backup Single Sign-On: sign-on with one user identity instead of creating the same user again and again for each service HTTPS Everywhere: work with nice names instead of IPs and ports, even when I\u0026rsquo;m accessing services internally. Adds security, too. Backup: robust backup for Docker containers and beyond Remote access: secure and easy remote access to selected services Home Server Hardware # When you think about it, laptops are the perfect home servers. The hardware is power efficient, they have an integrated uninterruptible power supply with their batteries and an integrated KVM with mouse, keyboard, and monitor. You can get a modern one on the internet for ~€200-€300. Or even for free if you ask around if someone has a spare one lying around. Just make sure that it is not too old (\u0026gt; Intel 7th generation), as older CPUs don\u0026rsquo;t have great power saving techniques.\nI went with a laptop with\nan 8th generation Core i7 a 1 TB NVMe SSD 16 GB memory More than enough to host a few docker containers and handle 2-4 Plex streams.\nI added an external 3,5\u0026quot; drive to store my media collection.\nBefore You Continue: helgeklein.com # My colleague Helge built a similar setup with an Intel NUC and Proxmox. He has a complete series on building a home lab with SSO and HTTPS certificates for internal services.\nHelge\u0026rsquo;s series on home automation It matches to some of my goals, so I took his series as a framework and added my ideas:\nObviously other hardware and operating system Automatic installation and configuration with Ansible pull Remote access without a full VPN tunnel. (Future article) As my set-up is built on top of his, I encourage you to read his whole series, especially the concept of using Caddy for services in the internal network. I link to his articles whenever there are numerous similarities and only note my customizations.\nIntroducing Ansible # Ansible is a free and open-source software to automate software provisioning, configuration management, and application deployments.\nI use it to install packages and configure the home server. No steps are done manually, so when the server breaks, I can spin up another, run the deployment there and have the same configuration.\nWith Ansible, one describes how a target system should look like in YAML. A YAML file is called a playbook. In organizations, there is typically a control server that connects to targets and applies the steps defined in the playbook. That works well but is too complicated for a home lab. You can save the control server and run the playbook directly from GitHub through Ansible pull.\nBelow is a diagram of the deployment architecture. I prepare the config of the home server on a client, push the config to GitHub, and execute it via Ansible pull on the home server.\nGitHub Flow Create a Repository On GitHub # Create a git repository on GitHub.com. I use a private one, as I would rather not share the exact config of my servers with the public.\nInstall the Windows Subsystem For Linux # The following steps need to be done on your client. I\u0026rsquo;m a Windows guy, but I feel that working with git and ssh keys is easier on Linux. Instead of creating a Linux VM dedicated to that purpose, I\u0026rsquo;m using the Windows Subsystem for Linux (WSL) which is integrated on Windows.\nwsl.exe --install -d Ubuntu Git # Install Git # From here on, we work completely in WSL.\napt install git Configure Git # Add your email and full name for git commits.\ngit config --global user.email \u0026#34;info@dominikbritz.com\u0026#34; git config --global user.name \u0026#34;Dominik Britz\u0026#34; Generate a New SSH Key for GitHub # ssh-keygen -t ed25519 -C \u0026#34;GitHub auth\u0026#34; Why ed25519?\nI named mine id_ed25519_github. You do want to use a passphrase.\nThis generates two files:\nid_ed25519_github ⇽ the private key id_ed25519_github.pub ⇽ the public key To use the key when connecting to GitHub via ssh later, you need to add the private key to the ssh agent. Create the alias ssha to do this conveniently.\nalias ssha=\u0026#39;eval $(ssh-agent) \u0026amp;\u0026amp; ssh-add ~/.ssh/id_ed25519_github\u0026#39; You probably need to commit and push a lot while testing, hence add the following alias as well.\nalias gitall=\u0026#39;git add * \u0026amp;\u0026amp; git commit --message \u0026#34;commit\u0026#34; \u0026amp;\u0026amp; git push\u0026#39; Make the aliases permanent by adding the commands to .bashrc. Otherwise, they are gone after you disconnect the session.\nvim ~/.bashrc Add Key To GitHub # cat ~/.ssh/id_ed25519_github.pub Go to your GitHub key settings and enter the output of the cat command as a new authentication key.\nClone the Repository # Change with your URL.\ncd ~ git clone git@github.com:DominikBritz/ansible-configs.git Set Up Ansible # Install Ansible # We need Ansible in the WSL environment to install public roles later. We do not manage the WSL environment with Ansible.\napt install ansible Create Ansible Folders And Files # Create the Ansible folder structure.\ncd ~/ansible-configs mkdir roles mkdir vars_files Roles # An Ansible role contains all tasks, variables, and files that logically belong together. They also allow easy sharing with others. I use a combination of my roles and public ones.\nBase # Updates the system and installs a cron job that updates the systems every Sunday at 02:00 and then reboots.\nmkdir roles/base/tasks -p vim roles/base/tasks/main.yml # Before we do anything on the machine, make sure it is up-to-date - name: Run the equivalent of \u0026#34;apt update\u0026#34; as a separate step ansible.builtin.apt: update_cache: yes - name: Upgrade the OS (apt dist-upgrade) ansible.builtin.apt: upgrade: dist - name: Check for pending reboots ansible.builtin.stat: path: /var/run/reboot-required register: reboot_required ignore_errors: yes - name: Reboot the server if required ansible.builtin.reboot: when: reboot_required.stat.exists # Periodically update the OS through a cron job - name: Create cron job for updates and reboot ansible.builtin.cron: name: Perform weekly updates and reboot minute: 0 hour: 2 weekday: 0 # 0 represents Sunday in cron job: \u0026#34;apt update \u0026amp;\u0026amp; apt dist-upgrade -y \u0026amp;\u0026amp; apt autoremove -y \u0026amp;\u0026amp; reboot\u0026#34; user: root Home # This role contains the configuration specific to my home server.\nInstalls the Unbound DNS server. See Unbound DNS Server Configuration. Installs restic and resticprofile for backups. See restic: Encrypted Offsite Backup for Your Homeserver. I added comments so you can follow along.\nNote the step where it mounts an external drive. I have an external drive connected to my home server, as the integrated 1 TB disk has not enough storage for my media collection. If you don\u0026rsquo;t have one, you can remove this step.\nmkdir roles/home/tasks -p vim roles/home/tasks/main.yml - name: Disable sleep when closing the lid lineinfile: path: /etc/systemd/logind.conf regexp: \u0026#39;^#?HandleLidSwitch=\u0026#39; line: \u0026#39;HandleLidSwitch=ignore\u0026#39; backup: yes ### ### Unbound ### - name: systemd is listening on port 53. Remove that as we need the port for our Unbound DNS server later ansible.builtin.lineinfile: path: /etc/systemd/resolved.conf regexp: \u0026#39;^#?DNSStubListener=\u0026#39; line: \u0026#39;DNSStubListener=0\u0026#39; - name: Restart systemd-resolved service ansible.builtin.service: name: systemd-resolved state: restarted - name: Update repositories cache and install \u0026#34;unbound\u0026#34; package ansible.builtin.apt: name: unbound update_cache: yes ### ### Backup with restic and resticprofile ### - name: Install \u0026#34;restic\u0026#34; package ansible.builtin.apt: name: restic update_cache: no - name: Run restic self-update command: restic self-update - name: Check if directory \u0026#34;/usr/local/bin/resticprofile\u0026#34; exists and store the result in the variable \u0026#34;resticprofile_dir\u0026#34; ansible.builtin.stat: path: /usr/local/bin/resticprofile register: resticprofile_dir - name: Download resticprofile install.sh script get_url: url: https://raw.githubusercontent.com/creativeprojects/resticprofile/master/install.sh dest: /tmp/install.sh when: not resticprofile_dir.stat.exists # this ensures we only do this step in case resticprofile is not already installed - name: Change permissions of install.sh file: path: /tmp/install.sh mode: \u0026#39;u+x\u0026#39; when: not resticprofile_dir.stat.exists # this ensures we only do this step in case resticprofile is not already installed - name: Run install.sh script shell: /tmp/install.sh -b /usr/local/bin args: chdir: /tmp when: not resticprofile_dir.stat.exists # this ensures we only do this step in case resticprofile is not already installed - name: Run resticprofile self-update shell: resticprofile self-update ### ### Mount external drive ### - name: Check if the external drive is already mounted shell: cmd: mount | grep \u0026#39;/media/18TB\u0026#39; register: mount_check ignore_errors: yes - name: Mount the drive if not already mounted and set it to mount at boot mount: path: \u0026#39;/media/18TB\u0026#39; src: \u0026#39;UUID=1c333c73-475d-4b3d-a82d-511321753eab\u0026#39; fstype: auto state: mounted boot: true when: mount_check.rc != 0 - name: Set permissions for dockerlimited group on /media/18TB acl: path: /media/18TB entity: dockerlimited etype: group permissions: \u0026#39;rwx\u0026#39; state: present recursive: yes default: true Ansible Galaxy Roles # Instead of implementing more complex roles myself, I utilize well-known community roles from Ansible Galaxy. Think of it as Ansible\u0026rsquo;s app store.\nAdd requirements.yml.\n--- roles: - name: geerlingguy.docker - name: geerlingguy.security - name: geerlingguy.ntp - name: oefenweb.ufw - name: geerlingguy.node_exporter Download the roles.\nansible-galaxy install --role-file requirements.yml --roles-path roles geerlingguy.docker\nInstalls Docker on Linux. Documentation.\ngeerlingguy.security\nPerforms some basic security configuration.\nInstall software to monitor bad SSH access (fail2ban) Configure SSH to be more secure (disabling root login, requiring key-based authentication, and allowing a custom SSH port to be set) Set up automatic updates (if configured to do so) Documentation geerlingguy.ntp\nManages NTP.\noefenweb.ufw\nSet up the ufw firewall on Ubuntu conveniently through variables. Documentation.\ngeerlingguy.node_exporter\nInstalls node exporter for monitoring your home server. Documentation\nVariables # Create a home.yml in vars_files for the home server and then accordingly for each server you want to manage. The files get referenced in playbooks later.\n# ntp ntp_timezone: Europe/Berlin # firewall my_ssh_port: 22 ufw_rules: # allow ssh traffic - rule: allow to_port: \u0026#34;{{ my_ssh_port }}\u0026#34; protocol: tcp # allow dns traffic for unbound - rule: allow to_port: \u0026#34;53\u0026#34; protocol: tcp - rule: allow to_port: \u0026#34;53\u0026#34; protocol: udp # allow https traffic to docker containers - rule: allow to_port: \u0026#34;443\u0026#34; protocol: tcp # security security_ssh_port: \u0026#34;{{ my_ssh_port }}\u0026#34; security_autoupdate_reboot: true # enable automatic updates security_autoupdate_reboot_time: \u0026#34;01:00\u0026#34; # if a reboot is necessary do it at this time security_autoupdate_mail_to: \u0026#34;info@dominikbritz.com\u0026#34; # send a notification email to your email address # node exporter node_exporter_version: \u0026#39;1.7.0\u0026#39; Root Playbooks # Create a home.yml in the repository root for your home server, and then accordingly for each server you would like to manage. The name does matter here, as Ansible pull expects it to be identical to the server\u0026rsquo;s hostname. If it does not find a file matching servername.domain.com.yml or servername.yml it falls back to local.yml.\n--- - name: home server hosts: localhost become: true vars_files: - vars_files/home.yml roles: - base - home - oefenweb.ufw - geerlingguy.docker - geerlingguy.ntp - geerlingguy.security - geerlingguy.node_exporter Push to GitHub # At this time, the contents of your local git folder should look like the below:\n├── LICENSE ├── README.md ├── home.yml ├── requirements.yml ├── roles/ │ ├── base/ │ │ └── tasks/ │ │ └── main.yml │ ├── geerlingguy.docker/ │ │ ├── ... │ ├── geerlingguy.node_exporter/ │ │ ├── ... │ ├── geerlingguy.ntp/ │ │ ├── ... │ ├── geerlingguy.security/ │ │ ├── ... │ ├── home/ │ │ └── tasks/ │ │ └── main.yml │ └── oefenweb.ufw/ │ ├── ... └── vars_files/ ├── home.yml Push the current state to GitHub.\ngitall We\u0026rsquo;re done now with the client.\nInstall And Configure the Server # The following steps have to be done on your home server. Operating System # I won\u0026rsquo;t cover the installation of Ubuntu, as there are hundreds of guides on the Internet. Just some aspects:\nYou want to go with the Ubuntu LTS server edition without a graphical interface for maximum performance and stability. Create an ssh key for remote management from you WSL client. Add the public key to the server\u0026rsquo;s ~/.ssh/authorized_keys file. Set the hostname to home. Run With Ansible Pull # To use Ansible pull with a private GitHub repository, create a personal access token. I use a fine-grained token that has access to one repository only.\nFine-grained access token in GitHub Run ansible-pull to configure your server. If the playbook requires a reboot after the update step, reboot the server and run the playbook again. Ansible can\u0026rsquo;t reboot automatically, as it would effectively kill itself.\nChange the PAT token and the GitHub URL.\nsudo apt update sudo apt install ansible export PAT=YOUR_PAT_TOKEN sudo ansible-pull -U https://$PAT:x-oauth-basic@github.com/DominikBritz/ansible-configs home.yml Create Docker Containers # Creating the Docker containers is a one-time manual step. Everything will be backed up later with restic and can be restored in case of data loss. Hence, I did not use Ansible to automate this task.\nThe first container is my management container Dockge. It\u0026rsquo;s fast, has a pretty UI, and, in comparison to Portainer, it has a file-based structure — Dockge won\u0026rsquo;t kidnap your compose files, they are stored on your drive as usual. You can interact with them using normal docker compose commands.\nBeautiful Dockge UI Create the Dockge folder. Putting content that is accessed by multiple users in /usr/local is my preference. Other options are:\n/opt /etc Create your own root like /data mkdir -p /usr/local/data/docker/dockge Create a Dockge compose file with the custom path from above.\ncd /usr/local/data/docker/dockge curl \u0026#34;https://dockge.kuma.pet/compose.yaml?port=5001\u0026amp;stacksPath=/usr/local/data/docker\u0026#34; --output compose.yaml Start the container with docker compose up -d. Browse to http://IPADDRESS:5001 to see the UI. This is where we create all future containers.\nUnbound DNS Server Configuration # I followed Helge\u0026rsquo;s guide and set up Unbound as a DNS resolver. I skipped the static IP part, as Ubuntu supports DHCP. Instead, I added a reservation for my home server on my DHCP server.\nI will discuss secure external access in a future blog article. Having an internal DNS resolver becomes essential then, so don\u0026rsquo;t skip this step.\nrestic: Encrypted Offsite Backup for Your Homeserver # I followed Helge\u0026rsquo;s guide for restic and resticprofile for backups. You can skip the installation part, though, as Ansible handles this for you.\nIn addition to Helge\u0026rsquo;s setup, I added an exclude file to add excludes anytime without having to reschedule.\nIn the data/resticprofile/profiles.yml add the iexclude-file setting to the backup section.\n# Backup command backup: iexclude-file: \u0026#34;excludes\u0026#34; # name of your exclude file Create the excludes file. Whenever you want to exclude files from your backup, add a line with the path to the file.\nexcludes:\n# Syntax: https://pkg.go.dev/path/filepath#Match # all strings are case-insensitive # exclude all files in the deemix/downloads folder */docker/deemix/downloads/* Automatic HTTPS Certificates for Services on Internal Home Network # I followed Helge\u0026rsquo;s guide and use Caddy for certificates for my internal services. He uses Cloudflare, but I use Netcup. To get it working with Netcup, do the below.\nEdit the Dockerfile in /caddy/dockerfile-dns.\nARG VERSION=2 FROM caddy:${VERSION}-builder AS builder RUN xcaddy build \\ --with github.com/caddy-dns/netcup FROM caddy:${VERSION} COPY --from=builder /usr/bin/caddy /usr/bin/caddy In your container-vars.env, remove the CLOUDFLARE_API_TOKEN and add Netcup specific variables.\nNETCUP_CUSTOMER_NUMBER=12345678 # get the ID from your CCP page NETCUP_API_KEY=ABCD # https://helpcenter.netcup.com/en/wiki/general/our-api NETCUP_API_PASSWORD=password # https://helpcenter.netcup.com/en/wiki/general/our-api The Caddyfile references these variables. Furthermore, Netcup requires some additional settings in the tls configuration.\ndockge.{$MY_DOMAIN} { reverse_proxy {$MY_HOST_IP}:5001 tls { dns netcup { customer_number {env.NETCUP_CUSTOMER_NUMBER} api_key {env.NETCUP_API_KEY} api_password {env.NETCUP_API_PASSWORD} } propagation_timeout 900s propagation_delay 600s resolvers 1.1.1.1 } } As you can guess from the timeout setting of 900 seconds, it can take a while until the certificate is successfully obtained by caddy. You can monitor the status in the Dockge terminal chart of the caddy container.\nSSO \u0026amp; Monitoring # I followed Helge\u0026rsquo;s other articles more or less exactly.\nAuthelia \u0026amp; lldap: Authentication, SSO, User Management \u0026amp; Password Reset for Home Networks Grafana Setup Guide With Automatic HTTPS \u0026amp; OAuth SSO via Authelia Docker Monitoring With Prometheus, Automatic HTTPS \u0026amp; SSO Authentication Conclusion And Next Articles # I achieved all my goals that I\u0026rsquo;ve set initially. Let\u0026rsquo;s go over them:\nAutomated deployment: all packages are installed and configured via Ansible. The only manual step is the Ubuntu installation. Docker: all services are running as Docker containers. Essential services like DNS and backup are installed natively. Single Sign-On: provided by LLDAP and Authelia HTTPS Everywhere: served through Caddy Backup: restic and resticprofile backup for Docker mount points and important files like the Unbound configuration Remote access: will be covered in a future article I will continue this series with articles on remote access, Nextcloud for files, Immich for photos, Sonarr/Radarr/Plex for media management, and maybe more.\nChangelog # 2025-03-24: moved the instructions for remote server management to the OS install section 2024-01-31: added section about changes to the Dockerfile for Caddy ","date":"23 January 2024","externalUrl":null,"permalink":"/posts/2024-01-22-laptop-home-server/","section":"Posts","summary":"A guide to setting up an old laptop as your new home server, powered by Ubuntu, Ansible, and Docker. It securely hosts videos, files, and photos. Add HTTPS everywhere, cloud backup and performance monitoring.\nBye Bye Synology, Hello Ubuntu # TL;DR # Synology has a great ecosystem, but one is locked in. I’m using Ubuntu LTS server now, as it gives full control and is focused on stability. I chose it over Proxmox for simplicity, as I don’t need virtual machines.\n","title":"Laptop Home Server Build","type":"posts"},{"content":"","date":"23 January 2024","externalUrl":null,"permalink":"/tags/restic/","section":"Tags","summary":"","title":"Restic","type":"tags"},{"content":"","date":"23 January 2024","externalUrl":null,"permalink":"/tags/ubuntu/","section":"Tags","summary":"","title":"Ubuntu","type":"tags"},{"content":"","date":"23 January 2024","externalUrl":null,"permalink":"/tags/unbound/","section":"Tags","summary":"","title":"Unbound","type":"tags"},{"content":"","date":"7 January 2021","externalUrl":null,"permalink":"/tags/powershell/","section":"Tags","summary":"","title":"Powershell","type":"tags"},{"content":"There are lots and lots of articles on the web describing how to save a whole webpage as PDF. They all use, more or less, the browser\u0026rsquo;s ability to print to PDF.\nI needed to convert many URLs at once. Doing that manually for every URL would have been cumbersome, and that\u0026rsquo;s why I automated the process.\nThe following tools helped me to achieve my goal:\nLinkKlipper: a Chrome extension to export all links from a website wkhtmltopdf: a command-line tool that saves a URL to PDF PDF24: my preferred PDF management tool. You can use any tool; it has to support combining multiple PDFs, though. PowerShell: to glue everything together Collecting All URLs # Install LinkKlipper in Chrome or Edge In the extension\u0026rsquo;s setting, change the output from CSV to TXT Browse to the website you want to convert By clicking on the extension\u0026rsquo;s icon in the menu bar, you can collect all links on the webpage. The webpage I was looking at contained a menu to all subpages, like a sitemap. That allowed me to export all the URLs I was interested in at once.\nConvert Each URL to a PDF File # With the help of the open-source tool wkhtmltopdf and PowerShell you can convert every URL in the TXT file from above to a PDF file.\nDownload wkhtmltopdf and install it.\nLook at the PowerShell script below, change the variables if needed, and run the script.\n$sourceFile = \u0026#34;C:\\links.txt\u0026#34; # the source file containing the URLs you want to convert $destFolder = \u0026#34;C:\\output\u0026#34; # converted PDFs will be saved here. Folder has to exist. $linkList = get-content $sourceFile foreach ($link in $linkList) { $outfile = $link -replace \u0026#39;/\u0026#39;,\u0026#39;-\u0026#39; # replace slashes with dashes $date = get-date -Format \u0026#34;yyyy-MM-dd HH-mm-ss\u0026#34; # adding a date to the filename allows for easy sorting later on $outfile = $date + \u0026#39; \u0026#39; + $outfile + \u0026#39;.pdf\u0026#39; $destFullPath = Join-Path $destFolder $outfile \u0026amp; \u0026#39;C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe\u0026#39; --disable-smart-shrinking --no-footer-line --no-header-line --no-outline \u0026#34;$link\u0026#34; \u0026#34;$destFullPath\u0026#34; } If you\u0026rsquo;re not satisfied with the PDFs\u0026rsquo; design, have a look at wkhtmltopdf\u0026rsquo;s command-line arguments.\nCombine PDFs # The last step is optional. But I prefer reading one big PDF over jumping through many different PDFs. Hence, take your PDF tool of choice, mine is PDF24, and combine all created PDFs into one big PDF file.\n","date":"7 January 2021","externalUrl":null,"permalink":"/posts/2021-01-07-batch-convert-urls-to-pdf/","section":"Posts","summary":"There are lots and lots of articles on the web describing how to save a whole webpage as PDF. They all use, more or less, the browser’s ability to print to PDF.\nI needed to convert many URLs at once. Doing that manually for every URL would have been cumbersome, and that’s why I automated the process.\n","title":"PowerShell – Batch Convert URLs to PDF","type":"posts"},{"content":"For a new feature in version 6.0 of uberAgent, we needed to run a few saved searches to do some calculations but let users modify input values.\nOur requirements in detail:\nRun saved searches to calculate experience scores for machines, user sessions, and applications Provide defaults for thresholds and weights but let users configure them. Users might make modifications once or twice, but not daily. So we needed something easy to understand but nothing too fancy. Only administrators should modify settings that should be valid for all users accessing a particular Splunk app. It needed to be configurable, but not for everyone. A poor man\u0026rsquo;s access control, so to say. I explain below how we achieved that by using Splunk anywhere examples. That\u0026rsquo;s probably easier to understand, and I don\u0026rsquo;t have to explain every bit of uberAgent here 😉\nSaved Searches Accept Tokens # That was not clear to me but saved searches accept tokens. That\u0026rsquo;s awesome as it\u0026rsquo;s the foundation of our solution to our requirements: we can run saved searches configured by tokens, and only Splunk users with access to the disk can change them. Poor man\u0026rsquo;s access control achieved!\nLet\u0026rsquo;s start with a simple search listing all source types in the index _internal.\nindex=_internal | top limit=0 sourcetype Do the same but output only source types with a count greater than 1000.\nindex=_internal | top limit=0 sourcetype | where count\u0026gt;1000 The 1000 is the part you want to be configurable. Hence create the following saved search:\n[im_accepting_tokens] dispatch.earliest_time = -30m dispatch.latest_time = now search = index=_internal | top limit=0 sourcetype | where count\u0026gt;$TokenCount$ run_on_startup = false enableSched = 0 Note that 1000 is now $TokenCount$. That can be replaced dynamically by calling the saved search like so:\n| savedsearch im_accepting_tokens TokenCount=500 Using Lookups for Easy Token Management # Saved searches are stored in savedsearches.conf. Letting users change things directly there can be dangerous. Searches can get quite long and hard to read, and cron schedules are not for everybody.\nHence we wanted to pass tokens to saved searches more safely and, most importantly, more user-friendly.\nA CSV lookup was the most convenient way we found. Everybody is more or less familiar with the syntax. And we are using them in uberAgent for several things, so it was just another one to manage.\nCreating a Lookup # Add the following to your transforms.conf.\n[lookup_input_tokens] filename = input_tokens.csv Create the input_tokens.csv with the following content.\ntoken,value TokenCount,1000 If you need more tokens, add them below the TokenCount one.\ntoken,value TokenCount,1000 AnotherToken,\u0026#34;ABC\u0026#34; AndAnotherOne,123 Reading a Lookup as Input for a Saved Search # To read the content of your newly created lookup when running the saved search, call the saved search like the following:\n| savedsearch im_accepting_tokens [ | inputlookup lookup_input_tokens | xyseries token token value | stats values(*) as * | fields - token] | inputlookup lookup_input_tokens reads the lookup xyseries token token value transforms the lookup. Everything in the token column gets a field with value as value. Think of a table. token will be the column header and value an entry in a row for that column. | stats values(*) as * removes all empty values (-\u0026gt; all fields in the table that are empty) | fields - token removes the token field itself as its not a field we want to pass to the saved search Conclusion # We solved all our requirements with not too much effort. Users with sufficient permissions may change things by just editing a CSV file. Nice and easy!\nI\u0026rsquo;ve to admit that using that method to pass one token is slightly overkill, but we had to pass a lot for maximum flexibility.\nHappy Splunking!\n","date":"2 November 2020","externalUrl":null,"permalink":"/posts/2020-11-02-using-lookups-as-token-input/","section":"Posts","summary":"For a new feature in version 6.0 of uberAgent, we needed to run a few saved searches to do some calculations but let users modify input values.\nOur requirements in detail:\nRun saved searches to calculate experience scores for machines, user sessions, and applications Provide defaults for thresholds and weights but let users configure them. Users might make modifications once or twice, but not daily. So we needed something easy to understand but nothing too fancy. Only administrators should modify settings that should be valid for all users accessing a particular Splunk app. It needed to be configurable, but not for everyone. A poor man’s access control, so to say. I explain below how we achieved that by using Splunk anywhere examples. That’s probably easier to understand, and I don’t have to explain every bit of uberAgent here 😉\n","title":"Running Splunk Saved Searches Powered by Tokens From Lookups","type":"posts"},{"content":"","date":"17 March 2020","externalUrl":null,"permalink":"/tags/euc/","section":"Tags","summary":"","title":"Euc","type":"tags"},{"content":"Managing applications on Windows in enterprises is complex and cumbersome. Admins are using a variety of tools to install, uninstall or reconfigure applications silently. The most popular tool is propably Microsoft\u0026rsquo;s ConfigMgr.\nWhile ConfigMgr (and others) makes sense for mid to large enterprises, the management is too time-consuming for smaller firms. I work in such a smaller firm. We have a lab to test our own applications uberAgent on several operating systems. These lab machines don\u0026rsquo;t only run uberAgent, of course, they also run standard applications like text editors, browsers, and more complex things.\nWhile we are using Microsoft MDT to install the OS and set everything up, all these applications need some sort of installing/updating mechanism. We use chocolatey for most of them. However, not all packages can be deployed with chocolatey. Some need special treatment, pre-install or post-install actions, or there is simply no chocolatey package available.\nFor these \u0026lsquo;special treatment\u0026rsquo; applications we developed our own PowerShell module called vlDeploy, which I\u0026rsquo;m happy to share publically.\nIntroducing vlDeploy # vlDeploy is a PowerShell module publically available in the PowerShell gallery. It allows the installation of applications locally, or even better, remotely. The remoting capability enables administrators to push applications to multiple machines.\nHere is a list of its features:\nGets a list of installed applications locally, or, from a remote machine Installs applications locally, or, on remote machines Uninstalls applications locally, or, on remote machines Support for .exe, .msi, and .ps1 installers Very cool: throw the .msi installer at the module and it will take care of installing the application silently. Same is true for .ps1 files. Support for URLs as installer The installer will be downloaded and then deployed Powershell pipeline support Valid exit codes handover Reboot after success Is it a full blown application distribution and everything else can get tossed to the void? No, of course not. It has its use-cases which I\u0026rsquo;m describing in the following.\nCombining vlDeploy with other tools gives you a very powerful PowerShell-based installation framework. See chapter Combine vlDeploy With Other Tools for details.\nPrerequisites # Before you begin, ensure you have met the following requirements:\nAt least PowerShell version 5.0 Windows operating system Installing vlDeploy # The module is availabe in the PowerShell Gallery. To install vlDeploy, follow these steps:\nInstall-Module vlDeploy Using vlDeploy # vlDeploy exposes three functions to list installed applications, install applications, and uninstall applications.\nTo see what\u0026rsquo;s possible with each, use:\nGet-Help Install-vlApplication -full Get-Help Uninstall-vlApplication -full Get-Help Get-vlInstalledApplication -full Examples # Simple application installation on a remote machine.\n$cred = Get-Credential Install-vlApplication -Computer PC1 -Sourcefiles \u0026#39;C:\\apps\\source\u0026#39; -Installer Setup.exe -InstallerArguments \u0026#39;/silent /noreboot\u0026#39; -Credential $cred Application installation on a remote machine with a PowerShell script downloaded from the Internet.\n$cred = Get-Credential Install-vlApplication -Computer PC1 -Installer \u0026#39;https://somewebsite.com/apps/Install.ps1\u0026#39; -Credential $cred Install-vlApplication accepts pipeline input for the -Computer parameter , which makes it easy to mass-deploy applications. It also recognizes .msi installer files and builds the full install command automatically.\n$cred = Get-Credential \u0026#39;PC1\u0026#39;, \u0026#39;PC2\u0026#39;, \u0026#39;PC3\u0026#39; | Install-vlApplication -Sourcefiles \u0026#39;C:\\apps\\source\u0026#39; -Installer Setup.msi -Credential $cred Get the uninstall string of an application from a remote machine and uninstall it there. Valid exit codes are 0, 3010, and 1 (default 0 and 3010).\n$cred = Get-Credential Get-vlInstalledApplication -Computer PC1 -Name \u0026#39;Google Chrome\u0026#39; -Credential $cred | Uninstall-vlApplication -Credential $cred -ValidExitCodes 0,3010,1 Get the uninstall string of an application from a remote machine and uninstall it on multiple machines. Reboot every machine if uninstallation was successful.\n$cred = Get-Credential $UninstallString = (Get-vlInstalledApplication -Computer PC1 -Name \u0026#39;Google Chrome\u0026#39; -Credential $cred).UninstallString \u0026#39;PC1\u0026#39;, \u0026#39;PC2\u0026#39;, \u0026#39;PC3\u0026#39; | Uninstall-vlApplication -UninstallString $UninstallString -Credential $cred -RebootAfterSuccess Combine vlDeploy With Other Tools # You can combine vlDeploy very nicely with other PowerShell tools. The combination with the following tools gives you a very powerful PowerShell-based installation framework\nEvergreen # Evergreen is a PowerShell module actively developed by Aaron Parker to return the latest version and download URLs for a set of common enterprise applications.\nAs of 2020-03-02, the following applications are included:\nAdobeAcrobatReaderDC Atom BISF CitrixAppLayeringFeed CitrixApplicationDeliveryManagementFeed CitrixEndpointManagementFeed CitrixGatewayFeed CitrixHypervisorFeed CitrixLicensingFeed CitrixReceiverFeed CitrixSdwanFeed CitrixVirtualAppsDesktopsFeed CitrixWorkspaceApp CitrixWorkspaceAppFeed CitrixXenServerTools ControlUpAgent Cyberduck FileZilla FoxitReader GitForWindows Good GoogleChrome Greenshot JamTreeSizeFree JamTreeSizeProfessional Java8 LibreOffice MicrosoftEdge MicrosoftFSLogixApps MicrosoftOffice MicrosoftPowerShellCore MicrosoftSQLServerManagementStudio MicrosoftSsms MicrosoftTeams MicrosoftVisualStudioCode MozillaFirefox mRemoteNG NotepadPlusPlus OpenJDK OracleJava8 OracleVirtualBox PaintDotNet ScooterBeyondCompare ShareX TeamViewer VideoLanVlcPlayer VMwareTools WinMerge Zoom After installing the module via Install-Module Evergreen you can see which applications are available with (Get-Command -Module Evergreen).Name -replace 'Get-','' | Sort-Object.\nIn combination with vlDeploy you can deploy applications to multiple hosts without having to maintain a fileshare with dozens of sources.\nBelow is a simple example using vlDeploy in combination with Evergreen to install Microsoft Edge.\n# Variables $Computers = \u0026#34;PC1\u0026#34;, \u0026#34;PC2\u0026#34; $cred = Get-Credential # Install modules $Modules = @(\u0026#34;Evergreen\u0026#34;, \u0026#34;vlDeploy\u0026#34;) Foreach ($Module in $Modules) { If (-not(Get-Module $Module)) { Install-Module $Module -Force Import-Module $Module } Else { Update-Module $Module Import-Module $Module } } # Get setup URI $App = Get-MicrosoftEdge | Where-Object Platform -eq \u0026#39;Windows\u0026#39; | Where-Object Product -eq \u0026#39;Stable\u0026#39; | Where-Object Architecture -eq \u0026#39;x64\u0026#39; | Select-Object -First 1 # Install Install-vlApplication -Installer \u0026#34;$($App.URI)\u0026#34; -Computer $Computers -Credential $cred I recommend having a look at Evergreen in more detail. It also includes silent installer arguments as well as pre-install and post-install actions. Here is nice example for Adobe Reader.\nXenAppBlog Automation Framework Applications # The XenAppBlog Automation Framework Applications by Trond Eirik Haavarstein are scripts that install applications without the need to maintain installer sources. The framework is using Evergreen in some scripts as well.\nInstall VMware Tools on multiple computers and reboot without having to think about managing installers.\n# Variables $Computers = \u0026#34;PC1\u0026#34;,\u0026#34;PC2\u0026#34; $cred = Get-Credential # Install app Install-vlApplication -Installer \u0026#39;https://raw.githubusercontent.com/haavarstein/Applications/master/VMware/Tools/Install.ps1\u0026#39; -Computer $Computers -Credential $cred -RebootAfterSuccess PowerShell App Deployment Toolkit # Evergreen and the XenAppBlog Automation Framework Applications are limited to a set of common enterprise applications. You won\u0026rsquo;t find your very special LOB application in there. LOB applications are best deployed with the PowerShell App Deployment Toolkit.\nThe PowerShell App Deployment Toolkit provides a set of functions to perform common application deployment tasks and to interact with the user during a deployment. It simplifies the complex scripting challenges of deploying applications in the enterprise, provides a consistent deployment experience and improves installation success rates. Source: https://github.com/PSAppDeployToolkit/PSAppDeployToolkit\nThe installation logic is defined in the file Deploy-Application.ps1. That makes the integration with vlDeploy very easy.\n# Variables $Computers = \u0026#34;PC1\u0026#34;,\u0026#34;PC2\u0026#34; $cred = Get-Credential $Sourcefiles = \u0026#34;\\\\server\\share\\apps\u0026#34; # Install apps $Folders = (Get-ChildItem $Sourcefiles -Directory).FullName Foreach ($Folder in $Folders) { Install-vlApplication -Sourcefiles $Folder -Installer \u0026#39;Deploy-Application.ps1\u0026#39; -Computer $Computers -Credential $cred } A step-by-step guide for the PowerShell App Deployment Toolkit is available here from replicajunction.\nDevelopment And Contributing # The module has a feature set tailored to the needs we have in our company. If you need something else, please contribute at GitHub.\nA list of features I would like to add, but there was not enough time yet:\nRun install/uninstall as PowerShell jobs to benefit from parallelism Perhaps use PowerShell 7 and Foreach-Object -Parallel. More information. Add support for .bat,.cmd, .msp, .mst, and .msu installers Output only applications which are listed in Add/Remove programs by default. Output everything only when desired. ","date":"17 March 2020","externalUrl":null,"permalink":"/posts/2020-03-17-introducing-vldeploy/","section":"Posts","summary":"Managing applications on Windows in enterprises is complex and cumbersome. Admins are using a variety of tools to install, uninstall or reconfigure applications silently. The most popular tool is propably Microsoft’s ConfigMgr.\nWhile ConfigMgr (and others) makes sense for mid to large enterprises, the management is too time-consuming for smaller firms. I work in such a smaller firm. We have a lab to test our own applications uberAgent on several operating systems. These lab machines don’t only run uberAgent, of course, they also run standard applications like text editors, browsers, and more complex things.\n","title":"Introducing vlDeploy - Deploy Applications With PowerShell Remotely","type":"posts"},{"content":" CV # Professional experience # Lead Consultant # Cloud Software Group, Citrix Aug. 2025 — Today\nIn my current role as lead consultant, I am responsible for driving the adoption of uberAgent in customer environments. This includes providing expert guidance on best practices, conducting workshops, and ensuring successful implementation of our product to meet customer needs.\nLead Product Manager # Cloud Software Group, Citrix Jan. 2024 — Jul. 2025\nAfter the acquisition of vast limit GmbH by Cloud Software Group, I transitioned into the role of lead product manager for uberAgent. My responsibilities include defining the product roadmap, prioritizing features, and ensuring alignment with customer needs and market trends.\nCustomer Success Engineer # vast limit GmbH (acquired by Cloud Software Group) Nov. 2017 — Dec. 2023\nIn my position as customer success engineer I had a broad set of responsibilities covering such diverse areas as key account management, technical presales, partner enablement, 3rd level technical support, Splunk dashboard development, and integration testing for our product uberAgent.\nIT System Architect Desktopvirtualisation # OBI Smart Technologies GmbH Oct. 2016 — Oct. 2017\nAs an architect I was responsible for the design and implementation of the virtual desktop and app infrastructure at OBI.\nSenior IT-Consultant # sepago GmbH Aug. 2011 — Sept. 2016\nSpecialized in operating, building, and designing virtual desktop infrastructures with a strong focus on automation. Technologies I\u0026rsquo;ve worked with:\nCitrix XenApp/XenDesktop Citrix NetScaler Citrix XenMobile Trainee # sepago GmbH Apr. 2011 — Sept. 2011\nEducation # Fachinformatiker Systemintegration # sepago GmbH 2009 — 2011\n","externalUrl":null,"permalink":"/cv/","section":"DominikBritz.com","summary":"CV # Professional experience # Lead Consultant # Cloud Software Group, Citrix Aug. 2025 — Today\n","title":"","type":"page"},{"content":" Impressum # Angaben gemäß § 5 TMG # Dominik Britz\nGengesfeld, 12\n51688 Wipperfürth\nKontakt # E-Mail: info@dominikbritz.com\nRedaktionell verantwortlich # Dominik Britz\n","externalUrl":null,"permalink":"/imprint/","section":"DominikBritz.com","summary":"Impressum # Angaben gemäß § 5 TMG # Dominik Britz\nGengesfeld, 12\n51688 Wipperfürth\nKontakt # E-Mail: info@dominikbritz.com\n","title":"","type":"page"},{"content":" Datenschutzerklärung # Allgemeiner Hinweis und Pflichtinformationen # Benennung der verantwortlichen Stelle # Die verantwortliche Stelle für die Datenverarbeitung auf dieser Website ist:\nDominik Britz\nGengesfeld, 12\n51688 Wipperfürth\nDie verantwortliche Stelle entscheidet allein oder gemeinsam mit anderen über die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten (z.B. Namen, Kontaktdaten o. Ä.).\nWiderruf Ihrer Einwilligung zur Datenverarbeitung # Nur mit Ihrer ausdrücklichen Einwilligung sind einige Vorgänge der Datenverarbeitung möglich. Ein Widerruf Ihrer bereits erteilten Einwilligung ist jederzeit möglich. Für den Widerruf genügt eine formlose Mitteilung per E-Mail. Die Rechtmäßigkeit der bis zum Widerruf erfolgten Datenverarbeitung bleibt vom Widerruf unberührt.\nRecht auf Beschwerde bei der zuständigen Aufsichtsbehörde # Als Betroffener steht Ihnen im Falle eines datenschutzrechtlichen Verstoßes ein Beschwerderecht bei der zuständigen Aufsichtsbehörde zu. Zuständige Aufsichtsbehörde bezüglich datenschutzrechtlicher Fragen ist der Landesdatenschutzbeauftragte des Bundeslandes, in dem sich der Sitz unseres Unternehmens befindet. Der folgende Link stellt eine Liste der Datenschutzbeauftragten sowie deren Kontaktdaten bereit: https://www.bfdi.bund.de/DE/Infothek/Anschriften_Links/anschriften_links-node.html.\nRecht auf Datenübertragbarkeit # Ihnen steht das Recht zu, Daten, die wir auf Grundlage Ihrer Einwilligung oder in Erfüllung eines Vertrags automatisiert verarbeiten, an sich oder an Dritte aushändigen zu lassen. Die Bereitstellung erfolgt in einem maschinenlesbaren Format. Sofern Sie die direkte Übertragung der Daten an einen anderen Verantwortlichen verlangen, erfolgt dies nur, soweit es technisch machbar ist.\nRecht auf Auskunft, Berichtigung, Sperrung, Löschung # Sie haben jederzeit im Rahmen der geltenden gesetzlichen Bestimmungen das Recht auf unentgeltliche Auskunft über Ihre gespeicherten personenbezogenen Daten, Herkunft der Daten, deren Empfänger und den Zweck der Datenverarbeitung und ggf. ein Recht auf Berichtigung, Sperrung oder Löschung dieser Daten. Diesbezüglich und auch zu weiteren Fragen zum Thema personenbezogene Daten können Sie sich jederzeit über die im Impressum aufgeführten Kontaktmöglichkeiten an uns wenden.\nSSL- bzw. TLS-Verschlüsselung # Aus Sicherheitsgründen und zum Schutz der Übertragung vertraulicher Inhalte, die Sie an uns als Seitenbetreiber senden, nutzt unsere Website eine SSL-bzw. TLS-Verschlüsselung. Damit sind Daten, die Sie über diese Website übermitteln, für Dritte nicht mitlesbar. Sie erkennen eine verschlüsselte Verbindung an der „https://“ Adresszeile Ihres Browsers und am Schloss-Symbol in der Browserzeile.\nServer-Log-Dateien # In Server-Log-Dateien erhebt und speichert der Provider der Website automatisch Informationen, die Ihr Browser automatisch an uns übermittelt. Dies sind:\nBesuchte Seite auf unserer Domain\nDatum und Uhrzeit der Serveranfrage\nBrowsertyp und Browserversion\nVerwendetes Betriebssystem\nReferrer URL\nHostname des zugreifenden Rechners\nIP-Adresse\nEs findet keine Zusammenführung dieser Daten mit anderen Datenquellen statt. Grundlage der Datenverarbeitung bildet Art. 6 Abs. 1 lit. b DSGVO, der die Verarbeitung von Daten zur Erfüllung eines Vertrags oder vorvertraglicher Maßnahmen gestattet.\nSpeicherdauer von Beiträgen und Kommentaren # Beiträge und Kommentare sowie damit in Verbindung stehende Daten, wie beispielsweise IP-Adressen, werden gespeichert. Der Inhalt verbleibt auf unserer Website, bis er vollständig gelöscht wurde oder aus rechtlichen Gründen gelöscht werden musste.\nDie Speicherung der Beiträge und Kommentare erfolgt auf Grundlage Ihrer Einwilligung (Art. 6 Abs. 1 lit. a DSGVO). Ein Widerruf Ihrer bereits erteilten Einwilligung ist jederzeit möglich. Für den Widerruf genügt eine formlose Mitteilung per E-Mail. Die Rechtmäßigkeit bereits erfolgter Datenverarbeitungsvorgänge bleibt vom Widerruf unberührt.\nAbonnieren von Kommentaren # Sie können als Nutzer unserer Website nach erfolgter Anmeldung Kommentare abonnieren. Mit einer Bestätigungs-E-Mail prüfen wir, ob Sie der Inhaber der angegebenen E-Mail-Adresse sind. Sie können die Abo-Funktion für Kommentare jederzeit über einen Link, der sich in einer Abo-Mail befindet, abbestellen. Zur Einrichtung des Abonnements eingegebene Daten werden im Falle der Abmeldung gelöscht. Sollten diese Daten für andere Zwecke und an anderer Stelle an uns übermittelt worden sein, verbleiben diese weiterhin bei uns.\nCookies # Unsere Website verwendet Cookies. Das sind kleine Textdateien, die Ihr Webbrowser auf Ihrem Endgerät speichert. Cookies helfen uns dabei, unser Angebot nutzerfreundlicher, effektiver und sicherer zu machen.\nEinige Cookies sind “Session-Cookies.” Solche Cookies werden nach Ende Ihrer Browser-Sitzung von selbst gelöscht. Hingegen bleiben andere Cookies auf Ihrem Endgerät bestehen, bis Sie diese selbst löschen. Solche Cookies helfen uns, Sie bei Rückkehr auf unserer Website wiederzuerkennen.\nMit einem modernen Webbrowser können Sie das Setzen von Cookies überwachen, einschränken oder unterbinden. Viele Webbrowser lassen sich so konfigurieren, dass Cookies mit dem Schließen des Programms von selbst gelöscht werden. Die Deaktivierung von Cookies kann eine eingeschränkte Funktionalität unserer Website zur Folge haben.\nDas Setzen von Cookies, die zur Ausübung elektronischer Kommunikationsvorgänge oder der Bereitstellung bestimmter, von Ihnen erwünschter Funktionen (z.B. Warenkorb) notwendig sind, erfolgt auf Grundlage von Art. 6 Abs. 1 lit. f DSGVO. Als Betreiber dieser Website haben wir ein berechtigtes Interesse an der Speicherung von Cookies zur technisch fehlerfreien und reibungslosen Bereitstellung unserer Dienste. Sofern die Setzung anderer Cookies (z.B. für Analyse-Funktionen) erfolgt, werden diese in dieser Datenschutzerklärung separat behandelt.\nYouTube Videos # Art und Umfang der Verarbeitung # Wir haben auf unserer Website YouTube Video integriert. YouTube Video ist eine Komponente der Videoplattform der YouTube, LLC, auf der Nutzer Inhalte hochladen, über das Internet teilen und detaillierte Statistiken erhalten können. YouTube Video ermöglicht es uns, Inhalte der Plattform in unsere Website zu integrieren.\nYouTube Video nutzt Cookies und weitere Browser-Technologien um Nutzerverhalten auszuwerten, Nutzer wiederzuerkennen und Nutzerprofile zu erstellen. Diese Informationen werden unter anderem genutzt, um die Aktivität der angehörten Inhalte zu analysieren und Berichte zu erstellen. Wenn ein Nutzer bei YouTube, LLC registriert ist, kann YouTube Video die abgespielten Videos dem Profil zuordnen.\nWenn Sie auf diese Inhalte zugreifen, stellen Sie eine Verbindung zu Servern der YouTube, LLC, Google Ireland Limited, Gordon House, Barrow Street Dublin 4 Irland her, wobei Ihre IP-Adresse und ggf. Browserdaten wie Ihr User-Agent übermittelt werden.\nZweck und Rechtsgrundlage # Die Nutzung des Dienstes erfolgt auf Grundlage Ihrer Einwilligung gemäß Art. 6 Abs. 1 lit. a. DSGVO und § 25 Abs. 1 TTDSG.\nWir beabsichtigen personenbezogenen Daten an Drittländer außerhalb des Europäischen Wirtschaftsraums, insbesondere die USA, zu übermitteln. In Fällen, in denen kein Angemessenheitsbeschluss der Europäischen Kommission existiert (z.B. in den USA) haben wir mit den Empfängern der Daten anderweitige geeignete Garantien im Sinne der Art. 44 ff. DSGVO vereinbart. Dies sind – sofern nicht anders angegeben – Standardvertragsklauseln der EU-Kommission gemäß Durchführungsbeschluss (EU) 2021/914 vom 4. Juni 2021. Eine Kopie dieser Standardvertragsklauseln können Sie unter https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32021D0914\u0026amp;from=DE einsehen.\nZudem holen wir vor einem solchen Drittlandtransfer Ihre Einwilligung nach Art. 49 Abs. 1 Satz 1 lit. a. DSGVO ein, die Sie über die Einwilligung im Consent Manager (oder sonstigen Formularen, Registrierungen etc.) erteilen. Wir weisen Sie darauf hin, dass bei Drittlandübermittlungen im Detail unbekannte Risiken (z.B. die Datenverarbeitung durch Sicherheitsbehörden des Drittlandes, deren genauer Umfang und deren Folgen für Sie wir nicht kennen, auf die wir keinen Einfluss haben und von denen Sie unter Umständen keine Kenntnis erlangen) bestehen können.\nSpeicherdauer # Die konkrete Speicherdauer der verarbeiteten Daten ist nicht durch uns beeinflussbar, sondern wird von YouTube, LLC bestimmt. Weitere Hinweise finden Sie in der Datenschutzerklärung für YouTube Video: https://policies.google.com/privacy.\nVimeo # Art und Umfang der Verarbeitung # Wir haben auf unserer Website Vimeo Video integriert. Vimeo Video ist eine Komponente der Videoplattform von Vimeo, LLC, auf der Nutzer Inhalte hochladen, über das Internet teilen und detaillierte Statistiken erhalten können.\nVimeo Video ermöglicht es uns, Inhalte der Plattform in unsere Website zu integrieren.\nVimeo Video nutzt Cookies und weitere Browser-Technologien um Nutzerverhalten auszuwerten, Nutzer wiederzuerkennen und Nutzerprofile zu erstellen. Diese Informationen werden unter anderem genutzt, um die Aktivität der angehörten Inhalte zu analysieren und Berichte zu erstellen.\nWenn Sie auf diese Inhalte zugreifen, stellen Sie eine Verbindung zu Servern der Vimeo, LLC, 555 W 18th St, New York, New York 10011 her, wobei Ihre IP-Adresse und ggf. Browserdaten wie Ihr User-Agent übermittelt werden.\nZweck und Rechtsgrundlage # Der Einsatz von Vimeo erfolgt auf Grundlage Ihrer Einwilligung gemäß Art. 6 Abs. 1 lit. a. DSGVO und § 25 Abs. 1 TTDSG.\nWir beabsichtigen personenbezogenen Daten an Drittländer außerhalb des Europäischen Wirtschaftsraums, insbesondere die USA, zu übermitteln. In Fällen, in denen kein Angemessenheitsbeschluss der Europäischen Kommission existiert (z.B. in den USA) haben wir mit den Empfängern der Daten anderweitige geeignete Garantien im Sinne der Art. 44 ff. DSGVO vereinbart. Dies sind – sofern nicht anders angegeben – Standardvertragsklauseln der EU-Kommission gemäß Durchführungsbeschluss (EU) 2021/914 vom 4. Juni 2021. Eine Kopie dieser Standardvertragsklauseln können Sie unter https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32021D0914\u0026amp;from=DE einsehen.\nZudem holen wir vor einem solchen Drittlandtransfer Ihre Einwilligung nach Art. 49 Abs. 1 Satz 1 lit. a. DSGVO ein, die Sie über die Einwilligung im Consent Manager (oder sonstigen Formularen, Registrierungen etc.) erteilen. Wir weisen Sie darauf hin, dass bei Drittlandübermittlungen im Detail unbekannte Risiken (z.B. die Datenverarbeitung durch Sicherheitsbehörden des Drittlandes, deren genauer Umfang und deren Folgen für Sie wir nicht kennen, auf die wir keinen Einfluss haben und von denen Sie unter Umständen keine Kenntnis erlangen) bestehen können.\nSpeicherdauer # Die konkrete Speicherdauer der verarbeiteten Daten ist nicht durch uns beeinflussbar, sondern wird von Vimeo, LLC bestimmt. Weitere Hinweise finden Sie in der Datenschutzerklärung für Vimeo Video: https://vimeo.com/privacy.\nCloudflare CDN # Art und Umfang der Verarbeitung # Wir verwenden zur ordnungsgemäßen Bereitstellung der Inhalte unserer Website Cloudflare CDN. Cloudflare CDN ist ein Dienst der Cloudflare, Inc., welcher auf unserer Website als Content Delivery Network (CDN) fungiert.\nEin CDN trägt dazu bei, Inhalte unseres Onlineangebotes, insbesondere Dateien wie Grafiken oder Skripte, mit Hilfe regional oder international verteilter Server schneller bereitzustellen. Wenn Sie auf diese Inhalte zugreifen, stellen Sie eine Verbindung zu Servern der Cloudflare, Inc., her, wobei Ihre IP-Adresse und ggf. Browserdaten wie Ihr User-Agent übermittelt werden. Diese Daten werden ausschließlich zu den oben genannten Zwecken und zur Aufrechterhaltung der Sicherheit und Funktionalität von Cloudflare CDN verarbeitet.\nZweck und Rechtsgrundlage # Die Nutzung des Content Delivery Networks erfolgt auf Grundlage unserer berechtigten Interessen, d.h. Interesse an einer sicheren und effizienten Bereitstellung sowie der Optimierung unseres Onlineangebotes gemäß Art. 6 Abs. 1 lit. f. DSGVO.\nWir beabsichtigen personenbezogenen Daten an Drittländer außerhalb des Europäischen Wirtschaftsraums, insbesondere die USA, zu übermitteln. In Fällen, in denen kein Angemessenheitsbeschluss der Europäischen Kommission existiert (z.B. in den USA) haben wir mit den Empfängern der Daten anderweitige geeignete Garantien im Sinne der Art. 44 ff. DSGVO vereinbart. Dies sind – sofern nicht anders angegeben – Standardvertragsklauseln der EU-Kommission gemäß Durchführungsbeschluss (EU) 2021/914 vom 4. Juni 2021. Eine Kopie dieser Standardvertragsklauseln können Sie unter https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32021D0914\u0026amp;from=DE einsehen.\nSpeicherdauer # Die konkrete Speicherdauer der verarbeiteten Daten ist nicht durch uns beeinflussbar, sondern wird von Cloudflare, Inc. bestimmt. Weitere Hinweise finden Sie in der Datenschutzerklärung für Cloudflare CDN: https://www.cloudflare.com/privacypolicy/.\nGoogle Maps # Art und Umfang der Verarbeitung # Wir verwenden zur Erstellung von Anfahrtsbeschreibungen den Kartendienst Google Maps. Google Maps ist ein Dienst der Google Ireland Limited, welcher auf unserer Website eine Karte darstellt. Wenn Sie auf diese Inhalte unserer Website zugreifen, stellen Sie eine Verbindung zu Servern der Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Irland her, wobei Ihre IP-Adresse und ggf. Browserdaten wie Ihr User-Agent übermittelt werden. Diese Daten werden ausschließlich zu den oben genannten Zwecken und zur Aufrechterhaltung der Sicherheit und Funktionalität von Google Maps verarbeitet.\nZweck und Rechtsgrundlage # Der Einsatz von Google Maps erfolgt auf Grundlage Ihrer Einwilligung gemäß Art. 6 Abs. 1 lit. a. DSGVO und § 25 Abs. 1 TTDSG.\nWir beabsichtigen personenbezogenen Daten an Drittländer außerhalb des Europäischen Wirtschaftsraums, insbesondere die USA, zu übermitteln. In Fällen, in denen kein Angemessenheitsbeschluss der Europäischen Kommission existiert (z.B. in den USA) haben wir mit den Empfängern der Daten anderweitige geeignete Garantien im Sinne der Art. 44 ff. DSGVO vereinbart. Dies sind – sofern nicht anders angegeben – Standardvertragsklauseln der EU-Kommission gemäß Durchführungsbeschluss (EU) 2021/914 vom 4. Juni 2021. Eine Kopie dieser Standardvertragsklauseln können Sie unter https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32021D0914\u0026amp;from=DE einsehen.\nZudem holen wir vor einem solchen Drittlandtransfer Ihre Einwilligung nach Art. 49 Abs. 1 Satz 1 lit. a. DSGVO ein, die Sie über die Einwilligung im Consent Manager (oder sonstigen Formularen, Registrierungen etc.) erteilen. Wir weisen Sie darauf hin, dass bei Drittlandübermittlungen im Detail unbekannte Risiken (z.B. die Datenverarbeitung durch Sicherheitsbehörden des Drittlandes, deren genauer Umfang und deren Folgen für Sie wir nicht kennen, auf die wir keinen Einfluss haben und von denen Sie unter Umständen keine Kenntnis erlangen) bestehen können.\nSpeicherdauer # Die konkrete Speicherdauer der verarbeiteten Daten ist nicht durch uns beeinflussbar, sondern wird von Google Ireland Limited bestimmt. Weitere Hinweise finden Sie in der Datenschutzerklärung für Google Maps: https://policies.google.com/privacy.\nGoogle Web Fonts # Art und Umfang der Verarbeitung # Wir verwenden Google Fonts von Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Irland, als Dienst zur Bereitstellung von Schriftarten für unser Onlineangebot. Um diese Schriftarten zu beziehen, stellen Sie eine Verbindung zu Servern von Google Ireland Limited her, wobei Ihre IP-Adresse übertragen wird.\nZweck und Rechtsgrundlage # Der Einsatz von Google Fonts erfolgt auf Grundlage Ihrer Einwilligung gemäß Art. 6 Abs. 1 lit. a. DSGVO und § 25 Abs. 1 TTDSG.\nWir beabsichtigen personenbezogenen Daten an Drittländer außerhalb des Europäischen Wirtschaftsraums, insbesondere die USA, zu übermitteln. In Fällen, in denen kein Angemessenheitsbeschluss der Europäischen Kommission existiert (z.B. in den USA) haben wir mit den Empfängern der Daten anderweitige geeignete Garantien im Sinne der Art. 44 ff. DSGVO vereinbart. Dies sind – sofern nicht anders angegeben – Standardvertragsklauseln der EU-Kommission gemäß Durchführungsbeschluss (EU) 2021/914 vom 4. Juni 2021. Eine Kopie dieser Standardvertragsklauseln können Sie unter https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32021D0914\u0026amp;from=DE einsehen.\nZudem holen wir vor einem solchen Drittlandtransfer Ihre Einwilligung nach Art. 49 Abs. 1 Satz 1 lit. a. DSGVO ein, die Sie über die Einwilligung im Consent Manager (oder sonstigen Formularen, Registrierungen etc.) erteilen. Wir weisen Sie darauf hin, dass bei Drittlandübermittlungen im Detail unbekannte Risiken (z.B. die Datenverarbeitung durch Sicherheitsbehörden des Drittlandes, deren genauer Umfang und deren Folgen für Sie wir nicht kennen, auf die wir keinen Einfluss haben und von denen Sie unter Umständen keine Kenntnis erlangen) bestehen können.\nSpeicherdauer # Die konkrete Speicherdauer der verarbeiteten Daten ist nicht durch uns beeinflussbar, sondern wird von Google Ireland Limited bestimmt. Weitere Hinweise finden Sie in der Datenschutzerklärung für Google Fonts: https://policies.google.com/privacy.\nFont Awesome # Art und Umfang der Verarbeitung # Wir verwenden zur ordnungsgemäßen Bereitstellung der Inhalte unserer Website Font Awesome des Anbieters Fonticons, Inc..\nZweck und Rechtsgrundlage # Der Einsatz von Font Awesome erfolgt auf Grundlage Ihrer Einwilligung gemäß Art. 6 Abs. 1 lit. a. DSGVO und § 25 Abs. 1 TTDSG.\nWir beabsichtigen personenbezogenen Daten an Drittländer außerhalb des Europäischen Wirtschaftsraums, insbesondere die USA, zu übermitteln. In Fällen, in denen kein Angemessenheitsbeschluss der Europäischen Kommission existiert (z.B. in den USA) haben wir mit den Empfängern der Daten anderweitige geeignete Garantien im Sinne der Art. 44 ff. DSGVO vereinbart. Dies sind – sofern nicht anders angegeben – Standardvertragsklauseln der EU-Kommission gemäß Durchführungsbeschluss (EU) 2021/914 vom 4. Juni 2021. Eine Kopie dieser Standardvertragsklauseln können Sie unter https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32021D0914\u0026amp;from=DE einsehen.\nZudem holen wir vor einem solchen Drittlandtransfer Ihre Einwilligung nach Art. 49 Abs. 1 Satz 1 lit. a. DSGVO ein, die Sie über die Einwilligung im Consent Manager (oder sonstigen Formularen, Registrierungen etc.) erteilen. Wir weisen Sie darauf hin, dass bei Drittlandübermittlungen im Detail unbekannte Risiken (z.B. die Datenverarbeitung durch Sicherheitsbehörden des Drittlandes, deren genauer Umfang und deren Folgen für Sie wir nicht kennen, auf die wir keinen Einfluss haben und von denen Sie unter Umständen keine Kenntnis erlangen) bestehen können.\nSpeicherdauer # Die konkrete Speicherdauer der verarbeiteten Daten ist nicht durch uns beeinflussbar, sondern wird von Fonticons, Inc. bestimmt. Weitere Hinweise finden Sie in der Datenschutzerklärung für Font Awesome CDN: https://cdn.fontawesome.com/privacy.\n","externalUrl":null,"permalink":"/privacypolicy/","section":"DominikBritz.com","summary":"Datenschutzerklärung # Allgemeiner Hinweis und Pflichtinformationen # Benennung der verantwortlichen Stelle # Die verantwortliche Stelle für die Datenverarbeitung auf dieser Website ist:\n","title":"","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"}]