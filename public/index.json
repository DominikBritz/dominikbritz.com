[{"content":"","date":"24 March 2024","permalink":"/","section":"DominikBritz.com","summary":"","title":"DominikBritz.com"},{"content":"","date":"24 March 2024","permalink":"/tags/home-server/","section":"Tags","summary":"","title":"home server"},{"content":"","date":"24 March 2024","permalink":"/series/home-server/","section":"Series","summary":"","title":"Home Server"},{"content":"","date":"24 March 2024","permalink":"/tags/immich/","section":"Tags","summary":"","title":"immich"},{"content":"Take Control Over Your Memories #\rPreserving photos for eternity is the ultimate challenge in a personal context. You want to make sure that your children, friends, and family photos will never be lost.\nIn the past, I have set up a cascade of external hard drives to store sensitive data, so that if one or more failed, a complete backup would always be available. However, there was always a nagging question in my mind that what if the house and all the hard drives caught fire?\nToday\u0026rsquo;s world has a lot of public cloud services that take care of this concern and give you the peace of mind that your pics are safe, no matter what. Google Photos is the market leader, and rightly so, as the app works exceptionally well — especially the facial recognition and memories features that have become so popular in our household.\nBut, that comes with a cost: you are no longer in control over your most precious data. I wanted my control back and started searching for alternatives.\nPhoto Gallery Requirements #\rPicking out a photo gallery is important, as you intend to keep it for a long time. Below is my list of criteria:\nSelf-hosted: must be running on my home server and not in the cloud Actively maintained: when Google started charging money for Google Photos, the open-source community quickly started creating alternatives. Some projects started out promising, but were abandoned quickly. I want a solution with a big community and active developers that will be here in a few years. Open-source: Synology, the vendor of my former home server, has a viable solution that ticks all boxes, but you have no influence on the development, nor can you see the current state or a roadmap. I want a solution where I can share ideas, discuss features, and see where the project is going to. Native app with upload functionality: a native app for mobile phones that is easy to use and uploads photos automatically to my home server. Using an additional app like Syncthing does not adhere to the WAF (wife acceptance factor). Facial recognition: group photos by people, place, and time Multi-user and sharing: I\u0026rsquo;m not the sole user of the service. At least my wife is using it, too. The service must have multi-user support and excellent sharing functionality. Memories: present memories from the past years in the mobile app. That\u0026rsquo;s a hard non-negotiable factor in my family. OIDC support: support single sign-on through OIDC with Authelia Photo Gallery Selection #\rBelow is a list of galleries that I\u0026rsquo;ve checked against my requirements. For a more detailed comparison, refer to this table.\nImmich: ticks all boxes, but is under active development. There may be breaking changes along the way, according to their website.\nLibrephotos: no native automatic mobile upload\nNextcloud and Nextcloud Memories: I use Nextcloud for files (\rdetails), so using it for photos as well is close. But I think that software should concentrate on a few things and then do them really well, rather than wanting to be a jack of all trades.\nPhotoprism: it\u0026rsquo;s a progressive web app. Uploads are handled through a third-party app. Also, certain features are only available on paid plans.\nFrom the above, Immich and Nextcloud have all the features I require. I prefer Immich\u0026rsquo;s design over Nextcloud\u0026rsquo;s though, as Immich looks nearly identical to Google Photos. After some research, it turned out that \u0026ldquo;breaking changes\u0026rdquo; in Immich means that you have to change something here and there in the Docker compose file and update it carefully. I have the confidence to do that. If that doesn\u0026rsquo;t suit you, you\u0026rsquo;re probably better off with Nextcloud.\nImmich Installation in Docker Container #\rPreparation #\rWe have to prepare a few things before we can create the stack.\nLimited User And Group #\rAs with Nextcloud, Immich should be running with limited permissions. Hence, we run this stack with the dockerlimited user (\rmore info).\nFiles And Folders #\rIn your WSL client, create a new Ansible role for Immich.\nmkdir -p roles/immich/tasks vim roles/immich/tasks/main.yml The playbook below is similar to my Nextcloud one, and one could argue that creating the dockerlimited user again makes no sense. But, that\u0026rsquo;s the beauty of Ansible\u0026rsquo;s idempotent nature: if the user already exists, it skips the step.\n# main.yml # Ansible role for Immich - name: Create user dockerlimited with UID 2000 user: name: dockerlimited # change to your preferred username uid: 2000 # change to your preferred UID - name: Create directories for Immich file: path: \u0026#34;/usr/local/data/docker/{{ item }}\u0026#34; # change to your preferred directory state: directory owner: dockerlimited group: dockerlimited mode: 0755 # owner has full read, write, and execute permissions, while the group and others have read and execute permissions, but no write permissions loop: - immich/data # upload storage - name: Create compose.yaml file file: path: \u0026#34;/usr/local/data/docker/immich/compose.yaml\u0026#34; state: touch owner: dockerlimited group: dockerlimited mode: 0644 # Owner has read and write permissions, while the group and others have read permissions Add the new role to the home.yml playbook in the root:\n--- - name: home server hosts: localhost become: true vars_files: - vars_files/home.yml roles: - base - home - oefenweb.ufw - geerlingguy.docker - geerlingguy.ntp - geerlingguy.security - geerlingguy.node_exporter - nextcloud - immich Push the changes to GitHub (\rdetails).\ngitall Deploy With Ansible #\rOn your home server, deploy the just created Immich role.\nexport PAT=YOUR_PAT_TOKEN sudo ansible-pull -U https://$PAT:x-oauth-basic@github.com/Username/repo-name home.yml Compose File #\rIn Dockge, click Scan Stacks Folder in the user menu at the top right. The immich stack should appear in the list.\nAt the time I was writing this article, my Immich server was on version 1.99. The compose file below is valid for this version. As Immich is under active development, the contents of the compose file change. Please check https://github.com/immich-app/immich/releases/latest/download/docker-compose.yml for the latest compose file.\nNote the following lines:\nlabels: - com.centurylinklabs.watchtower.enable=false I update all my Docker images with Watchtower every night. Immich must be excluded with com.centurylinklabs.watchtower.enable=false, as sometimes changes to the compose file are required when you go to the next version. I update Immich manually.\nuser: 2000:2000 This line is not in the original compose file. Change it to the IDs of your dockerlimited user.\nversion: \u0026#34;3.8\u0026#34; # # WARNING: Make sure to use the docker-compose.yml of the current release: # # https://github.com/immich-app/immich/releases/latest/download/docker-compose.yml # # The compose file on main may not be compatible with the latest release. # name: immich services: immich-server: container_name: immich_server hostname: photos user: 2000:2000 image: ghcr.io/immich-app/immich-server:${IMMICH_VERSION:-release} command: - start.sh - immich volumes: - ${UPLOAD_LOCATION}:/usr/src/app/upload - /etc/localtime:/etc/localtime:ro env_file: - .env expose: - 3001 depends_on: - redis - database restart: unless-stopped networks: - caddy_caddynet labels: - com.centurylinklabs.watchtower.enable=false immich-microservices: container_name: immich_microservices user: 2000:2000 image: ghcr.io/immich-app/immich-server:${IMMICH_VERSION:-release} # extends: # file: hwaccel.yml # service: hwaccel command: - start.sh - microservices volumes: - ${UPLOAD_LOCATION}:/usr/src/app/upload - /etc/localtime:/etc/localtime:ro env_file: - .env depends_on: - redis - database restart: unless-stopped networks: - caddy_caddynet labels: - com.centurylinklabs.watchtower.enable=false immich-machine-learning: container_name: immich_machine_learning image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_VERSION:-release} volumes: - model-cache:/cache env_file: - .env restart: unless-stopped networks: - caddy_caddynet labels: - com.centurylinklabs.watchtower.enable=false redis: container_name: immich_redis image: redis:6.2-alpine@sha256:b6124ab2e45cc332e16398022a411d7e37181f21ff7874835e0180f56a09e82a restart: unless-stopped environment: [] networks: - caddy_caddynet database: container_name: immich_postgres image: tensorchord/pgvecto-rs:pg14-v0.2.0@sha256:90724186f0a3517cf6914295b5ab410db9ce23190a2d9d0b9dd6463e3fa298f0 env_file: - .env environment: POSTGRES_PASSWORD: ${DB_PASSWORD} POSTGRES_USER: ${DB_USERNAME} POSTGRES_DB: ${DB_DATABASE_NAME} volumes: - pgdata:/var/lib/postgresql/data restart: unless-stopped networks: - caddy_caddynet labels: - com.centurylinklabs.watchtower.enable=false volumes: pgdata: null model-cache: null networks: caddy_caddynet: external: true env File #\rThe .env file holds configurations for the Immich stack and can be edited in the Dockge UI directly.\nGenerate a random alphanumeric string to be used as database password:\ntr -cd \u0026#39;[:alnum:]\u0026#39; \u0026lt; /dev/urandom | fold -w \u0026#34;32\u0026#34; | head -n 1 # You can find documentation for all the supported env variables at https://immich.app/docs/install/environment-variables # The location where your uploaded files are stored UPLOAD_LOCATION=/media/18TB/photos/library # The Immich version to use. You can pin this to a specific version like \u0026#34;v1.71.0\u0026#34; IMMICH_VERSION=release # Connection secret for postgres. You should change it to a random password DB_PASSWORD=your_password # The values below this line do not need to be changed ################################################################################### DB_HOSTNAME=immich_postgres DB_USERNAME=postgres DB_DATABASE_NAME=immich REDIS_HOSTNAME=immich_redis Let’s Encrypt Certificate via Caddy #\rCaddyfile #\rAdd the following to Caddyfile (\rdetails):\nphotos.{$MY_DOMAIN} { reverse_proxy immich:3001 tls { dns netcup { customer_number {env.NETCUP_CUSTOMER_NUMBER} api_key {env.NETCUP_API_KEY} api_password {env.NETCUP_API_PASSWORD} } propagation_timeout 900s propagation_delay 600s resolvers 1.1.1.1 } } DNS A Record #\rAdd the following A record to your DNS domain:\nphotos.home.yourdomain.com 192.168.178.254 # replace with your Docker host\u0026#39;s IP address Reload Caddy’s Configuration #\rInstruct Caddy to reload its configuration by running:\ndocker exec -w /etc/caddy caddy caddy reload Immich is now accessible via https://photos.home.yourdomain.com.\nPost Install Steps #\rFollow the official post install steps to:\nCreate your user as the admin user. Create additional users (optional) Update the storage template Download and set up the mobile app SSO to Immich via OpenID Connect (OIDC) Authentication to Authelia #\rThis section describes how to set up single sign-on to Immich via OpenID Connect authentication to Authelia.\nClient Secret #\rGenerate a random alphanumeric string to be used as client secret:\ntr -cd \u0026#39;[:alnum:]\u0026#39; \u0026lt; /dev/urandom | fold -w \u0026#34;64\u0026#34; | head -n 1 Authelia Configuration File #\rAdd the following to the oidc: section of Authelia’s configuration file config/configuration.yml:\nclients: - id: immich description: immich secret: \u0026#39;your secret\u0026#39; scopes: - openid - profile - email redirect_uris: - https://photos.home.yourdomain.com/auth/login - https://photos.home.yourdomain.com/user-settings - app.immich:/ Restart Authelia #\rWe changed the container’s environment, which makes it necessary to recreate the container (stopping and starting is not enough). Navigate to the Authelia stack in Dockge and click Stop \u0026amp; Inactive. Then start the stack.\nInspect Dockge\u0026rsquo;s terminal window for any errors.\nImmich Configuration #\rIn Immich, go to Administration → Settings → OAuth Authentication and configure the following:\nEnable: yes Issuer URL: https://authelia.home.yourdomain.com Client ID: Immich Client secret: your secret Scope: openid email profile Signing algorithm: RS256 Storage label claim: preferred_username Storage quota claim: immich_quota Default storage quota: you may define a quota here Auto register: yes Auto launch: yes Mobile redirect uri override: no Restart the Immich stack through Dockge. Log in with your user via Authelia.\nMigrate from Google Photos #\rThe community created a command-line tool to import your Google photos into Immich.\nExport your Google photos with Google Takeout. Read the best practices first. Import the exported zip files with immich-go Migrate from Synology Photos #\rI had many photos stored in a directory structure containing years and event names (like 2024\\2024-01-01 happy new year) on my old Synology NAS, which I moved to Immich as well. While I moved off a Synology, the instructions below can be used for any folder-based structure.\nClean the Import #\rI did not do a great job organizing my photos on the Synology and had numerous duplicates. I took the migration to Immich as an opportunity to fix that.\nCzkawka was very helpful in cleaning my files before importing.\nFile Copy Operations #\rMount the Synology photo share and copy the files to your home server.\n# Mount the Synology share on my home server mkdir /mnt/synology_photos mount -t cifs -o user=username //synology/photo /mnt/synology_photos # Copy the files to my external media drive mkdir /media/18TB/photos/from_synology rsync -avh /mnt/synology_photos/ /media/18TB/photos/from_synology Update The Compose File #\rAdd the from_synology folder as a read-only volume to the immich-server and immich-microservices services in the compose.yaml.\nvolumes: - /media/18TB/photos/from_synology:/mnt/media/from_synology:ro Add External Library in Immich #\rExternal libraries in Immich are perfect for adding existing photos. From their documentation:\nExternal libraries tracks assets stored outside of immich, i.e. in the file system. Immich will only read data from the files, and will not modify them in any way. Therefore, the delete button is disabled for external assets. When the external library is scanned, immich will read the metadata from the file and create an asset in the library for each image or video file. These items will then be shown in the main timeline, and they will look and behave like any other asset, including viewing on the map, adding to albums, etc.\nGo to Administration → External Libraries and add /mnt/media/from_synology as a library.\nAfter Immich has processed all files, they appear in your timeline.\nCloud Backup #\rA good backup is crucial for photos. I back up my data encrypted to the cloud with resticprofile.\nAccording to the Immich documentation, one must back up the database and a few folders in the file system.\nDatabase #\rAdd the pg_dumpall command to the run-before section of your resticprofile configuration in /usr/local/data/resticprofile/profiles.yaml.\nrun-before: - \u0026#34;docker exec -t immich_postgres pg_dumpall -c -U postgres | gzip \u0026gt; /usr/local/data/docker/immich/db_dump.sql.gz\u0026#34; - \u0026#34;systemctl stop docker.socket\u0026#34; - \u0026#34;systemctl stop docker\u0026#34; Verify that the Docker folder is part of the source section.\nsource: - \u0026#34;/usr/local/data/docker\u0026#34; File System #\rIf you place the uploaded photos in the stack folder, i.e., /usr/local/data/docker/immich, add /usr/local/data/docker/immich/thumbs and /usr/local/data/docker/immich/encoded-video to the excludes file (\rmore info).\nIf you don\u0026rsquo;t place the uploaded photos in the stack folder, like me, add the following paths to the source section. Replace UPLOAD_LOCATION with the path set in your .env file.\nUPLOAD_LOCATION/library UPLOAD_LOCATION/upload UPLOAD_LOCATION/profile Also, add any external library.\nHere is my source section in the profiles.yaml:\nsource: - \u0026#34;/usr/local/data/docker\u0026#34; - \u0026#34;/usr/local/data/resticprofile\u0026#34; - \u0026#34;/media/18TB/photos/library/library\u0026#34; - \u0026#34;/media/18TB/photos/library/upload\u0026#34; - \u0026#34;/media/18TB/photos/library/profile\u0026#34; - \u0026#34;/media/18TB/photos/from_synology\u0026#34; - \u0026#34;/etc/unbound/unbound.conf\u0026#34; ","date":"24 March 2024","permalink":"/posts/immich-self-hosted-photos-with-cloud-backup/","section":"Posts","summary":"Take Control Over Your Memories #\rPreserving photos for eternity is the ultimate challenge in a personal context. You want to make sure that your children, friends, and family photos will never be lost.\nIn the past, I have set up a cascade of external hard drives to store sensitive data, so that if one or more failed, a complete backup would always be available.","title":"Immich: Self-Hosted Photo Gallery With Cloud Backup \u0026 OpenID Connect"},{"content":"","date":"24 March 2024","permalink":"/tags/openid-connect/","section":"Tags","summary":"","title":"OpenID Connect"},{"content":"","date":"24 March 2024","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"24 March 2024","permalink":"/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"24 March 2024","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"5 February 2024","permalink":"/tags/nextcloud/","section":"Tags","summary":"","title":"nextcloud"},{"content":"This article explains how to set up a fast and efficient Nextcloud as file management tool with OpenID Connect to Authelia.\nIt is part of my series on creating a home server on an old laptop. I\u0026rsquo;m assuming that you\u0026rsquo;ve set up Docker, Caddy, and Authelia, as described in the first article of the series.\nWhy Nextcloud? #\rMy previous home server was a Synology NAS, that came with a file management/syncing solution, which worked pretty well. It has all the nice features like selective sync, sharing, requesting files, and it was fast and fun to work with. Before I moved to a custom home server, I did some research on possible solutions with similar features and came up with ownCloud and Nextcloud.\nownCloud vs. Nextcloud #\rownCloud and Nextcloud used to be one solution but then went in different directions. In the first article of the series, I\u0026rsquo;ve written that my home server is based on Helge Klein\u0026rsquo;s series on home automation and if you are following his series, you know that he decided to go with ownCloud Infinite Scale (oCIS) instead of Nextcloud. I tested ownCloud, but was not convinced.\nBlob Storage #\rownCloud kidnaps your files and stores them in blob storage format. You can\u0026rsquo;t access your files in the classic way by browsing the file system. I understand the technical background, but I don\u0026rsquo;t like the fact that I can\u0026rsquo;t access my data directly, but always have to go through an ownCloud client.\nExternal Storage #\roCIS can\u0026rsquo;t integrate other storage types like WebDAV, not even a folder on your local disks.\nCommunity #\roCIS is relatively new compared to Nextcloud and there doesn\u0026rsquo;t seem to be a huge community. I had some issues while testing ownCloud and if you search for something on Google but get exactly 0 results, that makes me wonder.\nNextcloud on the other hand has a large community and you can be sure that every question you have is already answered somewhere on the Internet.\nStreamlining Nextcloud #\rI only need Nextcloud to synchronise files. But the makers seem to think it\u0026rsquo;s a good idea to pack everything that can be programmed into Nextcloud. It\u0026rsquo;s not for nothing that Nextcloud has a reputation for eating up plenty of resources.\nTo top it all off, there is a dedicated Nextcloud All-In-One project including antivirus, backup, and image updates, to name just a few. Ridiculously, it\u0026rsquo;s the recommended method to deploy Nextcloud.\nMy Nextcloud build, instead, is focused on the core functionality, file syncing, as well as performance. When you need additional features, you can simply turn them on.\nNextcloud Installation in Docker Container #\rPreparation #\rWe have to prepare a few things before we can create the stack.\nLimited User And Group #\rDocker is running as root and so are the containers. If the image supports it, you can specify another user and the container will access the file system with that user.\nNextcloud is one of the services I want to be accessible externally (future article) hence running it with root permissions is a no-go.\nStatic User ID #\rUsers and groups get IDs on Ubuntu. System users get an ID below 1000. Your user most likely has the ID 1000 as you were the first user on the machine. When you create a user, Ubuntu assignes the next available ID. For our limited user we want something static and something that\u0026rsquo;s not already taken on the current machine and on any other machine you might deploy in the future. We want that because the restic backup includes permissions based on IDs. So when you restore your backup on a new machine, the ID of the user in the backup and the one on the new machine must match to be able to access files.\n2000 is easy to remember and on a home server with few users most likely free. If you want to be sure, you can check the allocated IDs first.\nsudo cut -d: -f1,3 /etc/passwd Files And Folders #\rIn your WSL client, create a new Ansible role for Nextcloud.\nmkdir -p roles/nextcloud/tasks vim roles/nextcloud/tasks/main.yml # main.yml # Ansible role for Nextcloud - name: Create user dockerlimited with UID 2000 user: name: dockerlimited # change to your preferred username uid: 2000 # change to your preferred UID - name: Create directories for Nextcloud file: path: \u0026#34;/usr/local/data/docker/{{ item }}\u0026#34; # change to your preferred directory state: directory owner: dockerlimited group: dockerlimited mode: 0755 # owner has full read, write, and execute permissions, while the group and others have read and execute permissions, but no write permissions loop: - nextcloud/secrets # files containing secrets for Nextcloud - nextcloud/config # config for Nextcloud - nextcloud/data # your files for Nextcloud - name: Create compose.yaml file file: path: \u0026#34;/usr/local/data/docker/nextcloud/compose.yaml\u0026#34; state: touch owner: dockerlimited group: dockerlimited mode: 0644 # Owner has read and write permissions, while the group and others have read permissions # generate a random 32 character alphanumeric string for the Nextcloud database password and the Nextcloud database root password. # 32 is the max for MyQSL. Reduce the length if you run in to issues. - name: Generate MySQL password for Nextcloud shell: \u0026#34;tr -cd \u0026#39;[:alnum:]\u0026#39; \u0026lt; /dev/urandom | fold -w \u0026#39;32\u0026#39; | head -n 1 \u0026gt; /usr/local/data/docker/nextcloud/secrets/MYSQL_PASSWORD\u0026#34; args: creates: \u0026#34;/usr/local/data/docker/nextcloud/secrets/MYSQL_PASSWORD\u0026#34; # run only if not exists - name: Generate MySQL root password for Nextcloud shell: \u0026#34;tr -cd \u0026#39;[:alnum:]\u0026#39; \u0026lt; /dev/urandom | fold -w \u0026#39;32\u0026#39; | head -n 1 \u0026gt; /usr/local/data/docker/nextcloud/secrets/MYSQL_ROOT_PASSWORD\u0026#34; args: creates: \u0026#34;/usr/local/data/docker/nextcloud/secrets/MYSQL_ROOT_PASSWORD\u0026#34; # run only if not exists Add the new role to the home.yml playbook in the root:\n--- - name: home server hosts: localhost become: true vars_files: - vars_files/home.yml roles: - base - home - oefenweb.ufw - geerlingguy.docker - geerlingguy.ntp - geerlingguy.security - geerlingguy.node_exporter - nextcloud Push the changes to GitHub (\rdetails).\ngitall Deploy With Ansible #\rOn your home server, deploy the just created Nextcloud role.\nexport PAT=YOUR_PAT_TOKEN sudo ansible-pull -U https://$PAT:x-oauth-basic@github.com/Username/repo-name home.yml Compose File #\rIf available, I prefer Docker images from linuxserver.io, as they are updated regularly with the latest security patches, the documentation is excellent, and they support using different user IDs than root. The latter is crucial to run the container with the dockerlimited user.\nIn Dockge, click Scan Stacks Folder in the user menu top right. The nextcloud stack should appear in the list. Paste the following contents in the compose.yaml section:\nservices: nextcloud: image: lscr.io/linuxserver/nextcloud:latest container_name: nextcloud hostname: nextcloud volumes: - ./config:/config - ./data:/data # the folder where files are placed - /media/18TB:/media # add as many local folders as you like and access/share them via nextcloud. Remember that UID=2000 needs read/write permissions. expose: - 80 # default is 443 with a self-signed certificate. We use 80 and create the certificate through Caddy. restart: unless-stopped networks: - caddy_caddynet mariadb: image: lscr.io/linuxserver/mariadb:latest container_name: nextcloud-mariadb hostname: nextcloud-mariadb volumes: - ./db-config:/config expose: - 3306 restart: unless-stopped networks: - caddy_caddynet redis: container_name: nextcloud-redis hostname: nextcloud-redis image: redis:latest restart: unless-stopped networks: - caddy_caddynet expose: - 6379 volumes: - ./redis-data:/data networks: caddy_caddynet: external: true Add the following variables to the env section:\nPUID=2000 # UID of the dockerlimited user PGID=2000 # GID of the dockerlimited group TZ=Europe/Berlin # Your timezone MYSQL_DATABASE=nextcloud MYSQL_USER=nextcloud FILE__MYSQL_PASSWORD=/secrets/MYSQL_PASSWORD FILE__MYSQL_ROOT_PASSWORD=/secrets/MYSQL_ROOT_PASSWORD Deploy the stack through the Dockge UI. Check the terminal window for any errors and verify that the container is running with your UID.\nnextcloud | ─────────────────────────────────────── nextcloud | nextcloud | ██╗ ███████╗██╗ ██████╗ nextcloud | ██║ ██╔════╝██║██╔═══██╗ nextcloud | ██║ ███████╗██║██║ ██║ nextcloud | ██║ ╚════██║██║██║ ██║ nextcloud | ███████╗███████║██║╚██████╔╝ nextcloud | ╚══════╝╚══════╝╚═╝ ╚═════╝ nextcloud | nextcloud | Brought to you by linuxserver.io nextcloud | ─────────────────────────────────────── nextcloud | nextcloud | To support LSIO projects visit: nextcloud | https://www.linuxserver.io/donate/ nextcloud | nextcloud | ─────────────────────────────────────── nextcloud | GID/UID nextcloud | ─────────────────────────────────────── nextcloud | nextcloud | User UID: 2000 nextcloud | User GID: 2000 nextcloud | ─────────────────────────────────────── Let’s Encrypt Certificate via Caddy #\rCaddyfile #\rAdd the following to Caddyfile (\rdetails):\nnextcloud.{$MY_DOMAIN} { reverse_proxy nextcloud:80 tls { dns netcup { customer_number {env.NETCUP_CUSTOMER_NUMBER} api_key {env.NETCUP_API_KEY} api_password {env.NETCUP_API_PASSWORD} } propagation_timeout 900s propagation_delay 600s resolvers 1.1.1.1 } } DNS A Record #\rAdd the following A record to your DNS domain:\nnextcloud.home.yourdomain.com 192.168.178.254 # replace with your Docker host\u0026#39;s IP address Reload Caddy’s Configuration #\rInstruct Caddy to reload its configuration by running:\ndocker exec -w /etc/caddy caddy caddy reload Nextcloud Setup #\rBrowse to nextcloud.home.yourdomain.com and fill out the necessary fields in the setup screen. Take the database password from the /secrets/MYSQL_PASSWORD file.\nNextcloud setup\rIf you can\u0026rsquo;t log in with your admin credentials after the setup, try logging in with a private browser window.\nChoose Skip when Nextclouds asks you to install recommended apps.\nOn the top right, click the user icon and choose Apps in the menu. Disable all apps except the ones on the list below.\nComments Deleted files File sharing Log Reader Notifications Password policy PDF viewer Privacy Text Versions In the list of apps find and install the apps External storage support, Group folders, and OpenID Connect Login.\nShut down the stack.\nBest Practice Settings #\rOpen Nextcloud\u0026rsquo;s configuration file.\nvim /usr/local/data/docker/nextcloud/config/www/nextcloud/config/config.php Add the following settings to the $CONFIG = array ( block.\n// disable upgrade notifications in nextcloud web \u0026#39;upgrade.disable-web\u0026#39; =\u0026gt; true, // disable \u0026#34;welcome\u0026#34; files for new users \u0026#39;skeletondirectory\u0026#39; =\u0026gt; \u0026#39;\u0026#39;, \u0026#39;templatedirectory\u0026#39; =\u0026gt; \u0026#39;\u0026#39;, // route users directly to the files view \u0026#39;defaultapp\u0026#39; =\u0026gt; \u0026#39;files\u0026#39;, // language \u0026#39;default_language\u0026#39; =\u0026gt; \u0026#39;de\u0026#39;, // delete all files in the trash bin that are older than 3 days automatically, delete other files anytime if space needed \u0026#39;trashbin_retention_obligation\u0026#39; =\u0026gt; \u0026#39;auto, 3\u0026#39;, E-Mail Settings #\rI\u0026rsquo;ve configured Gmail as my SMTP server. Follow these instructions to activate Gmail SMTP.\n// config.php // mail \u0026#39;mail_smtpmode\u0026#39; =\u0026gt; \u0026#39;smtp\u0026#39;, \u0026#39;mail_sendmailmode\u0026#39; =\u0026gt; \u0026#39;smtp\u0026#39;, \u0026#39;mail_smtpauth\u0026#39; =\u0026gt; 1, \u0026#39;mail_smtphost\u0026#39; =\u0026gt; \u0026#39;smtp.gmail.com\u0026#39;, \u0026#39;mail_smtpport\u0026#39; =\u0026gt; \u0026#39;587\u0026#39;, \u0026#39;mail_from_address\u0026#39; =\u0026gt; \u0026#39;your email without the @gmail.com\u0026#39;, \u0026#39;mail_domain\u0026#39; =\u0026gt; \u0026#39;gmail.com\u0026#39;, \u0026#39;mail_smtpname\u0026#39; =\u0026gt; \u0026#39;your full gmail address\u0026#39;, \u0026#39;mail_smtppassword\u0026#39; =\u0026gt; \u0026#39;your smtp password\u0026#39;, SSO to Nextcloud via OpenID Connect (OIDC) Authentication to Authelia #\rThis section describes how to set up single sign-on to Nextcloud via OpenID Connect authentication to Authelia. It is based on the Authelia Nextcloud integration guide.\nClient Secret #\rGenerate a random alphanumeric string to be used as client secret:\ntr -cd \u0026#39;[:alnum:]\u0026#39; \u0026lt; /dev/urandom | fold -w \u0026#34;64\u0026#34; | head -n 1 Authelia Configuration File #\rAdd the following to the oidc: section of Authelia’s configuration file config/configuration.yml:\nclients: - id: nextcloud description: Nextcloud secret: \u0026#39;your secret\u0026#39; public: false redirect_uris: - https://nextcloud.home.yourdomain.com/apps/oidc_login/oidc scopes: - openid - profile - email userinfo_signing_algorithm: none Differing from the original guide I removed the groups scope, as it erased all group memberships in Nextcloud after every log off.\nRestart Authelia #\rWe changed the container’s environment, which makes it necessary to recreate the container (stopping and starting is not enough). Navigate to the Authelia stack in Dockge and click Stop \u0026amp; Inactive. Then start the stack.\nInspect Dockge\u0026rsquo;s terminal window for any errors.\nNextcloud Configuration File #\rOpen the config file again.\nvim /usr/local/data/docker/nextcloud/config/www/nextcloud/config/config.php Add the following:\n// OIDC \u0026#39;allow_user_to_change_display_name\u0026#39; =\u0026gt; false, \u0026#39;lost_password_link\u0026#39; =\u0026gt; \u0026#39;disabled\u0026#39;, \u0026#39;oidc_login_provider_url\u0026#39; =\u0026gt; \u0026#39;https://authelia.home.yourdomain.com\u0026#39;, \u0026#39;oidc_login_client_id\u0026#39; =\u0026gt; \u0026#39;nextcloud\u0026#39;, \u0026#39;oidc_login_client_secret\u0026#39; =\u0026gt; \u0026#39;your secret\u0026#39;, \u0026#39;oidc_login_auto_redirect\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_end_session_redirect\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_button_text\u0026#39; =\u0026gt; \u0026#39;SSO\u0026#39;, \u0026#39;oidc_login_hide_password_form\u0026#39; =\u0026gt; true, \u0026#39;oidc_login_use_id_token\u0026#39; =\u0026gt; true, \u0026#39;oidc_login_attributes\u0026#39; =\u0026gt; array ( \u0026#39;id\u0026#39; =\u0026gt; \u0026#39;preferred_username\u0026#39;, \u0026#39;name\u0026#39; =\u0026gt; \u0026#39;name\u0026#39;, \u0026#39;mail\u0026#39; =\u0026gt; \u0026#39;email\u0026#39;, ), \u0026#39;oidc_login_default_group\u0026#39; =\u0026gt; \u0026#39;users\u0026#39;, \u0026#39;oidc_login_use_external_storage\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_scope\u0026#39; =\u0026gt; \u0026#39;openid profile email\u0026#39;, \u0026#39;oidc_login_proxy_ldap\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_disable_registration\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_redir_fallback\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_alt_login_page\u0026#39; =\u0026gt; \u0026#39;assets/login.php\u0026#39;, \u0026#39;oidc_login_tls_verify\u0026#39; =\u0026gt; true, \u0026#39;oidc_create_groups\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_webdav_enabled\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_password_authentication\u0026#39; =\u0026gt; false, \u0026#39;oidc_login_public_key_caching_time\u0026#39; =\u0026gt; 86400, \u0026#39;oidc_login_min_time_between_jwks_requests\u0026#39; =\u0026gt; 10, \u0026#39;oidc_login_well_known_caching_time\u0026#39; =\u0026gt; 86400, \u0026#39;oidc_login_update_avatar\u0026#39; =\u0026gt; false, Start the stack through Dockge.\nClick on the new SSO button below the login field and log in with your user via Authelia. The account gets auto-provisioned in Nextcloud. Logout directly after that.\nNew SSO button\rLog in with the admin one last time. In the menu top right, click Users and add your user to the admins group.\nWe can now disable normal user login. Open the config file yet again.\nvim /usr/local/data/docker/nextcloud/config/www/nextcloud/config/config.php Hide the normal login form and only present the SSO button.\n\u0026#39;auth.webauthn.enabled\u0026#39; =\u0026gt; false, \u0026#39;hide_login_form\u0026#39; =\u0026gt; true, \u0026#39;simpleSignUpLink.shown\u0026#39; =\u0026gt; false, Restart the stack through Dockge.\nAt the time of writing this article, Nextcloud presents an ugly warning message. It is a known issue and the team is working on it.\nUgly warning message\rMemory Cache #\rTo reduce the load on the database, we introduce a Redis memory cache. If you have set up Authelia like me, do not use the existing Redis container for Nextcloud. I had issues signing in with Authelia when it shared its Redis container with Nextcloud.\nOpen the config file a last time and replace the existing memory cache with the new Redis container.\nvim /usr/local/data/docker/nextcloud/config/www/nextcloud/config/config.php // config.php \u0026#39;memcache.local\u0026#39; =\u0026gt; \u0026#39;\\\\OC\\\\Memcache\\\\Redis\u0026#39;, \u0026#39;filelocking.enabled\u0026#39; =\u0026gt; true, \u0026#39;memcache.locking\u0026#39; =\u0026gt; \u0026#39;\\\\OC\\\\Memcache\\\\Redis\u0026#39;, \u0026#39;redis\u0026#39; =\u0026gt; array ( \u0026#39;host\u0026#39; =\u0026gt; \u0026#39;nextcloud-redis\u0026#39;, \u0026#39;port\u0026#39; =\u0026gt; 6379, ), Restart the Nextcloud stack and check Dockge\u0026rsquo;s terminal window for errors.\nExternal Storage \u0026amp; Group Folders #\rGo to the user menu top right and choose Administration settings.\nExternal Storage #\rThe External storage app allows you to map FTP, WebDAV, and more options into the Nextcloud interface for easy access.\nI\u0026rsquo;ve used the type local to get access to my external drive in my Nextcloud client apps. I can easily share anything I have through the Nextcloud UI with others.\nExternal storage in Nextcloud\rGroup Folders #\rGroup folders are much like Spaces in ownCloud oCIS. They appear at the root for each user and are perfect for families to share common documents and pictures. One can assign granular permissions per folder.\nGroup folders in Nextcloud\rChangelog #\r2024-02-08: added the creates parameter to the Nextcloud playbook in the files and folders section. ","date":"5 February 2024","permalink":"/posts/streamlined-nextcloud-with-openid-connect-authentication/","section":"Posts","summary":"This article explains how to set up a fast and efficient Nextcloud as file management tool with OpenID Connect to Authelia.\nIt is part of my series on creating a home server on an old laptop. I\u0026rsquo;m assuming that you\u0026rsquo;ve set up Docker, Caddy, and Authelia, as described in the first article of the series.","title":"Streamlined Nextcloud With OpenID Connect Authentication"},{"content":"A guide to setting up an old laptop as your new home server, powered by Ubuntu, Ansible, and Docker. It securely hosts videos, files, and photos. Add HTTPS everywhere, cloud backup and performance monitoring.\nBye Bye Synology, Hello Ubuntu #\rTL;DR #\rSynology has a great ecosystem, but one is locked in. I\u0026rsquo;m using Ubuntu LTS server now, as it gives full control and is focused on stability. I chose it over Proxmox for simplicity, as I don\u0026rsquo;t need virtual machines.\nFull Story #\rI had a home server for years. I started with a Synology DS210+ thirteen years ago and upgraded to a DS720+ three years ago. The latter allowed running Docker containers and extend the feature set of Synology packages easily. With time, I noticed that I was using only a few Synology packages, namely photos and drive, but the rest was already running in Docker.\nIn addition, I have these problems with Synology:\nIt is built to hold 3,5” spinning disks. These are fine for storage, but not for performance. You can add SSDs as a read cache, but the community is divided about the benefits. The CPUs are crap. The DS720+ comes with an Intel Celeron J4125\u0026hellip; Synology OS can\u0026rsquo;t map external drives in a storage pool. When you add a drive with files to a storage pool, everything is lost as the drive needs to be formatted. Frustrating\u0026hellip; In addition to the point above: I had two volumes and a degraded RAID configuration that I could only solve with manual hacking through ssh, which is frustrating (again!) as Synology OS is a very lightweight Linux built. There is no package manager, for example. It\u0026rsquo;s a viable solution, but not for tech enthusiasts like me who like to have full control over their devices.\nI examined alternatives and ended up with Ubuntu. I\u0026rsquo;m used to it, and the LTS server variant has a strong focus on stability. It\u0026rsquo;s also free. Second place was Proxmox. I don\u0026rsquo;t need virtual machines, though, where Proxmox excels in. I also wanted to use Ansible to automatically install and configure everything, and having Proxmox in between just adds complexity.\nHome Server Goals #\rThese were my goals for the laptop home server:\nAutomated deployment: do as little as possible manually. In case of a hardware failure, I can immediately switch to new hardware and have the exact same configuration. Docker: put as much as possible into Docker containers for easy management and backup Single Sign-On: sign-on with one user identity instead of creating the same user again and again for each service HTTPS Everywhere: work with nice names instead of IPs and ports, even when I\u0026rsquo;m accessing services internally. Adds security, too. Backup: robust backup for Docker containers and beyond Remote access: secure and easy remote access to selected services Home Server Hardware #\rWhen you think about it, laptops are the perfect home servers. The hardware is power efficient, they have an integrated uninterruptible power supply with their batteries and an integrated KVM with mouse, keyboard, and monitor. You can get a modern one on the internet for ~€200-€300. Or even for free if you ask around if someone has a spare one lying around. Just make sure that it is not too old (\u0026gt; Intel 7th generation), as older CPUs don\u0026rsquo;t have great power saving techniques.\nI went with a laptop with\nan 8th generation Core i7 a 1 TB NVMe SSD 16 GB memory More than enough to host a few docker containers and handle 2-4 Plex streams.\nI added an external 3,5\u0026quot; drive to store my media collection.\nBefore You Continue: helgeklein.com #\rMy colleague Helge built a similar setup with an Intel NUC and Proxmox. He has a complete series on building a home lab with SSO and HTTPS certificates for internal services.\nHelge\u0026rsquo;s series on home automation\rIt matches to some of my goals, so I took his series as a framework and added my ideas:\nObviously other hardware and operating system Automatic installation and configuration with Ansible pull Remote access without a full VPN tunnel. (Future article) As my set-up is built on top of his, I encourage you to read his whole series, especially the concept of using Caddy for services in the internal network. I link to his articles whenever there are numerous similarities and only note my customizations.\nIntroducing Ansible #\rAnsible is a free and open-source software to automate software provisioning, configuration management, and application deployments.\nI use it to install packages and configure the home server. No steps are done manually, so when the server breaks, I can spin up another, run the deployment there and have the same configuration.\nWith Ansible, one describes how a target system should look like in YAML. A YAML file is called a playbook. In organizations, there is typically a control server that connects to targets and applies the steps defined in the playbook. That works well but is too complicated for a home lab. You can save the control server and run the playbook directly from GitHub through Ansible pull.\nBelow is a diagram of the deployment architecture. I prepare the config of the home server on a client, push the config to GitHub, and execute it via Ansible pull on the home server.\nflowchart LR\ra(Windows client\\n#8226; WSL\\n#8226; Git \\n#8226; SSH) --\u003e|Push| c(GitHub)\rb(\"Home server\\n#8226; Ansible\\n#8226; Docker\\n#8226; Unbound DNS\\n#8226; restic Backup\") --\u003e|Pull| c\rstyle a text-align:left\rstyle b text-align:left\rCreate a Repository On GitHub #\rCreate a git repository on GitHub.com. I use a private one, as I would rather not share the exact config of my servers with the public.\nInstall the Windows Subsystem For Linux #\rThe following steps need to be done on your client.\rI\u0026rsquo;m a Windows guy, but I feel that working with git and ssh keys is easier on Linux. Instead of creating a Linux VM dedicated to that purpose, I\u0026rsquo;m using the Windows Subsystem for Linux (WSL) which is integrated on Windows.\nwsl.exe --install -d Ubuntu Git #\rInstall Git #\rFrom here on, we work completely in WSL.\napt install git Configure Git #\rAdd your email and full name for git commits.\ngit config --global user.email \u0026#34;info@dominikbritz.com\u0026#34; git config --global user.name \u0026#34;Dominik Britz\u0026#34; Generate a New SSH Key for GitHub #\rssh-keygen -t ed25519 -C \u0026#34;GitHub auth\u0026#34; Why ed25519?\nI named mine id_ed25519_github. You do want to use a passphrase.\nThis generates two files:\nid_ed25519_github ⇽ the private key id_ed25519_github.pub ⇽ the public key To use the key when connecting to GitHub via ssh later, you need to add the private key to the ssh agent. Create the alias ssha to do this conveniently.\nalias ssha=\u0026#39;eval $(ssh-agent) \u0026amp;\u0026amp; ssh-add ~/.ssh/id_ed25519_github\u0026#39; You probably need to commit and push a lot while testing, hence add the following alias as well.\nalias gitall=\u0026#39;git add * \u0026amp;\u0026amp; git commit --message \u0026#34;commit\u0026#34; \u0026amp;\u0026amp; git push\u0026#39; Make the aliases permanent by adding the commands to .bashrc. Otherwise, they are gone after you disconnect the session.\nvim ~/.bashrc Add Key To GitHub #\rcat ~/.ssh/id_ed25519_github.pub Go to your GitHub key settings and enter the output of the cat command as a new authentication key.\nClone the Repository #\rChange with your URL.\ncd ~ git clone git@github.com:DominikBritz/ansible-configs.git Set Up Ansible #\rInstall Ansible #\rWe need Ansible in the WSL environment to install public roles later. We do not manage the WSL environment with Ansible.\napt install ansible Create Ansible Folders And Files #\rCreate the Ansible folder structure.\ncd ~/ansible-configs mkdir roles mkdir vars_files Roles #\rAn Ansible role contains all tasks, variables, and files that logically belong together. They also allow easy sharing with others. I use a combination of my roles and public ones.\nBase #\rUpdates the system and installs a cron job that updates the systems every Sunday at 02:00 and then reboots.\nmkdir roles/base/tasks -p vim roles/base/tasks/main.yml # Before we do anything on the machine, make sure it is up-to-date - name: Run the equivalent of \u0026#34;apt update\u0026#34; as a separate step ansible.builtin.apt: update_cache: yes - name: Upgrade the OS (apt dist-upgrade) ansible.builtin.apt: upgrade: dist - name: Check for pending reboots ansible.builtin.stat: path: /var/run/reboot-required register: reboot_required ignore_errors: yes - name: Reboot the server if required ansible.builtin.reboot: when: reboot_required.stat.exists # Periodically update the OS through a cron job - name: Create cron job for updates and reboot ansible.builtin.cron: name: Perform weekly updates and reboot minute: 0 hour: 2 weekday: 0 # 0 represents Sunday in cron job: \u0026#34;apt update \u0026amp;\u0026amp; apt dist-upgrade -y \u0026amp;\u0026amp; apt autoremove -y \u0026amp;\u0026amp; reboot\u0026#34; user: root Home #\rThis role contains the configuration specific to my home server.\nInstalls the Unbound DNS server. See Unbound DNS Server Configuration. Installs restic and resticprofile for backups. See restic: Encrypted Offsite Backup for Your Homeserver. I added comments so you can follow along.\nNote the step where it mounts an external drive. I have an external drive connected to my home server, as the integrated 1 TB disk has not enough storage for my media collection. If you don\u0026rsquo;t have one, you can remove this step.\nmkdir roles/home/tasks -p vim roles/home/tasks/main.yml - name: Disable sleep when closing the lid lineinfile: path: /etc/systemd/logind.conf regexp: \u0026#39;^#?HandleLidSwitch=\u0026#39; line: \u0026#39;HandleLidSwitch=ignore\u0026#39; backup: yes ### ### Unbound ### - name: systemd is listening on port 53. Remove that as we need the port for our Unbound DNS server later ansible.builtin.lineinfile: path: /etc/systemd/resolved.conf regexp: \u0026#39;^#?DNSStubListener=\u0026#39; line: \u0026#39;DNSStubListener=0\u0026#39; - name: Restart systemd-resolved service ansible.builtin.service: name: systemd-resolved state: restarted - name: Update repositories cache and install \u0026#34;unbound\u0026#34; package ansible.builtin.apt: name: unbound update_cache: yes ### ### Backup with restic and resticprofile ### - name: Install \u0026#34;restic\u0026#34; package ansible.builtin.apt: name: restic update_cache: no - name: Run restic self-update command: restic self-update - name: Check if directory \u0026#34;/usr/local/bin/resticprofile\u0026#34; exists and store the result in the variable \u0026#34;resticprofile_dir\u0026#34; ansible.builtin.stat: path: /usr/local/bin/resticprofile register: resticprofile_dir - name: Download resticprofile install.sh script get_url: url: https://raw.githubusercontent.com/creativeprojects/resticprofile/master/install.sh dest: /tmp/install.sh when: not resticprofile_dir.stat.exists # this ensures we only do this step in case resticprofile is not already installed - name: Change permissions of install.sh file: path: /tmp/install.sh mode: \u0026#39;u+x\u0026#39; when: not resticprofile_dir.stat.exists # this ensures we only do this step in case resticprofile is not already installed - name: Run install.sh script shell: /tmp/install.sh -b /usr/local/bin args: chdir: /tmp when: not resticprofile_dir.stat.exists # this ensures we only do this step in case resticprofile is not already installed - name: Run resticprofile self-update shell: resticprofile self-update ### ### Mount external drive ### - name: Check if the external drive is already mounted shell: cmd: mount | grep \u0026#39;/media/18TB\u0026#39; register: mount_check ignore_errors: yes - name: Mount the drive if not already mounted and set it to mount at boot mount: path: \u0026#39;/media/18TB\u0026#39; src: \u0026#39;UUID=1c333c73-475d-4b3d-a82d-511321753eab\u0026#39; fstype: auto state: mounted boot: true when: mount_check.rc != 0 - name: Set permissions for dockerlimited group on /media/18TB acl: path: /media/18TB entity: dockerlimited etype: group permissions: \u0026#39;rwx\u0026#39; state: present recursive: yes default: true Ansible Galaxy Roles #\rInstead of implementing more complex roles myself, I utilize well-known community roles from Ansible Galaxy. Think of it as Ansible\u0026rsquo;s app store.\nAdd requirements.yml.\n--- roles: - name: geerlingguy.docker - name: geerlingguy.security - name: geerlingguy.ntp - name: oefenweb.ufw - name: geerlingguy.node_exporter Download the roles.\nansible-galaxy install --role-file requirements.yml --roles-path roles geerlingguy.docker\nInstalls Docker on Linux. Documentation.\ngeerlingguy.security\nPerforms some basic security configuration.\nInstall software to monitor bad SSH access (fail2ban) Configure SSH to be more secure (disabling root login, requiring key-based authentication, and allowing a custom SSH port to be set) Set up automatic updates (if configured to do so) Documentation geerlingguy.ntp\nManages NTP.\noefenweb.ufw\nSet up the ufw firewall on Ubuntu conveniently through variables. Documentation.\ngeerlingguy.node_exporter\nInstalls node exporter for monitoring your home server. Documentation\nVariables #\rCreate a home.yml in vars_files for the home server and then accordingly for each server you want to manage. The files get referenced in playbooks later.\n# ntp ntp_timezone: Europe/Berlin # firewall my_ssh_port: 22 ufw_rules: # allow ssh traffic - rule: allow to_port: \u0026#34;{{ my_ssh_port }}\u0026#34; protocol: tcp # allow dns traffic for unbound - rule: allow to_port: \u0026#34;53\u0026#34; protocol: tcp - rule: allow to_port: \u0026#34;53\u0026#34; protocol: udp # allow https traffic to docker containers - rule: allow to_port: \u0026#34;443\u0026#34; protocol: tcp # security security_ssh_port: \u0026#34;{{ my_ssh_port }}\u0026#34; security_autoupdate_reboot: true # enable automatic updates security_autoupdate_reboot_time: \u0026#34;01:00\u0026#34; # if a reboot is necessary do it at this time security_autoupdate_mail_to: \u0026#34;info@dominikbritz.com\u0026#34; # send a notification email to your email address # node exporter node_exporter_version: \u0026#39;1.7.0\u0026#39; Root Playbooks #\rCreate a home.yml in the repository root for your home server, and then accordingly for each server you would like to manage. The name does matter here, as Ansible pull expects it to be identical to the server\u0026rsquo;s hostname. If it does not find a file matching servername.domain.com.yml or servername.yml it falls back to local.yml.\n--- - name: home server hosts: localhost become: true vars_files: - vars_files/home.yml roles: - base - home - oefenweb.ufw - geerlingguy.docker - geerlingguy.ntp - geerlingguy.security - geerlingguy.node_exporter Push to GitHub #\rAt this time, the contents of your local git folder should look like the below:\n├── LICENSE\r├── README.md\r├── home.yml\r├── requirements.yml\r├── roles/\r│ ├── base/\r│ │ └── tasks/\r│ │ └── main.yml\r│ ├── geerlingguy.docker/\r│ │ ├── ...\r│ ├── geerlingguy.node_exporter/\r│ │ ├── ...\r│ ├── geerlingguy.ntp/\r│ │ ├── ...\r│ ├── geerlingguy.security/\r│ │ ├── ...\r│ ├── home/\r│ │ └── tasks/\r│ │ └── main.yml\r│ └── oefenweb.ufw/\r│ ├── ...\r└── vars_files/\r├── home.yml Push the current state to GitHub.\ngitall We\u0026rsquo;re done now with the client.\nInstall And Configure the Server #\rThe following steps have to be done on your home server.\rOperating System #\rI won\u0026rsquo;t cover the installation of Ubuntu, as there are hundreds of guides on the Internet. Just some aspects:\nYou want to go with the Ubuntu LTS server edition without a graphical interface for maximum performance and stability. Set the hostname to home. Generate a New SSH Key For Remote Management #\rssh-keygen -t ed25519 -C \u0026#34;remote management\u0026#34; Store the private key in your WSL environment on your client. You can now close the lid of your laptop home server and hopefully never touch it again.\nRun With Ansible Pull #\rTo use Ansible pull with a private GitHub repository, create a personal access token. I use a fine-grained token that has access to one repository only.\nFine-grained access token in GitHub\rRun ansible-pull to configure your server. If the playbook requires a reboot after the update step, reboot the server and run the playbook again. Ansible can\u0026rsquo;t reboot automatically, as it would effectively kill itself.\nChange the PAT token and the GitHub URL.\nsudo apt update sudo apt install ansible export PAT=YOUR_PAT_TOKEN sudo ansible-pull -U https://$PAT:x-oauth-basic@github.com/DominikBritz/ansible-configs home.yml Create Docker Containers #\rCreating the Docker containers is a one-time manual step. Everything will be backed up later with restic and can be restored in case of data loss. Hence, I did not use Ansible to automate this task.\nThe first container is my management container Dockge. It\u0026rsquo;s fast, has a pretty UI, and, in comparison to Portainer, it has a file-based structure — Dockge won\u0026rsquo;t kidnap your compose files, they are stored on your drive as usual. You can interact with them using normal docker compose commands.\nBeautiful Dockge UI\rCreate the Dockge folder. Putting content that is accessed by multiple users in /usr/local is my preference. Other options are:\n/opt /etc Create your own root like /data mkdir -p /usr/local/data/docker/dockge Create a Dockge compose file with the custom path from above.\ncd /usr/local/data/docker/dockge curl \u0026#34;https://dockge.kuma.pet/compose.yaml?port=5001\u0026amp;stacksPath=/usr/local/data/docker\u0026#34; --output compose.yaml Start the container with docker compose up -d. Browse to http://IPADDRESS:5001 to see the UI. This is where we create all future containers.\nUnbound DNS Server Configuration #\rI followed Helge\u0026rsquo;s guide and set up Unbound as a DNS resolver. I skipped the static IP part, as Ubuntu supports DHCP. Instead, I added a reservation for my home server on my DHCP server.\nI will discuss secure external access in a future blog article. Having an internal DNS resolver becomes essential then, so don\u0026rsquo;t skip this step.\nrestic: Encrypted Offsite Backup for Your Homeserver #\rI followed Helge\u0026rsquo;s guide for restic and resticprofile for backups. You can skip the installation part, though, as Ansible handles this for you.\nIn addition to Helge\u0026rsquo;s setup, I added an exclude file to add excludes anytime without having to reschedule.\nIn the data/resticprofile/profiles.yml add the iexclude-file setting to the backup section.\n# Backup command backup: iexclude-file: \u0026#34;excludes\u0026#34; # name of your exclude file Create the excludes file. Whenever you want to exclude files from your backup, add a line with the path to the file.\nexcludes:\n# Syntax: https://pkg.go.dev/path/filepath#Match\r# all strings are case-insensitive\r# exclude all files in the deemix/downloads folder\r*/docker/deemix/downloads/* Automatic HTTPS Certificates for Services on Internal Home Network #\rI followed Helge\u0026rsquo;s guide and use Caddy for certificates for my internal services. He uses Cloudflare, but I use Netcup. To get it working with Netcup, do the below.\nEdit the Dockerfile in /caddy/dockerfile-dns.\nARG VERSION=2\rFROM caddy:${VERSION}-builder AS builder\rRUN xcaddy build \\\r--with github.com/caddy-dns/netcup\rFROM caddy:${VERSION}\rCOPY --from=builder /usr/bin/caddy /usr/bin/caddy In your container-vars.env, remove the CLOUDFLARE_API_TOKEN and add Netcup specific variables.\nNETCUP_CUSTOMER_NUMBER=12345678 # get the ID from your CCP page NETCUP_API_KEY=ABCD # https://helpcenter.netcup.com/en/wiki/general/our-api NETCUP_API_PASSWORD=password # https://helpcenter.netcup.com/en/wiki/general/our-api The Caddyfile references these variables. Furthermore, Netcup requires some additional settings in the tls configuration.\ndockge.{$MY_DOMAIN} {\rreverse_proxy {$MY_HOST_IP}:5001\rtls {\rdns netcup {\rcustomer_number {env.NETCUP_CUSTOMER_NUMBER}\rapi_key {env.NETCUP_API_KEY}\rapi_password {env.NETCUP_API_PASSWORD}\r}\rpropagation_timeout 900s\rpropagation_delay 600s\rresolvers 1.1.1.1\r}\r} As you can guess from the timeout setting of 900 seconds, it can take a while until the certificate is successfully obtained by caddy. You can monitor the status in the Dockge terminal chart of the caddy container.\nSSO \u0026amp; Monitoring #\rI followed Helge\u0026rsquo;s other articles more or less exactly.\nAuthelia \u0026amp; lldap: Authentication, SSO, User Management \u0026amp; Password Reset for Home Networks Grafana Setup Guide With Automatic HTTPS \u0026amp; OAuth SSO via Authelia Docker Monitoring With Prometheus, Automatic HTTPS \u0026amp; SSO Authentication Conclusion And Next Articles #\rI achieved all my goals that I\u0026rsquo;ve set initially. Let\u0026rsquo;s go over them:\nAutomated deployment: all packages are installed and configured via Ansible. The only manual step is the Ubuntu installation. Docker: all services are running as Docker containers. Essential services like DNS and backup are installed natively. Single Sign-On: provided by LLDAP and Authelia HTTPS Everywhere: served through Caddy Backup: restic and resticprofile backup for Docker mount points and important files like the Unbound configuration Remote access: will be covered in a future article I will continue this series with articles on remote access, Nextcloud for files, Immich for photos, Sonarr/Radarr/Plex for media management, and maybe more.\nChangelog #\r-2024-01-31: added section about changes to the Dockerfile for Caddy\n","date":"23 January 2024","permalink":"/posts/2024-01-22-laptop-home-server/","section":"Posts","summary":"A guide to setting up an old laptop as your new home server, powered by Ubuntu, Ansible, and Docker. It securely hosts videos, files, and photos. Add HTTPS everywhere, cloud backup and performance monitoring.\nBye Bye Synology, Hello Ubuntu #\rTL;DR #\rSynology has a great ecosystem, but one is locked in.","title":"Laptop Home Server Build"},{"content":"","date":"7 January 2021","permalink":"/tags/powershell/","section":"Tags","summary":"","title":"powershell"},{"content":"There are lots and lots of articles on the web describing how to save a whole webpage as PDF. They all use, more or less, the browser\u0026rsquo;s ability to print to PDF.\nI needed to convert many URLs at once. Doing that manually for every URL would have been cumbersome, and that\u0026rsquo;s why I automated the process.\nThe following tools helped me to achieve my goal:\nLinkKlipper: a Chrome extension to export all links from a website wkhtmltopdf: a command-line tool that saves a URL to PDF PDF24: my preferred PDF management tool. You can use any tool; it has to support combining multiple PDFs, though. PowerShell: to glue everything together Collecting All URLs #\rInstall LinkKlipper in Chrome or Edge In the extension\u0026rsquo;s setting, change the output from CSV to TXT Browse to the website you want to convert By clicking on the extension\u0026rsquo;s icon in the menu bar, you can collect all links on the webpage. The webpage I was looking at contained a menu to all subpages, like a sitemap. That allowed me to export all the URLs I was interested in at once.\nConvert Each URL to a PDF File #\rWith the help of the open-source tool wkhtmltopdf and PowerShell you can convert every URL in the TXT file from above to a PDF file.\nDownload wkhtmltopdf and install it.\nLook at the PowerShell script below, change the variables if needed, and run the script.\n$sourceFile = \u0026#34;C:\\links.txt\u0026#34; # the source file containing the URLs you want to convert $destFolder = \u0026#34;C:\\output\u0026#34; # converted PDFs will be saved here. Folder has to exist. $linkList = get-content $sourceFile foreach ($link in $linkList) { $outfile = $link -replace \u0026#39;/\u0026#39;,\u0026#39;-\u0026#39; # replace slashes with dashes $date = get-date -Format \u0026#34;yyyy-MM-dd HH-mm-ss\u0026#34; # adding a date to the filename allows for easy sorting later on $outfile = $date + \u0026#39; \u0026#39; + $outfile + \u0026#39;.pdf\u0026#39; $destFullPath = Join-Path $destFolder $outfile \u0026amp; \u0026#39;C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe\u0026#39; --disable-smart-shrinking --no-footer-line --no-header-line --no-outline \u0026#34;$link\u0026#34; \u0026#34;$destFullPath\u0026#34; } If you\u0026rsquo;re not satisfied with the PDFs\u0026rsquo; design, have a look at wkhtmltopdf\u0026rsquo;s command-line arguments.\nCombine PDFs #\rThe last step is optional. But I prefer reading one big PDF over jumping through many different PDFs. Hence, take your PDF tool of choice, mine is PDF24, and combine all created PDFs into one big PDF file.\n","date":"7 January 2021","permalink":"/posts/2021-01-07-batch-convert-urls-to-pdf/","section":"Posts","summary":"There are lots and lots of articles on the web describing how to save a whole webpage as PDF. They all use, more or less, the browser\u0026rsquo;s ability to print to PDF.\nI needed to convert many URLs at once. Doing that manually for every URL would have been cumbersome, and that\u0026rsquo;s why I automated the process.","title":"PowerShell – Batch Convert URLs to PDF"},{"content":"For a new feature in version 6.0 of uberAgent, we needed to run a few saved searches to do some calculations but let users modify input values.\nOur requirements in detail:\nRun saved searches to calculate experience scores for machines, user sessions, and applications Provide defaults for thresholds and weights but let users configure them. Users might make modifications once or twice, but not daily. So we needed something easy to understand but nothing too fancy. Only administrators should modify settings that should be valid for all users accessing a particular Splunk app. It needed to be configurable, but not for everyone. A poor man\u0026rsquo;s access control, so to say. I explain below how we achieved that by using Splunk anywhere examples. That\u0026rsquo;s probably easier to understand, and I don\u0026rsquo;t have to explain every bit of uberAgent here 😉\nSaved Searches Accept Tokens #\rThat was not clear to me but saved searches accept tokens. That\u0026rsquo;s awesome as it\u0026rsquo;s the foundation of our solution to our requirements: we can run saved searches configured by tokens, and only Splunk users with access to the disk can change them. Poor man\u0026rsquo;s access control achieved!\nLet\u0026rsquo;s start with a simple search listing all source types in the index _internal.\nindex=_internal | top limit=0 sourcetype Do the same but output only source types with a count greater than 1000.\nindex=_internal | top limit=0 sourcetype | where count\u0026gt;1000 The 1000 is the part you want to be configurable. Hence create the following saved search:\n[im_accepting_tokens] dispatch.earliest_time = -30m dispatch.latest_time = now search = index=_internal | top limit=0 sourcetype | where count\u0026gt;$TokenCount$ run_on_startup = false enableSched = 0 Note that 1000 is now $TokenCount$. That can be replaced dynamically by calling the saved search like so:\n| savedsearch im_accepting_tokens TokenCount=500 Using Lookups for Easy Token Management #\rSaved searches are stored in savedsearches.conf. Letting users change things directly there can be dangerous. Searches can get quite long and hard to read, and cron schedules are not for everybody.\nHence we wanted to pass tokens to saved searches more safely and, most importantly, more user-friendly.\nA CSV lookup was the most convenient way we found. Everybody is more or less familiar with the syntax. And we are using them in uberAgent for several things, so it was just another one to manage.\nCreating a Lookup #\rAdd the following to your transforms.conf.\n[lookup_input_tokens]\rfilename = input_tokens.csv Create the input_tokens.csv with the following content.\ntoken,value\rTokenCount,1000 If you need more tokens, add them below the TokenCount one.\ntoken,value\rTokenCount,1000\rAnotherToken,\u0026#34;ABC\u0026#34;\rAndAnotherOne,123 Reading a Lookup as Input for a Saved Search #\rTo read the content of your newly created lookup when running the saved search, call the saved search like the following:\n| savedsearch im_accepting_tokens [ | inputlookup lookup_input_tokens | xyseries token token value | stats values(*) as * | fields - token] | inputlookup lookup_input_tokens reads the lookup xyseries token token value transforms the lookup. Everything in the token column gets a field with value as value. Think of a table. token will be the column header and value an entry in a row for that column. | stats values(*) as * removes all empty values (-\u0026gt; all fields in the table that are empty) | fields - token removes the token field itself as its not a field we want to pass to the saved search Conclusion #\rWe solved all our requirements with not too much effort. Users with sufficient permissions may change things by just editing a CSV file. Nice and easy!\nI\u0026rsquo;ve to admit that using that method to pass one token is slightly overkill, but we had to pass a lot for maximum flexibility.\nHappy Splunking!\n","date":"2 November 2020","permalink":"/posts/2020-11-02-using-lookups-as-token-input/","section":"Posts","summary":"For a new feature in version 6.0 of uberAgent, we needed to run a few saved searches to do some calculations but let users modify input values.\nOur requirements in detail:\nRun saved searches to calculate experience scores for machines, user sessions, and applications Provide defaults for thresholds and weights but let users configure them.","title":"Running Splunk Saved Searches Powered by Tokens From Lookups"},{"content":"","date":"2 November 2020","permalink":"/tags/splunk/","section":"Tags","summary":"","title":"splunk"},{"content":"","date":"17 March 2020","permalink":"/tags/euc/","section":"Tags","summary":"","title":"euc"},{"content":"Managing applications on Windows in enterprises is complex and cumbersome. Admins are using a variety of tools to install, uninstall or reconfigure applications silently. The most popular tool is propably Microsoft\u0026rsquo;s ConfigMgr.\nWhile ConfigMgr (and others) makes sense for mid to large enterprises, the management is too time-consuming for smaller firms. I work in such a smaller firm. We have a lab to test our own applications uberAgent on several operating systems. These lab machines don\u0026rsquo;t only run uberAgent, of course, they also run standard applications like text editors, browsers, and more complex things.\nWhile we are using Microsoft MDT to install the OS and set everything up, all these applications need some sort of installing/updating mechanism. We use chocolatey for most of them. However, not all packages can be deployed with chocolatey. Some need special treatment, pre-install or post-install actions, or there is simply no chocolatey package available.\nFor these \u0026lsquo;special treatment\u0026rsquo; applications we developed our own PowerShell module called vlDeploy, which I\u0026rsquo;m happy to share publically.\nIntroducing vlDeploy #\rvlDeploy is a PowerShell module publically available in the PowerShell gallery. It allows the installation of applications locally, or even better, remotely. The remoting capability enables administrators to push applications to multiple machines.\nHere is a list of its features:\nGets a list of installed applications locally, or, from a remote machine Installs applications locally, or, on remote machines Uninstalls applications locally, or, on remote machines Support for .exe, .msi, and .ps1 installers Very cool: throw the .msi installer at the module and it will take care of installing the application silently. Same is true for .ps1 files. Support for URLs as installer The installer will be downloaded and then deployed Powershell pipeline support Valid exit codes handover Reboot after success Is it a full blown application distribution and everything else can get tossed to the void? No, of course not. It has its use-cases which I\u0026rsquo;m describing in the following.\nCombining vlDeploy with other tools gives you a very powerful PowerShell-based installation framework. See chapter Combine vlDeploy With Other Tools for details.\nPrerequisites #\rBefore you begin, ensure you have met the following requirements:\nAt least PowerShell version 5.0 Windows operating system Installing vlDeploy #\rThe module is availabe in the PowerShell Gallery. To install vlDeploy, follow these steps:\nInstall-Module vlDeploy Using vlDeploy #\rvlDeploy exposes three functions to list installed applications, install applications, and uninstall applications.\nTo see what\u0026rsquo;s possible with each, use:\nGet-Help Install-vlApplication -full Get-Help Uninstall-vlApplication -full Get-Help Get-vlInstalledApplication -full Examples #\rSimple application installation on a remote machine.\n$cred = Get-Credential Install-vlApplication -Computer PC1 -Sourcefiles \u0026#39;C:\\apps\\source\u0026#39; -Installer Setup.exe -InstallerArguments \u0026#39;/silent /noreboot\u0026#39; -Credential $cred Application installation on a remote machine with a PowerShell script downloaded from the Internet.\n$cred = Get-Credential Install-vlApplication -Computer PC1 -Installer \u0026#39;https://somewebsite.com/apps/Install.ps1\u0026#39; -Credential $cred Install-vlApplication accepts pipeline input for the -Computer parameter , which makes it easy to mass-deploy applications. It also recognizes .msi installer files and builds the full install command automatically.\n$cred = Get-Credential \u0026#39;PC1\u0026#39;, \u0026#39;PC2\u0026#39;, \u0026#39;PC3\u0026#39; | Install-vlApplication -Sourcefiles \u0026#39;C:\\apps\\source\u0026#39; -Installer Setup.msi -Credential $cred Get the uninstall string of an application from a remote machine and uninstall it there. Valid exit codes are 0, 3010, and 1 (default 0 and 3010).\n$cred = Get-Credential Get-vlInstalledApplication -Computer PC1 -Name \u0026#39;Google Chrome\u0026#39; -Credential $cred | Uninstall-vlApplication -Credential $cred -ValidExitCodes 0,3010,1 Get the uninstall string of an application from a remote machine and uninstall it on multiple machines. Reboot every machine if uninstallation was successful.\n$cred = Get-Credential $UninstallString = (Get-vlInstalledApplication -Computer PC1 -Name \u0026#39;Google Chrome\u0026#39; -Credential $cred).UninstallString \u0026#39;PC1\u0026#39;, \u0026#39;PC2\u0026#39;, \u0026#39;PC3\u0026#39; | Uninstall-vlApplication -UninstallString $UninstallString -Credential $cred -RebootAfterSuccess Combine vlDeploy With Other Tools #\rYou can combine vlDeploy very nicely with other PowerShell tools. The combination with the following tools gives you a very powerful PowerShell-based installation framework\nEvergreen #\rEvergreen is a PowerShell module actively developed by Aaron Parker to return the latest version and download URLs for a set of common enterprise applications.\nAs of 2020-03-02, the following applications are included:\nAdobeAcrobatReaderDC\rAtom\rBISF\rCitrixAppLayeringFeed\rCitrixApplicationDeliveryManagementFeed\rCitrixEndpointManagementFeed\rCitrixGatewayFeed\rCitrixHypervisorFeed\rCitrixLicensingFeed\rCitrixReceiverFeed\rCitrixSdwanFeed\rCitrixVirtualAppsDesktopsFeed\rCitrixWorkspaceApp\rCitrixWorkspaceAppFeed\rCitrixXenServerTools\rControlUpAgent\rCyberduck\rFileZilla\rFoxitReader\rGitForWindows\rGood\rGoogleChrome\rGreenshot\rJamTreeSizeFree\rJamTreeSizeProfessional\rJava8\rLibreOffice\rMicrosoftEdge\rMicrosoftFSLogixApps\rMicrosoftOffice\rMicrosoftPowerShellCore\rMicrosoftSQLServerManagementStudio\rMicrosoftSsms\rMicrosoftTeams\rMicrosoftVisualStudioCode\rMozillaFirefox\rmRemoteNG\rNotepadPlusPlus\rOpenJDK\rOracleJava8\rOracleVirtualBox\rPaintDotNet\rScooterBeyondCompare\rShareX\rTeamViewer\rVideoLanVlcPlayer\rVMwareTools\rWinMerge\rZoom After installing the module via Install-Module Evergreen you can see which applications are available with (Get-Command -Module Evergreen).Name -replace 'Get-','' | Sort-Object.\nIn combination with vlDeploy you can deploy applications to multiple hosts without having to maintain a fileshare with dozens of sources.\nBelow is a simple example using vlDeploy in combination with Evergreen to install Microsoft Edge.\n# Variables $Computers = \u0026#34;PC1\u0026#34;, \u0026#34;PC2\u0026#34; $cred = Get-Credential # Install modules $Modules = @(\u0026#34;Evergreen\u0026#34;, \u0026#34;vlDeploy\u0026#34;) Foreach ($Module in $Modules) { If (-not(Get-Module $Module)) { Install-Module $Module -Force Import-Module $Module } Else { Update-Module $Module Import-Module $Module } } # Get setup URI $App = Get-MicrosoftEdge | Where-Object Platform -eq \u0026#39;Windows\u0026#39; | Where-Object Product -eq \u0026#39;Stable\u0026#39; | Where-Object Architecture -eq \u0026#39;x64\u0026#39; | Select-Object -First 1 # Install Install-vlApplication -Installer \u0026#34;$($App.URI)\u0026#34; -Computer $Computers -Credential $cred I recommend having a look at Evergreen in more detail. It also includes silent installer arguments as well as pre-install and post-install actions. Here is nice example for Adobe Reader.\nXenAppBlog Automation Framework Applications #\rThe XenAppBlog Automation Framework Applications by Trond Eirik Haavarstein are scripts that install applications without the need to maintain installer sources. The framework is using Evergreen in some scripts as well.\nInstall VMware Tools on multiple computers and reboot without having to think about managing installers.\n# Variables $Computers = \u0026#34;PC1\u0026#34;,\u0026#34;PC2\u0026#34; $cred = Get-Credential # Install app Install-vlApplication -Installer \u0026#39;https://raw.githubusercontent.com/haavarstein/Applications/master/VMware/Tools/Install.ps1\u0026#39; -Computer $Computers -Credential $cred -RebootAfterSuccess PowerShell App Deployment Toolkit #\rEvergreen and the XenAppBlog Automation Framework Applications are limited to a set of common enterprise applications. You won\u0026rsquo;t find your very special LOB application in there. LOB applications are best deployed with the PowerShell App Deployment Toolkit.\nThe PowerShell App Deployment Toolkit provides a set of functions to perform common application deployment tasks and to interact with the user during a deployment. It simplifies the complex scripting challenges of deploying applications in the enterprise, provides a consistent deployment experience and improves installation success rates. Source: https://github.com/PSAppDeployToolkit/PSAppDeployToolkit\nThe installation logic is defined in the file Deploy-Application.ps1. That makes the integration with vlDeploy very easy.\n# Variables $Computers = \u0026#34;PC1\u0026#34;,\u0026#34;PC2\u0026#34; $cred = Get-Credential $Sourcefiles = \u0026#34;\\\\server\\share\\apps\u0026#34; # Install apps $Folders = (Get-ChildItem $Sourcefiles -Directory).FullName Foreach ($Folder in $Folders) { Install-vlApplication -Sourcefiles $Folder -Installer \u0026#39;Deploy-Application.ps1\u0026#39; -Computer $Computers -Credential $cred } A step-by-step guide for the PowerShell App Deployment Toolkit is available here from replicajunction.\nDevelopment And Contributing #\rThe module has a feature set tailored to the needs we have in our company. If you need something else, please contribute at GitHub.\nA list of features I would like to add, but there was not enough time yet:\nRun install/uninstall as PowerShell jobs to benefit from parallelism Perhaps use PowerShell 7 and Foreach-Object -Parallel. More information. Add support for .bat,.cmd, .msp, .mst, and .msu installers Output only applications which are listed in Add/Remove programs by default. Output everything only when desired. ","date":"17 March 2020","permalink":"/posts/2020-03-17-introducing-vldeploy/","section":"Posts","summary":"Managing applications on Windows in enterprises is complex and cumbersome. Admins are using a variety of tools to install, uninstall or reconfigure applications silently. The most popular tool is propably Microsoft\u0026rsquo;s ConfigMgr.\nWhile ConfigMgr (and others) makes sense for mid to large enterprises, the management is too time-consuming for smaller firms.","title":"Introducing vlDeploy - Deploy Applications With PowerShell Remotely"},{"content":"CV #\rProfessional experience #\rLead Product Manager #\rCloud Software Group, Citrix Jan. 2024 — Today\nManaging the transition of uberAgent into the Citrix platform.\nCustomer Success Engineer #\rvast limit GmbH (acquired by Cloud Software Group) Nov. 2017 — Dec. 2023\nIn my position as customer success engineer I had a broad set of responsibilities covering such diverse areas as key account management, technical presales, partner enablement, 3rd level technical support, Splunk dashboard development, and integration testing for our product uberAgent.\nIT System Architect Desktopvirtualisation #\rOBI Smart Technologies GmbH Oct. 2016 — Oct. 2017\nAs an architect I was responsible for the design and implementation of the virtual desktop and app infrastructure at OBI.\nSenior IT-Consultant #\rsepago GmbH Aug. 2011 — Sept. 2016\nSpecialized in operating, building, and designing virtual desktop infrastructures with a strong focus on automation. Technologies I\u0026rsquo;ve worked with:\nCitrix XenApp/XenDesktop Citrix NetScaler Citrix XenMobile Trainee #\rsepago GmbH Apr. 2011 — Sept. 2011\nEducation #\rFachinformatiker Systemintegration #\rsepago GmbH 2009 — 2011\n","date":"1 January 0001","permalink":"/cv/","section":"DominikBritz.com","summary":"CV #\rProfessional experience #\rLead Product Manager #\rCloud Software Group, Citrix Jan. 2024 — Today\nManaging the transition of uberAgent into the Citrix platform.\nCustomer Success Engineer #\rvast limit GmbH (acquired by Cloud Software Group) Nov. 2017 — Dec. 2023\nIn my position as customer success engineer I had a broad set of responsibilities covering such diverse areas as key account management, technical presales, partner enablement, 3rd level technical support, Splunk dashboard development, and integration testing for our product uberAgent.","title":""},{"content":"Impressum #\rAngaben gemäß § 5 TMG #\rDominik Britz\nGengesfeld, 12\n51688 Wipperfürth\nKontakt #\rE-Mail: info@dominikbritz.com\nRedaktionell verantwortlich #\rDominik Britz\n","date":"1 January 0001","permalink":"/imprint/","section":"DominikBritz.com","summary":"Impressum #\rAngaben gemäß § 5 TMG #\rDominik Britz\nGengesfeld, 12\n51688 Wipperfürth\nKontakt #\rE-Mail: info@dominikbritz.com\nRedaktionell verantwortlich #\rDominik Britz","title":""},{"content":"Datenschutzerklärung #\rAllgemeiner Hinweis und Pflichtinformationen #\rBenennung der verantwortlichen Stelle #\rDie verantwortliche Stelle für die Datenverarbeitung auf dieser Website ist:\nDominik Britz\nGengesfeld, 12\n51688 Wipperfürth\nDie verantwortliche Stelle entscheidet allein oder gemeinsam mit anderen über die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten (z.B. Namen, Kontaktdaten o. Ä.).\nWiderruf Ihrer Einwilligung zur Datenverarbeitung #\rNur mit Ihrer ausdrücklichen Einwilligung sind einige Vorgänge der Datenverarbeitung möglich. Ein Widerruf Ihrer bereits erteilten Einwilligung ist jederzeit möglich. Für den Widerruf genügt eine formlose Mitteilung per E-Mail. Die Rechtmäßigkeit der bis zum Widerruf erfolgten Datenverarbeitung bleibt vom Widerruf unberührt.\nRecht auf Beschwerde bei der zuständigen Aufsichtsbehörde #\rAls Betroffener steht Ihnen im Falle eines datenschutzrechtlichen Verstoßes ein Beschwerderecht bei der zuständigen Aufsichtsbehörde zu. Zuständige Aufsichtsbehörde bezüglich datenschutzrechtlicher Fragen ist der Landesdatenschutzbeauftragte des Bundeslandes, in dem sich der Sitz unseres Unternehmens befindet. Der folgende Link stellt eine Liste der Datenschutzbeauftragten sowie deren Kontaktdaten bereit: https://www.bfdi.bund.de/DE/Infothek/Anschriften_Links/anschriften_links-node.html.\nRecht auf Datenübertragbarkeit #\rIhnen steht das Recht zu, Daten, die wir auf Grundlage Ihrer Einwilligung oder in Erfüllung eines Vertrags automatisiert verarbeiten, an sich oder an Dritte aushändigen zu lassen. Die Bereitstellung erfolgt in einem maschinenlesbaren Format. Sofern Sie die direkte Übertragung der Daten an einen anderen Verantwortlichen verlangen, erfolgt dies nur, soweit es technisch machbar ist.\nRecht auf Auskunft, Berichtigung, Sperrung, Löschung #\rSie haben jederzeit im Rahmen der geltenden gesetzlichen Bestimmungen das Recht auf unentgeltliche Auskunft über Ihre gespeicherten personenbezogenen Daten, Herkunft der Daten, deren Empfänger und den Zweck der Datenverarbeitung und ggf. ein Recht auf Berichtigung, Sperrung oder Löschung dieser Daten. Diesbezüglich und auch zu weiteren Fragen zum Thema personenbezogene Daten können Sie sich jederzeit über die im Impressum aufgeführten Kontaktmöglichkeiten an uns wenden.\nSSL- bzw. TLS-Verschlüsselung #\rAus Sicherheitsgründen und zum Schutz der Übertragung vertraulicher Inhalte, die Sie an uns als Seitenbetreiber senden, nutzt unsere Website eine SSL-bzw. TLS-Verschlüsselung. Damit sind Daten, die Sie über diese Website übermitteln, für Dritte nicht mitlesbar. Sie erkennen eine verschlüsselte Verbindung an der „https://“ Adresszeile Ihres Browsers und am Schloss-Symbol in der Browserzeile.\nServer-Log-Dateien #\rIn Server-Log-Dateien erhebt und speichert der Provider der Website automatisch Informationen, die Ihr Browser automatisch an uns übermittelt. Dies sind:\nBesuchte Seite auf unserer Domain\nDatum und Uhrzeit der Serveranfrage\nBrowsertyp und Browserversion\nVerwendetes Betriebssystem\nReferrer URL\nHostname des zugreifenden Rechners\nIP-Adresse\nEs findet keine Zusammenführung dieser Daten mit anderen Datenquellen statt. Grundlage der Datenverarbeitung bildet Art. 6 Abs. 1 lit. b DSGVO, der die Verarbeitung von Daten zur Erfüllung eines Vertrags oder vorvertraglicher Maßnahmen gestattet.\nSpeicherdauer von Beiträgen und Kommentaren #\rBeiträge und Kommentare sowie damit in Verbindung stehende Daten, wie beispielsweise IP-Adressen, werden gespeichert. Der Inhalt verbleibt auf unserer Website, bis er vollständig gelöscht wurde oder aus rechtlichen Gründen gelöscht werden musste.\nDie Speicherung der Beiträge und Kommentare erfolgt auf Grundlage Ihrer Einwilligung (Art. 6 Abs. 1 lit. a DSGVO). Ein Widerruf Ihrer bereits erteilten Einwilligung ist jederzeit möglich. Für den Widerruf genügt eine formlose Mitteilung per E-Mail. Die Rechtmäßigkeit bereits erfolgter Datenverarbeitungsvorgänge bleibt vom Widerruf unberührt.\nAbonnieren von Kommentaren #\rSie können als Nutzer unserer Website nach erfolgter Anmeldung Kommentare abonnieren. Mit einer Bestätigungs-E-Mail prüfen wir, ob Sie der Inhaber der angegebenen E-Mail-Adresse sind. Sie können die Abo-Funktion für Kommentare jederzeit über einen Link, der sich in einer Abo-Mail befindet, abbestellen. Zur Einrichtung des Abonnements eingegebene Daten werden im Falle der Abmeldung gelöscht. Sollten diese Daten für andere Zwecke und an anderer Stelle an uns übermittelt worden sein, verbleiben diese weiterhin bei uns.\nCookies #\rUnsere Website verwendet Cookies. Das sind kleine Textdateien, die Ihr Webbrowser auf Ihrem Endgerät speichert. Cookies helfen uns dabei, unser Angebot nutzerfreundlicher, effektiver und sicherer zu machen.\nEinige Cookies sind “Session-Cookies.” Solche Cookies werden nach Ende Ihrer Browser-Sitzung von selbst gelöscht. Hingegen bleiben andere Cookies auf Ihrem Endgerät bestehen, bis Sie diese selbst löschen. Solche Cookies helfen uns, Sie bei Rückkehr auf unserer Website wiederzuerkennen.\nMit einem modernen Webbrowser können Sie das Setzen von Cookies überwachen, einschränken oder unterbinden. Viele Webbrowser lassen sich so konfigurieren, dass Cookies mit dem Schließen des Programms von selbst gelöscht werden. Die Deaktivierung von Cookies kann eine eingeschränkte Funktionalität unserer Website zur Folge haben.\nDas Setzen von Cookies, die zur Ausübung elektronischer Kommunikationsvorgänge oder der Bereitstellung bestimmter, von Ihnen erwünschter Funktionen (z.B. Warenkorb) notwendig sind, erfolgt auf Grundlage von Art. 6 Abs. 1 lit. f DSGVO. Als Betreiber dieser Website haben wir ein berechtigtes Interesse an der Speicherung von Cookies zur technisch fehlerfreien und reibungslosen Bereitstellung unserer Dienste. Sofern die Setzung anderer Cookies (z.B. für Analyse-Funktionen) erfolgt, werden diese in dieser Datenschutzerklärung separat behandelt.\nYouTube Videos #\rArt und Umfang der Verarbeitung #\rWir haben auf unserer Website YouTube Video integriert. YouTube Video ist eine Komponente der Videoplattform der YouTube, LLC, auf der Nutzer Inhalte hochladen, über das Internet teilen und detaillierte Statistiken erhalten können. YouTube Video ermöglicht es uns, Inhalte der Plattform in unsere Website zu integrieren.\nYouTube Video nutzt Cookies und weitere Browser-Technologien um Nutzerverhalten auszuwerten, Nutzer wiederzuerkennen und Nutzerprofile zu erstellen. Diese Informationen werden unter anderem genutzt, um die Aktivität der angehörten Inhalte zu analysieren und Berichte zu erstellen. Wenn ein Nutzer bei YouTube, LLC registriert ist, kann YouTube Video die abgespielten Videos dem Profil zuordnen.\nWenn Sie auf diese Inhalte zugreifen, stellen Sie eine Verbindung zu Servern der YouTube, LLC, Google Ireland Limited, Gordon House, Barrow Street Dublin 4 Irland her, wobei Ihre IP-Adresse und ggf. Browserdaten wie Ihr User-Agent übermittelt werden.\nZweck und Rechtsgrundlage #\rDie Nutzung des Dienstes erfolgt auf Grundlage Ihrer Einwilligung gemäß Art. 6 Abs. 1 lit. a. DSGVO und § 25 Abs. 1 TTDSG.\nWir beabsichtigen personenbezogenen Daten an Drittländer außerhalb des Europäischen Wirtschaftsraums, insbesondere die USA, zu übermitteln. In Fällen, in denen kein Angemessenheitsbeschluss der Europäischen Kommission existiert (z.B. in den USA) haben wir mit den Empfängern der Daten anderweitige geeignete Garantien im Sinne der Art. 44 ff. DSGVO vereinbart. Dies sind – sofern nicht anders angegeben – Standardvertragsklauseln der EU-Kommission gemäß Durchführungsbeschluss (EU) 2021/914 vom 4. Juni 2021. Eine Kopie dieser Standardvertragsklauseln können Sie unter https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32021D0914\u0026amp;from=DE einsehen.\nZudem holen wir vor einem solchen Drittlandtransfer Ihre Einwilligung nach Art. 49 Abs. 1 Satz 1 lit. a. DSGVO ein, die Sie über die Einwilligung im Consent Manager (oder sonstigen Formularen, Registrierungen etc.) erteilen. Wir weisen Sie darauf hin, dass bei Drittlandübermittlungen im Detail unbekannte Risiken (z.B. die Datenverarbeitung durch Sicherheitsbehörden des Drittlandes, deren genauer Umfang und deren Folgen für Sie wir nicht kennen, auf die wir keinen Einfluss haben und von denen Sie unter Umständen keine Kenntnis erlangen) bestehen können.\nSpeicherdauer #\rDie konkrete Speicherdauer der verarbeiteten Daten ist nicht durch uns beeinflussbar, sondern wird von YouTube, LLC bestimmt. Weitere Hinweise finden Sie in der Datenschutzerklärung für YouTube Video: https://policies.google.com/privacy.\nVimeo #\rArt und Umfang der Verarbeitung #\rWir haben auf unserer Website Vimeo Video integriert. Vimeo Video ist eine Komponente der Videoplattform von Vimeo, LLC, auf der Nutzer Inhalte hochladen, über das Internet teilen und detaillierte Statistiken erhalten können.\nVimeo Video ermöglicht es uns, Inhalte der Plattform in unsere Website zu integrieren.\nVimeo Video nutzt Cookies und weitere Browser-Technologien um Nutzerverhalten auszuwerten, Nutzer wiederzuerkennen und Nutzerprofile zu erstellen. Diese Informationen werden unter anderem genutzt, um die Aktivität der angehörten Inhalte zu analysieren und Berichte zu erstellen.\nWenn Sie auf diese Inhalte zugreifen, stellen Sie eine Verbindung zu Servern der Vimeo, LLC, 555 W 18th St, New York, New York 10011 her, wobei Ihre IP-Adresse und ggf. Browserdaten wie Ihr User-Agent übermittelt werden.\nZweck und Rechtsgrundlage #\rDer Einsatz von Vimeo erfolgt auf Grundlage Ihrer Einwilligung gemäß Art. 6 Abs. 1 lit. a. DSGVO und § 25 Abs. 1 TTDSG.\nWir beabsichtigen personenbezogenen Daten an Drittländer außerhalb des Europäischen Wirtschaftsraums, insbesondere die USA, zu übermitteln. In Fällen, in denen kein Angemessenheitsbeschluss der Europäischen Kommission existiert (z.B. in den USA) haben wir mit den Empfängern der Daten anderweitige geeignete Garantien im Sinne der Art. 44 ff. DSGVO vereinbart. Dies sind – sofern nicht anders angegeben – Standardvertragsklauseln der EU-Kommission gemäß Durchführungsbeschluss (EU) 2021/914 vom 4. Juni 2021. Eine Kopie dieser Standardvertragsklauseln können Sie unter https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32021D0914\u0026amp;from=DE einsehen.\nZudem holen wir vor einem solchen Drittlandtransfer Ihre Einwilligung nach Art. 49 Abs. 1 Satz 1 lit. a. DSGVO ein, die Sie über die Einwilligung im Consent Manager (oder sonstigen Formularen, Registrierungen etc.) erteilen. Wir weisen Sie darauf hin, dass bei Drittlandübermittlungen im Detail unbekannte Risiken (z.B. die Datenverarbeitung durch Sicherheitsbehörden des Drittlandes, deren genauer Umfang und deren Folgen für Sie wir nicht kennen, auf die wir keinen Einfluss haben und von denen Sie unter Umständen keine Kenntnis erlangen) bestehen können.\nSpeicherdauer #\rDie konkrete Speicherdauer der verarbeiteten Daten ist nicht durch uns beeinflussbar, sondern wird von Vimeo, LLC bestimmt. Weitere Hinweise finden Sie in der Datenschutzerklärung für Vimeo Video: https://vimeo.com/privacy.\nCloudflare CDN #\rArt und Umfang der Verarbeitung #\rWir verwenden zur ordnungsgemäßen Bereitstellung der Inhalte unserer Website Cloudflare CDN. Cloudflare CDN ist ein Dienst der Cloudflare, Inc., welcher auf unserer Website als Content Delivery Network (CDN) fungiert.\nEin CDN trägt dazu bei, Inhalte unseres Onlineangebotes, insbesondere Dateien wie Grafiken oder Skripte, mit Hilfe regional oder international verteilter Server schneller bereitzustellen. Wenn Sie auf diese Inhalte zugreifen, stellen Sie eine Verbindung zu Servern der Cloudflare, Inc., her, wobei Ihre IP-Adresse und ggf. Browserdaten wie Ihr User-Agent übermittelt werden. Diese Daten werden ausschließlich zu den oben genannten Zwecken und zur Aufrechterhaltung der Sicherheit und Funktionalität von Cloudflare CDN verarbeitet.\nZweck und Rechtsgrundlage #\rDie Nutzung des Content Delivery Networks erfolgt auf Grundlage unserer berechtigten Interessen, d.h. Interesse an einer sicheren und effizienten Bereitstellung sowie der Optimierung unseres Onlineangebotes gemäß Art. 6 Abs. 1 lit. f. DSGVO.\nWir beabsichtigen personenbezogenen Daten an Drittländer außerhalb des Europäischen Wirtschaftsraums, insbesondere die USA, zu übermitteln. In Fällen, in denen kein Angemessenheitsbeschluss der Europäischen Kommission existiert (z.B. in den USA) haben wir mit den Empfängern der Daten anderweitige geeignete Garantien im Sinne der Art. 44 ff. DSGVO vereinbart. Dies sind – sofern nicht anders angegeben – Standardvertragsklauseln der EU-Kommission gemäß Durchführungsbeschluss (EU) 2021/914 vom 4. Juni 2021. Eine Kopie dieser Standardvertragsklauseln können Sie unter https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32021D0914\u0026amp;from=DE einsehen.\nSpeicherdauer #\rDie konkrete Speicherdauer der verarbeiteten Daten ist nicht durch uns beeinflussbar, sondern wird von Cloudflare, Inc. bestimmt. Weitere Hinweise finden Sie in der Datenschutzerklärung für Cloudflare CDN: https://www.cloudflare.com/privacypolicy/.\nGoogle Maps #\rArt und Umfang der Verarbeitung #\rWir verwenden zur Erstellung von Anfahrtsbeschreibungen den Kartendienst Google Maps. Google Maps ist ein Dienst der Google Ireland Limited, welcher auf unserer Website eine Karte darstellt. Wenn Sie auf diese Inhalte unserer Website zugreifen, stellen Sie eine Verbindung zu Servern der Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Irland her, wobei Ihre IP-Adresse und ggf. Browserdaten wie Ihr User-Agent übermittelt werden. Diese Daten werden ausschließlich zu den oben genannten Zwecken und zur Aufrechterhaltung der Sicherheit und Funktionalität von Google Maps verarbeitet.\nZweck und Rechtsgrundlage #\rDer Einsatz von Google Maps erfolgt auf Grundlage Ihrer Einwilligung gemäß Art. 6 Abs. 1 lit. a. DSGVO und § 25 Abs. 1 TTDSG.\nWir beabsichtigen personenbezogenen Daten an Drittländer außerhalb des Europäischen Wirtschaftsraums, insbesondere die USA, zu übermitteln. In Fällen, in denen kein Angemessenheitsbeschluss der Europäischen Kommission existiert (z.B. in den USA) haben wir mit den Empfängern der Daten anderweitige geeignete Garantien im Sinne der Art. 44 ff. DSGVO vereinbart. Dies sind – sofern nicht anders angegeben – Standardvertragsklauseln der EU-Kommission gemäß Durchführungsbeschluss (EU) 2021/914 vom 4. Juni 2021. Eine Kopie dieser Standardvertragsklauseln können Sie unter https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32021D0914\u0026amp;from=DE einsehen.\nZudem holen wir vor einem solchen Drittlandtransfer Ihre Einwilligung nach Art. 49 Abs. 1 Satz 1 lit. a. DSGVO ein, die Sie über die Einwilligung im Consent Manager (oder sonstigen Formularen, Registrierungen etc.) erteilen. Wir weisen Sie darauf hin, dass bei Drittlandübermittlungen im Detail unbekannte Risiken (z.B. die Datenverarbeitung durch Sicherheitsbehörden des Drittlandes, deren genauer Umfang und deren Folgen für Sie wir nicht kennen, auf die wir keinen Einfluss haben und von denen Sie unter Umständen keine Kenntnis erlangen) bestehen können.\nSpeicherdauer #\rDie konkrete Speicherdauer der verarbeiteten Daten ist nicht durch uns beeinflussbar, sondern wird von Google Ireland Limited bestimmt. Weitere Hinweise finden Sie in der Datenschutzerklärung für Google Maps: https://policies.google.com/privacy.\nGoogle Web Fonts #\rArt und Umfang der Verarbeitung #\rWir verwenden Google Fonts von Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Irland, als Dienst zur Bereitstellung von Schriftarten für unser Onlineangebot. Um diese Schriftarten zu beziehen, stellen Sie eine Verbindung zu Servern von Google Ireland Limited her, wobei Ihre IP-Adresse übertragen wird.\nZweck und Rechtsgrundlage #\rDer Einsatz von Google Fonts erfolgt auf Grundlage Ihrer Einwilligung gemäß Art. 6 Abs. 1 lit. a. DSGVO und § 25 Abs. 1 TTDSG.\nWir beabsichtigen personenbezogenen Daten an Drittländer außerhalb des Europäischen Wirtschaftsraums, insbesondere die USA, zu übermitteln. In Fällen, in denen kein Angemessenheitsbeschluss der Europäischen Kommission existiert (z.B. in den USA) haben wir mit den Empfängern der Daten anderweitige geeignete Garantien im Sinne der Art. 44 ff. DSGVO vereinbart. Dies sind – sofern nicht anders angegeben – Standardvertragsklauseln der EU-Kommission gemäß Durchführungsbeschluss (EU) 2021/914 vom 4. Juni 2021. Eine Kopie dieser Standardvertragsklauseln können Sie unter https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32021D0914\u0026amp;from=DE einsehen.\nZudem holen wir vor einem solchen Drittlandtransfer Ihre Einwilligung nach Art. 49 Abs. 1 Satz 1 lit. a. DSGVO ein, die Sie über die Einwilligung im Consent Manager (oder sonstigen Formularen, Registrierungen etc.) erteilen. Wir weisen Sie darauf hin, dass bei Drittlandübermittlungen im Detail unbekannte Risiken (z.B. die Datenverarbeitung durch Sicherheitsbehörden des Drittlandes, deren genauer Umfang und deren Folgen für Sie wir nicht kennen, auf die wir keinen Einfluss haben und von denen Sie unter Umständen keine Kenntnis erlangen) bestehen können.\nSpeicherdauer #\rDie konkrete Speicherdauer der verarbeiteten Daten ist nicht durch uns beeinflussbar, sondern wird von Google Ireland Limited bestimmt. Weitere Hinweise finden Sie in der Datenschutzerklärung für Google Fonts: https://policies.google.com/privacy.\nFont Awesome #\rArt und Umfang der Verarbeitung #\rWir verwenden zur ordnungsgemäßen Bereitstellung der Inhalte unserer Website Font Awesome des Anbieters Fonticons, Inc..\nZweck und Rechtsgrundlage #\rDer Einsatz von Font Awesome erfolgt auf Grundlage Ihrer Einwilligung gemäß Art. 6 Abs. 1 lit. a. DSGVO und § 25 Abs. 1 TTDSG.\nWir beabsichtigen personenbezogenen Daten an Drittländer außerhalb des Europäischen Wirtschaftsraums, insbesondere die USA, zu übermitteln. In Fällen, in denen kein Angemessenheitsbeschluss der Europäischen Kommission existiert (z.B. in den USA) haben wir mit den Empfängern der Daten anderweitige geeignete Garantien im Sinne der Art. 44 ff. DSGVO vereinbart. Dies sind – sofern nicht anders angegeben – Standardvertragsklauseln der EU-Kommission gemäß Durchführungsbeschluss (EU) 2021/914 vom 4. Juni 2021. Eine Kopie dieser Standardvertragsklauseln können Sie unter https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32021D0914\u0026amp;from=DE einsehen.\nZudem holen wir vor einem solchen Drittlandtransfer Ihre Einwilligung nach Art. 49 Abs. 1 Satz 1 lit. a. DSGVO ein, die Sie über die Einwilligung im Consent Manager (oder sonstigen Formularen, Registrierungen etc.) erteilen. Wir weisen Sie darauf hin, dass bei Drittlandübermittlungen im Detail unbekannte Risiken (z.B. die Datenverarbeitung durch Sicherheitsbehörden des Drittlandes, deren genauer Umfang und deren Folgen für Sie wir nicht kennen, auf die wir keinen Einfluss haben und von denen Sie unter Umständen keine Kenntnis erlangen) bestehen können.\nSpeicherdauer #\rDie konkrete Speicherdauer der verarbeiteten Daten ist nicht durch uns beeinflussbar, sondern wird von Fonticons, Inc. bestimmt. Weitere Hinweise finden Sie in der Datenschutzerklärung für Font Awesome CDN: https://cdn.fontawesome.com/privacy.\n","date":"1 January 0001","permalink":"/privacypolicy/","section":"DominikBritz.com","summary":"Datenschutzerklärung #\rAllgemeiner Hinweis und Pflichtinformationen #\rBenennung der verantwortlichen Stelle #\rDie verantwortliche Stelle für die Datenverarbeitung auf dieser Website ist:\nDominik Britz\nGengesfeld, 12\n51688 Wipperfürth\nDie verantwortliche Stelle entscheidet allein oder gemeinsam mit anderen über die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten (z.B. Namen, Kontaktdaten o.","title":""},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]